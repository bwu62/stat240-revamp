[{"path":"index.html","id":"welcome","chapter":"Welcome!","heading":"Welcome!","text":"-progress working draft revamp STAT 240 course notes. Significant changes made order topics, degree coverage, examples employed. Currently, intended supplement rather replace existing body STAT 240 notes.","code":""},{"path":"index.html","id":"prerequisitesscope","chapter":"Welcome!","heading":"Prerequisites/scope","text":"notes, prior R computer science knowledge assumed; everything taught scratch.consult notes, please keep mind aim teach everything need know topic, rather equip foundational understanding encourage learn explore . , typically cover basic usage operations demonstrate key examples, leaving details practice.Also note occasionally bonus/extra/aside content considered advanced knowledge may mentioned passing sake completeness discussing topic, considered outside scope need know.","code":""},{"path":"index.html","id":"how-to-use-this-book","chapter":"Welcome!","heading":"How to use this book","text":"’s tips get book.","code":""},{"path":"index.html","id":"organization","chapter":"Welcome!","heading":"Organization","text":"notes loosely organized following order topics:Setup,R crash course, rapidly bring speed basic R usage,Data exploration, introduce data exploration R,Data transformation, demonstrate common data cleaning techniques,Probability theory (progress), introduce basic probability theory,Inference (added soon), teach foundational inference techniques, specifically:\nInference means,\nInference proportions,\nInference regression.\nInference means,Inference proportions,Inference regression.also appendices additional info:Datasets: info sources preprocessing done data set examples,Cheat sheets: list cheat sheets various packages/programs used notes.","code":""},{"path":"index.html","id":"notes-layout","chapter":"Welcome!","heading":"Notes layout","text":"First, note table contents left chapter navigation bar right page. Use quickly navigate around notes. Also note search bar top corner; use search highlight keywords across entire site. smaller phone screens elements may collapse, fully visible wider laptop/tablet screens.notes pages mostly composed paragraph text (like one), code chunks (see ). also occasionally encounter additional reference links, embedded images, footnotes extra info, tables, elements.block code chunk. frequently annotated comments. Note can copy contents chunk using clipboard icon corner. Also note functions automatically link help pages usage notes, argument explanations, examples.Important notes, often warning common mistakes/errors, appear yellow alert boxes.Tips improving R understanding optimizing workflow sppear green alert boxes.","code":"\n# this is a code chunk; lines starting with # are comments\n# R code in here will be run and output shown below\nprint(\"Hello world!\")[1] \"Hello world!\""},{"path":"index.html","id":"source-code","chapter":"Welcome!","heading":"Source code","text":"notes open-sourced GitHub built using bookdown served GitHub pages, provides convenient, easily editable, reproducible workflow. page link “View source” page right-side navbar, want see ’s hood.Note code base primarily written R Markdown syntax, may include ordinary text, markdown code, YAML headers, R chunks, knitr tweaks, \\(\\LaTeX\\) formulae, pandoc elements (especially fenced divs braced attributes). auxiliary files, may even find HTML/CSS/jQuery snippets. obviously made read/understand, browse curiosity.","code":""},{"path":"index.html","id":"contributing","chapter":"Welcome!","heading":"Contributing","text":"work hard avoid errors, alas nothing perfect! notice errors, please consider contributing suggestion! can 2 ways. (Note: ways require GitHub account, make sure sign register first!1)Directly propose change GitHub pull request:\npage error, click “Edit page” right-side navbar.\nfirst time contributing project, asked “Fork repository”, .e. make copy.\nforking, make edits text editor window appears click “Commit changes…”. Make sure add brief, descriptive title, well additional necessary details description box. Note description box supports markdown syntax.\nNext, click “Propose changes”, click “Create pull request”. , make sure good title description, click “Create pull request” . Make sure leave “Allow edits maintainers” checked, can modify edit want!\ncan check status pull request (PR) PR tab repo.\nPR looks perfect, may immediately merge .\nPR good perfect, may make comments/edits eventually merging.\nPR isn’t par reason, may discuss , ask follow questions, simply close . close PR, don’t discouraged! ’re welcome make contributions, just make sure understand didn’t merge try make better PR next time!\n\npage error, click “Edit page” right-side navbar.first time contributing project, asked “Fork repository”, .e. make copy.forking, make edits text editor window appears click “Commit changes…”. Make sure add brief, descriptive title, well additional necessary details description box. Note description box supports markdown syntax.Next, click “Propose changes”, click “Create pull request”. , make sure good title description, click “Create pull request” . Make sure leave “Allow edits maintainers” checked, can modify edit want!can check status pull request (PR) PR tab repo.\nPR looks perfect, may immediately merge .\nPR good perfect, may make comments/edits eventually merging.\nPR isn’t par reason, may discuss , ask follow questions, simply close . close PR, don’t discouraged! ’re welcome make contributions, just make sure understand didn’t merge try make better PR next time!\nPR looks perfect, may immediately merge .PR good perfect, may make comments/edits eventually merging.PR isn’t par reason, may discuss , ask follow questions, simply close . close PR, don’t discouraged! ’re welcome make contributions, just make sure understand didn’t merge try make better PR next time!seems like much work, can also simply raise issue point error. Note since work , usually lower priority well-written PR, can easily merged single click.","code":""},{"path":"index.html","id":"acknowledgements","chapter":"Welcome!","heading":"Acknowledgements","text":"good time acknowledge people made contributions. Bret Larget original creator STAT 240 author first set STAT 240 notes, primary source inspiration many aspects notes. Cameron Jones also agreed help write practice materials notes evolve. Beyond , thanks also @jennamotto1, @ellasantoro, @isis-plesnikova, @Soham109, @mrschmidt9 also contributing repo (make successful PR get name list!).","code":""},{"path":"index.html","id":"future-ideas","chapter":"Welcome!","heading":"Future ideas","text":"list additional ideas future improvements notes, considered implementation unspecified future time (prioritized finishing first-pass writeup).dark mode?add exercises pageautomagic index generator using _common.R?glossary?DT datatable fancy printouts??","code":""},{"path":"setup.html","id":"setup","chapter":"1 Setup","heading":"1 Setup","text":"chapter take steps necessary fully setup computer. several things need :First, section 1.1 install R Rstudio.Next, section 1.2 install necessary R packages configure recommended Rstudio settings.Finally, section 1.3 setup organization system files.","code":""},{"path":"setup.html","id":"setup-install","chapter":"1 Setup","heading":"1.1 R/Rstudio setup","text":"using R Rstudio throughout course.R free open-source statistical computing software.Rstudio IDE (integrated development environment) makes developing R much easier.\nprefer, can also use different IDE like Visual Studio Jupyter.\nprefer, can also use different IDE like Visual Studio Jupyter.R Rstudio two different programs need installed! Also, previously installed R Rstudio, highly recommended first completely uninstall reinstalling latest version avoid conflicts (instructions Windows Mac).First, install latest R release, version 4.5.1 released Jun 13, 2025; install Rstudio. instructions separated operating system, depending Windows Mac machine.Linux system, follow one linked instructions .Chromebook, try steps.","code":""},{"path":"setup.html","id":"setup-win","chapter":"1 Setup","heading":"1.1.1 Windows instructions","text":"Download R-4.5.1-win.exe run , accepting default settings.Download R-4.5.1-win.exe run , accepting default settings.Download Rstudio-latest.exe run , accepting default settings.Download Rstudio-latest.exe run , accepting default settings.Sometimes, R may need recompile package installation, require Rtools utility. download right version, check system’s page look “System type” line.\nshows “… x64-based processor”, download Rtools installer run , accepting default settings.\nshows “… ARM-based processor”, download Rtools installer run , accepting default settings.\nSometimes, R may need recompile package installation, require Rtools utility. download right version, check system’s page look “System type” line.shows “… x64-based processor”, download Rtools installer run , accepting default settings.shows “… ARM-based processor”, download Rtools installer run , accepting default settings.Now, R Rstudio setup. check installation, find “Rstudio” start menu run . asked choose installation, just accept default click OK.get window looks like , ’re set ready move next section!","code":""},{"path":"setup.html","id":"setup-mac","chapter":"1 Setup","heading":"1.1.2 Mac instructions","text":"First, need determine right R installer file specific machine. Open Apple menu top left corner screen open “mac”.\nshows “Chip: Apple M1/M2/M3”, download R-4.5.1-arm64.pkg run , accepting default settings.\nshows “Processor: …Intel Core”, download R-4.5.1-x86_64.pkg run , accepting default settings.\nget error, check OS version window. ’s older (.e. <11) may need either upgrade OS download older version .\nshows “Chip: Apple M1/M2/M3”, download R-4.5.1-arm64.pkg run , accepting default settings.shows “Processor: …Intel Core”, download R-4.5.1-x86_64.pkg run , accepting default settings.get error, check OS version window. ’s older (.e. <11) may need either upgrade OS download older version .Now, download Rstudio-latest.dmg install . Note dmg virtual disk image file, need follow steps install :\nDouble click file open . mount virtual drive desktop open new Finder window.\nnew window, drag Rstudio icon Applications directory. install computer.\nOpen new Finder window, go Applications directory, find new Rstudio program drag dock easy access.\n(Optional) can now unmount virtual disk. Right click mounted virtual disk desktop choose “Unmount”, alternatively find mounted drive right side dock drag trash bin. can also delete .dmg install file.\nDouble click file open . mount virtual drive desktop open new Finder window.new window, drag Rstudio icon Applications directory. install computer.Open new Finder window, go Applications directory, find new Rstudio program drag dock easy access.(Optional) can now unmount virtual disk. Right click mounted virtual disk desktop choose “Unmount”, alternatively find mounted drive right side dock drag trash bin. can also delete .dmg install file.many () systems, two additional programs need installed everything run smoothly. recommended everyone install just case (’s harm didn’t actually need ).\nDownload XQuartz-2.8.5.pkg run , accepting default settings. installs tool Rstudio uses display certain outputs.\nUsing either Spotlight, Launchpad, Applications directory, open “Terminal” type copy line xcode-select --install hit enter, follow -screen instructions. may asked fingerprint password (note password show type, normal done security purposes).\nDownload XQuartz-2.8.5.pkg run , accepting default settings. installs tool Rstudio uses display certain outputs.Using either Spotlight, Launchpad, Applications directory, open “Terminal” type copy line xcode-select --install hit enter, follow -screen instructions. may asked fingerprint password (note password show type, normal done security purposes).machines, R Rstudio may blocked OS overabundance caution. Follow steps unblock try .Now, assuming everything went smoothly, R Rstudio setup. check installation, find “Rstudio” Dock Applications directory run . asked choose installation, just accept default click OK.get window looks like , ’re set!","code":""},{"path":"setup.html","id":"setup-packs-config","chapter":"1 Setup","heading":"1.2 Packages/config","text":"continuing, make sure can open Rstudio correct version (R-4.5.1) installed!Next, install necessary packages configure recommended options improve workflow.Open Rstudio. Console window, type copy line install.packages(c(\"tidyverse\",\"rmarkdown\")) hit enter.\nRstudio asks whether “use personal library”, choose yes.\nRstudio asks whether “install source”, first try choosing work people. fails, try repeating step 1, time choose yes.\nMake sure see either “successfully unpacked” “downloaded binary packages ” console messages confirm installation succeeded.Open Rstudio. Console window, type copy line install.packages(c(\"tidyverse\",\"rmarkdown\")) hit enter.Rstudio asks whether “use personal library”, choose yes.Rstudio asks whether “install source”, first try choosing work people. fails, try repeating step 1, time choose yes.Make sure see either “successfully unpacked” “downloaded binary packages ” console messages confirm installation succeeded.Next, menu bar top, go “Tools” > “Global Options”.\nfirst page, “Workspace” section, set following:\nTurn “Restore .RData workspace startup”\nensures every time close reopen Rstudio start fresh session uncluttered junk previous sessions.\n\nChange “Save workspace .Rdata exit:” “Never”\nstops Rstudio asking shutdown want save session, also preventing clutter.\n\n\nNext, “R Markdown” page Options window, set following:\nChange “Show output preview :” “Viewer Pane”\nimproves workflow displaying plots Viewer pane instead opening new window.\n\nTurn “Show output inline R Markdown documents”\nalso improves workflow displaying output console instead inline documents.\n\n\n(Optional) wish customize interface, can go “Appearance” page change font theme.2\nPress OK save changes.\nLOTS options can feel free explore later , now move .Next, menu bar top, go “Tools” > “Global Options”.first page, “Workspace” section, set following:\nTurn “Restore .RData workspace startup”\nensures every time close reopen Rstudio start fresh session uncluttered junk previous sessions.\n\nChange “Save workspace .Rdata exit:” “Never”\nstops Rstudio asking shutdown want save session, also preventing clutter.\n\nTurn “Restore .RData workspace startup”\nensures every time close reopen Rstudio start fresh session uncluttered junk previous sessions.\nensures every time close reopen Rstudio start fresh session uncluttered junk previous sessions.Change “Save workspace .Rdata exit:” “Never”\nstops Rstudio asking shutdown want save session, also preventing clutter.\nstops Rstudio asking shutdown want save session, also preventing clutter.Next, “R Markdown” page Options window, set following:\nChange “Show output preview :” “Viewer Pane”\nimproves workflow displaying plots Viewer pane instead opening new window.\n\nTurn “Show output inline R Markdown documents”\nalso improves workflow displaying output console instead inline documents.\n\nChange “Show output preview :” “Viewer Pane”\nimproves workflow displaying plots Viewer pane instead opening new window.\nimproves workflow displaying plots Viewer pane instead opening new window.Turn “Show output inline R Markdown documents”\nalso improves workflow displaying output console instead inline documents.\nalso improves workflow displaying output console instead inline documents.(Optional) wish customize interface, can go “Appearance” page change font theme.2Press OK save changes.LOTS options can feel free explore later , now move .Due poor design, need repeat previous R Markdown configuration steps Rstudio duplicates settings another menu.\nmenu bar top, go “File” > “New File” > “R Markdown”. Ignore options new window click “OK” create new R Markdown file (learn R Markdown files later).\nnew editor pane, open gear-icon dropdown menu make sure reselect options (highlighted red).\nDue poor design, need repeat previous R Markdown configuration steps Rstudio duplicates settings another menu.menu bar top, go “File” > “New File” > “R Markdown”. Ignore options new window click “OK” create new R Markdown file (learn R Markdown files later).new editor pane, open gear-icon dropdown menu make sure reselect options (highlighted red).Now R/Rstudio properly setup.","code":""},{"path":"setup.html","id":"setup-files","chapter":"1 Setup","heading":"1.3 File organization","text":"lot files keep track course, highly recommended create neat directory structure computer help organize files. recommend directory structure similar :“Directory” synonymous “folder”. technical spaces, “directory” often preferred term.tree diagram shows somewhere computer (“..”) make “STAT240” directory. Inside, directories like “homework” “discussion” subdirectory specific assignment (e.g. “hw01”, “hw02”, “ds01”, “ds02”, etc..).also recommend creating directories “data”, place datasets download; “notes”, can place notes download take; “project”, can place project files assigned later. necessary, can also create directories discretion.Make sure STAT240 directory backed cloud synchronized app like OneDrive, Box, iCloud. programs sometimes interfere R/Rstudio may also cut larger datasets.recommend pick different parent directory (“..”) “Desktop” “Documents” since often backed default many modern systems. good alternative either “Downloads” directory usually backed , user’s home directory (instructions Windows Mac).creation, can make directory convenient access making shortcut desktop, pinning somewhere accessible like Quick Access Windows Dock Macs.","code":"..\n└── STAT240/\n    ├── data/\n    ├── discussion/\n    │   ├── ds01/\n    │   ├── ds02/\n    │   ├── ds03/\n    │   :    :\n    ├── homework/\n    │   ├── hw01/\n    │   ├── hw02/\n    │   ├── hw03/\n    │   :    :\n    ├── notes/\n    └── project/"},{"path":"setup.html","id":"setup-troubleshooting-faq","chapter":"1 Setup","heading":"1.4 Setup troubleshooting FAQ","text":"**FAQ added collect commonly encountered problems**","code":""},{"path":"rstudio-intro.html","id":"rstudio-intro","chapter":"2 Intro to R/Rstudio","heading":"2 Intro to R/Rstudio","text":"chapter introduce basics Rstudio help develop workflow testing R code producing beautiful R Markdown documents, using throughout semester.","code":""},{"path":"rstudio-intro.html","id":"rstudio-why","chapter":"2 Intro to R/Rstudio","heading":"2.1 Why Rstudio?","text":"Rstudio free open-source IDE (integrated development environment) designed help facilitate development R code. course don’t need (can write R code using text editor execute terminal) using Rstudio gives access host modern conveniences, just name :R code completion & highlightingeasy access interpreter console, plots, history, help, etc.integration scientific communication tools (e.g. R Markdown, Shiny, etc.)robust debugging toolseasy package/environment managementcustom project workflows (e.g. building websites, presentations, packages, etc.)GitHub SVN integrationand much …learn small fraction Rstudio offer course. always, encouraged explore .","code":""},{"path":"rstudio-intro.html","id":"rstudio-interface","chapter":"2 Intro to R/Rstudio","heading":"2.2 Rstudio interface","text":"default interface setup Rstudio. course can customize , see ---box:brief description purpose tab. ones using frequently course highlighted bold.section find Console, Terminal, Background Jobs tabs\nConsole: arguably important tab Rstudio. provides direct access R interpreter, allowing run code see outputs.\nuseful tips working console:\ncan use TAB key autocomplete code type. works built functions, user-defined objects, even file paths (later).\n\nhighly recommended use TAB often can, can save keystrokes, helps avoid typos!\n\ncan also easily rerun previously executed commands either using ↑ ↓ arrow keys navigate history, even search history using CTRL+R ⌘+R.\n\nTerminal: tab opens terminal current working directory (later). default, Git Bash terminal Windows zsh terminal Macs, can easily changed Options menu.\nBackground Jobs: Rstudio may sometimes run certain operations background jobs . Alternatively, can also run background R scripts desire.\nConsole: arguably important tab Rstudio. provides direct access R interpreter, allowing run code see outputs.\nuseful tips working console:\ncan use TAB key autocomplete code type. works built functions, user-defined objects, even file paths (later).\n\nhighly recommended use TAB often can, can save keystrokes, helps avoid typos!\n\ncan also easily rerun previously executed commands either using ↑ ↓ arrow keys navigate history, even search history using CTRL+R ⌘+R.\nConsole: arguably important tab Rstudio. provides direct access R interpreter, allowing run code see outputs.useful tips working console:can use TAB key autocomplete code type. works built functions, user-defined objects, even file paths (later).highly recommended use TAB often can, can save keystrokes, helps avoid typos!can also easily rerun previously executed commands either using ↑ ↓ arrow keys navigate history, even search history using CTRL+R ⌘+R.Terminal: tab opens terminal current working directory (later). default, Git Bash terminal Windows zsh terminal Macs, can easily changed Options menu.Terminal: tab opens terminal current working directory (later). default, Git Bash terminal Windows zsh terminal Macs, can easily changed Options menu.Background Jobs: Rstudio may sometimes run certain operations background jobs . Alternatively, can also run background R scripts desire.Background Jobs: Rstudio may sometimes run certain operations background jobs . Alternatively, can also run background R scripts desire.section B Environment, History, Connections, Tutorial tabs\nEnvironment: probably second important tab Rstudio. created variables, defined functions, imported datasets, objects used current session appear , along brief descriptions .\nbroomstick icon tab can used clear current session environment, removing defined objects. basically equivalent restarting Rstudio.\nNext broomstick, ’s icon shows memory usage current R session, well “Import Dataset” option can help load datasets, although primarily try write code hand learning purposes.\n\nHistory: tab, find history previous commands current session, want edit rerun something. can also search history, search just current session previous commands anytime opened Rstudio past.\nConnections/Tutorial: can start special data connections, find extra R tutorials wish.\nEnvironment: probably second important tab Rstudio. created variables, defined functions, imported datasets, objects used current session appear , along brief descriptions .\nbroomstick icon tab can used clear current session environment, removing defined objects. basically equivalent restarting Rstudio.\nNext broomstick, ’s icon shows memory usage current R session, well “Import Dataset” option can help load datasets, although primarily try write code hand learning purposes.\nbroomstick icon tab can used clear current session environment, removing defined objects. basically equivalent restarting Rstudio.Next broomstick, ’s icon shows memory usage current R session, well “Import Dataset” option can help load datasets, although primarily try write code hand learning purposes.History: tab, find history previous commands current session, want edit rerun something. can also search history, search just current session previous commands anytime opened Rstudio past.Connections/Tutorial: can start special data connections, find extra R tutorials wish.section C Files, Plots, Packages, Help, Viewer, Presentation tabs.\nFiles: tab gives small, integrated file explorer. can navigate around computer, create/delete/rename files, copy/move files, etc.\nPlots: plots/graphs make show (set options right, otherwise may show pop-). useful features know:\ntop left corner tab, left/right arrows navigating previous next plots (made multiple plots)\nNext , ’s “Zoom” button opens plot larger window.\nNext, ’s “Export” button allows export plot image/pdf/clipboard. opens window additional export options.\nNext, buttons remove current plot, clear plots.\n\nPackages: can view/install/update/load/unload packages. Note can also install packages using console, like previous section.\nHelp: one useful tabs Rstudio. , can access built R help pages. one FIRST places visit help R functions/objects.\nseveral ways access help pages. Suppose want help install.packages() function. can either run ?install.packages help(install.packages) console, put cursor function code hit F1 key.\nhelp page may contain following sections, presents different types information:\nDescription, showing brief summary purpose function\nUsage, listing available arguments (.e. options). argument = sign value, denotes default value\nArguments, details arguments can found\nDetails, details function can found\nValue, gives info function returns output\nSometimes, sections may appear specialized info\nend, may also find advanced notes, links related functions, additional references, example code demos.\n\nleast briefly scan help page time encounter new function. often several different ways use function depending /arguments set, can prevent needing “reinvent wheel” , example, trying manually change output format ’s already built-way output desired format.\n\n\nViewer: preview Rmd document output appear knitting (learn soon).\ntop corner, buttons letting clear current viewer items, well button open viewer new window default web browser, can also useful sometimes checking work printing/exporting.\n\nPresentation: final tab useful ever make presentations Rstudio, e.g. using R’s Beamer reveal.js integrations.\nFiles: tab gives small, integrated file explorer. can navigate around computer, create/delete/rename files, copy/move files, etc.Plots: plots/graphs make show (set options right, otherwise may show pop-). useful features know:\ntop left corner tab, left/right arrows navigating previous next plots (made multiple plots)\nNext , ’s “Zoom” button opens plot larger window.\nNext, ’s “Export” button allows export plot image/pdf/clipboard. opens window additional export options.\nNext, buttons remove current plot, clear plots.\ntop left corner tab, left/right arrows navigating previous next plots (made multiple plots)Next , ’s “Zoom” button opens plot larger window.Next, ’s “Export” button allows export plot image/pdf/clipboard. opens window additional export options.Next, buttons remove current plot, clear plots.Packages: can view/install/update/load/unload packages. Note can also install packages using console, like previous section.Help: one useful tabs Rstudio. , can access built R help pages. one FIRST places visit help R functions/objects.\nseveral ways access help pages. Suppose want help install.packages() function. can either run ?install.packages help(install.packages) console, put cursor function code hit F1 key.\nhelp page may contain following sections, presents different types information:\nDescription, showing brief summary purpose function\nUsage, listing available arguments (.e. options). argument = sign value, denotes default value\nArguments, details arguments can found\nDetails, details function can found\nValue, gives info function returns output\nSometimes, sections may appear specialized info\nend, may also find advanced notes, links related functions, additional references, example code demos.\n\nleast briefly scan help page time encounter new function. often several different ways use function depending /arguments set, can prevent needing “reinvent wheel” , example, trying manually change output format ’s already built-way output desired format.\n\nseveral ways access help pages. Suppose want help install.packages() function. can either run ?install.packages help(install.packages) console, put cursor function code hit F1 key.help page may contain following sections, presents different types information:\nDescription, showing brief summary purpose function\nUsage, listing available arguments (.e. options). argument = sign value, denotes default value\nArguments, details arguments can found\nDetails, details function can found\nValue, gives info function returns output\nSometimes, sections may appear specialized info\nend, may also find advanced notes, links related functions, additional references, example code demos.\n\nleast briefly scan help page time encounter new function. often several different ways use function depending /arguments set, can prevent needing “reinvent wheel” , example, trying manually change output format ’s already built-way output desired format.\nDescription, showing brief summary purpose functionUsage, listing available arguments (.e. options). argument = sign value, denotes default valueArguments, details arguments can foundDetails, details function can foundValue, gives info function returns outputSometimes, sections may appear specialized infoAt end, may also find advanced notes, links related functions, additional references, example code demos.least briefly scan help page time encounter new function. often several different ways use function depending /arguments set, can prevent needing “reinvent wheel” , example, trying manually change output format ’s already built-way output desired format.Viewer: preview Rmd document output appear knitting (learn soon).\ntop corner, buttons letting clear current viewer items, well button open viewer new window default web browser, can also useful sometimes checking work printing/exporting.\ntop corner, buttons letting clear current viewer items, well button open viewer new window default web browser, can also useful sometimes checking work printing/exporting.Presentation: final tab useful ever make presentations Rstudio, e.g. using R’s Beamer reveal.js integrations.concludes tour basic Rstudio interface. also file editor window (also known source panel/window) discuss later , now, let’s learn run basic R commands!","code":""},{"path":"rstudio-intro.html","id":"r-basics","chapter":"2 Intro to R/Rstudio","heading":"2.3 Basics of R","text":"section, give brief introduction working R. prior coding experience assumed. highly encouraged copy run examples read. time capacity, also encouraged peruse linked help pages extra reference links, mandatory.","code":""},{"path":"rstudio-intro.html","id":"r-running","chapter":"2 Intro to R/Rstudio","heading":"2.3.1 Running R code","text":"main way run R code type copy console. Comments can written hashtag # run. output, displayed either console directly ’s text, plot window ’s visual. notes, output shown separate box .Try running examples console observe output:[1] appears start output line just means first output value. bracketed numbers part actual output ignored.functions notes automagically link online help pages (help pages inside Rstudio saw previous section). Try clicking barplot() function previous code block see help page.","code":"\n# this line is a comment and will not be run\n# use the copy button in the top corner to easily run this example --->\nprint(\"output is shown here\") # you can also add comments here[1] \"output is shown here\"\n# a VERY simple barplot\n# the c() function creates a vector of numbers\nbarplot(c(1, 3, 2))"},{"path":"rstudio-intro.html","id":"r-math","chapter":"2 Intro to R/Rstudio","heading":"2.3.2 Basic math","text":"One first things learn R use calculator basic math. Operators like +, -, *, /, ^, parentheses ( ) work just like ’d expect. Note R respects standard order operations (see page operation order details).R sometimes output scientific notation, especially number exactly numerically represented due limitations computers. example, compute 2^50 R show result 1.1259e+15, .e. 1.1259×1015. can also type 1.1259e+15 R understand 1.1259×1015.Also note due limitations computers represent numbers, R often distinguish numbers differ less 10-15, .e. 0.000000000000001.3You can also perform integer division R, .e. dividing get quotient remainder, using %/% quotient %% remainder. allows check, example, number even odd.Operators like %/% %% may seem strange first, work just like binary operators R + ^ . examples operators like %% %>% learn later.Trigonometric functions sin(), cos(), tan(), (inverses asin(), acos(), atan() – prefix means arc–), also work ’d expect use radian units. Note pi conveniently predefined \\(\\pi\\). Hyperbolic trig functions also exist need .Exponential logarithm functions exp() log(), also work ’d expect default natural base \\(e\\). Note log function optional base argument using different base. also special base 10 2 versions log10() log2() .Additionally, abs() computes absolute value sqrt() square root (note convention, positive root returned). Taking square root negative number return NaN.","code":"\n(-5 * -3^2 + 4) / 7 - 6[1] 1\n2^50[1] 1.1259e+15\n13 %/% 2[1] 6\n13 %% 2[1] 1\ncos(2 * pi)[1] 1\natan(-1) * 4[1] -3.141593\nlog(exp(2)) * log10(100)[1] 4\nlog(3^5, base = 3)[1] 5\nsqrt(abs(-9))[1] 3"},{"path":"rstudio-intro.html","id":"aside-function-arguments","chapter":"2 Intro to R/Rstudio","heading":"2.3.3 Aside: function arguments","text":"quick aside function “arguments” (.e. additional options can set). R, arguments functions names (see function’s help page details) depending function needed, might set explicitly.previous section, used base = 3 inside log() function explicitly set base argument 3. However, log(3^5, 3) also work, since base second argument (, see help page). Without explicit naming, arguments passed order function.Explicitly naming argument often used clarify teaching purposes, improve debugging legibility, skip certain preceding arguments either unnecessary whose default values acceptable. E.g. suppose function f() 3 arguments, , b, c order. wish set =0 c=1 leaving b blank, can write f(0, c=1).","code":""},{"path":"rstudio-intro.html","id":"r-specials","chapter":"2 Intro to R/Rstudio","heading":"2.3.4 Special values","text":"already mentioned pi predefined. important special values R. TRUE FALSE, along abbreviations T F also predefined. Note capitalization; R case-sensitive language true, True, t TRUE (first two defined, t() matrix transpose function).important thing note R, kind math turns TRUE 1 FALSE 0.Mathematical expressions may also return NaN Number, .e. undefined; Inf infinite. Note R differentiates positive infinity Inf negative infinity -Inf.Additionally, NA used represent missing values, .e. data available. Note NA NaN . learn later handle NA missing values.","code":"\nT[1] TRUE\ntrueError: object 'true' not found\nexp(FALSE) * (TRUE + sqrt(TRUE))[1] 2\nsqrt(-4)[1] NaN\n1 / 0[1] Inf\nlog(0)[1] -Inf"},{"path":"rstudio-intro.html","id":"assignment","chapter":"2 Intro to R/Rstudio","heading":"2.3.5 Assignment","text":"R, variables typically assigned using <- operator, just less < minus - put together. can also use = <- recommended stylistic reasons (see blog post details). class, acceptable prefer <- notes.4You can quickly insert assignment <- operator ALT+- Windows ⌥+- Mac.Generally, = reserved setting arguments inside functions, e.g. like previous code chunk computed log() custom base setting base argument.Variable names can combination upper lower-case letters, numbers, period . underscore _ (treated similar letters), one caveat: variables must begin letter period, number underscore. may use characters variable names.5Observe results expression saved variable, R print default. often confusing first time R users, since may seem like nothing happened. example, running following:produces visible output. normal behavior, since output redirected variable. inspect result, must explicitly call print() object :","code":"\n# this is preferred\nx <- 5\nprint(x)[1] 5\n# this is equivalent and acceptable, but discouraged\nx = 5\nprint(x)[1] 5\nlog(3^5, base = 3)[1] 5\n# these variable names are ok,\n# also remember R is case sensitive!\nvar1 <- 1\nVar1 <- 2\n.OtherVariable <- 3\nanother.variable_42 <- 4\n\nvar1 + Var1 + .OtherVariable + another.variable_42[1] 10\n# even these morse code looking variable names,\n# while not recommended, are technically ok:\n. <- 1\n.. <- 2\n._ <- 3\n._..__ <- 4\n\n. + .. + ._ + ._..__[1] 10# these variable names will raise errors:\n#   1var, _var, bad-var,   e.g.:\n1var <- 1Error: unexpected symbol in \"1var\"\nresult <- 3 * 4 - 5\nresult[1] 7"},{"path":"rstudio-intro.html","id":"summary-functions","chapter":"2 Intro to R/Rstudio","heading":"2.3.6 Summary functions","text":"far, ’ve seen functions run individual values, also functions R run summarize dataset. often statistical nature. give brief summary compute R. -depth discussion meaning applications saved later course.Suppose gather sample observe following values: 3, 6, 6, 2, 4, 1, 5 (don’t worry mean, ’re just using demo). can create vector using c() function store data save :sum() length() functions work like expect produce sum length sample. can use compute mean sample, can also done directly using mean().can also find median (.e. middle number) median() function. (Sadly, ’s built-mode function R, can achieved packages.)can generalize median (50-th percentile) compute percentile using quantile() function, e.g. suppose want compute 30-th percentile:standard deviation another important statistic (think distance average observation mean) can computed using sd(). Note equivalent square root variance can found var().can also find min() max() sample (together give us range() dataset).Another important function working samples %% operator, lets us check value exists dataset.One final statistical function extremely common cor() computes correlation 2 vectors. E.g. suppose following \\((x,y)\\) points: (3.4,1), (5,4.6), (5.7,6.8), (6.5,5.3). can compute correlation \\(x\\) \\(y\\) points like :miscellaneous functions working vectors sometimes useful won’t cover detail now, can explore , prod() computing product numbers vector, sort() sorting vector, rev() reversing vector, unique() getting unique values vector, scale() linearly shifting scaling data mean 0 standard deviation 1, cumsum() cumprod() cumulative sum product along vector, many many …","code":"\ndata <- c(3, 6, 6, 2, 4, 1, 5)\ndata[1] 3 6 6 2 4 1 5\nsum(data) / length(data)[1] 3.857143\nmean(data)[1] 3.857143\nmedian(data)[1] 4\nquantile(data, 0.3)30% \n2.8 \nsqrt(var(data))[1] 1.9518\nsd(data)[1] 1.9518\nmin(data)[1] 1\nmax(data)[1] 6\n# 6 appears in the sample\n6 %in% data[1] TRUE\n# 7 does not appear in the sample\n7 %in% data[1] FALSE\nx <- c(5, 6.5, 3.4, 5.7)\ny <- c(4.6, 5.3, 1, 6.8)\ncor(x, y)[1] 0.8690548"},{"path":"rstudio-intro.html","id":"logical-comparison","chapter":"2 Intro to R/Rstudio","heading":"2.3.7 Logical comparison","text":"Finally, let’s learn basic logical comparisons. crucial data cleaning filtering operations later .R, equality comparison done using == operator. Note double equal sign; single equal assignments arguments. Inequality can checked using !=.Note ! used individually operator, .e. turns TRUE FALSE vice versa.Inequalities done using <, <=, >, >=, less (equal ) greater (equal ).Logical statements can chained together using & operator well | operator (keyboards, vertical bar character typed using SHIFT+\\).& return true expressions sides true; | return true least one expressions sides true. Note & appears higher R’s order operations |.can course chain together R commands compare complicated expressions. sky limit!Since computers don’t infinite precision, arithmetic operations can introduce small errors, especially producing repeating-decimal irrational numbers:imprecisions usually result errors 10-15 less. Generally values around magnitude R treated indistinguishable 0. comparing inexact values like , ’s recommended use .equal() instead ==, allows small tolerance.","code":"\nx <- (2 + 3)^2\nx == 25[1] TRUE\n# if instead we ask \"is x not equal to 25\", we should get FALSE\nx != 25[1] FALSE\n!TRUE[1] FALSE\n# this is equivalent to x != 25\n!(x == 25)[1] FALSE\nx < 30[1] TRUE\nx >= 25[1] TRUE\n(x > 20) & (x <= 30)[1] TRUE\n(x > 20) | (x != 25)[1] TRUE\n# check if x² is even OR if mean of data + 2 * sd is greater than the max\n# note order of operations means we don't need extra parentheses\n# of course, you can add extra parentheses for readability if you wish!\nx^2 %% 2 == 0 | mean(data) + 2 * sd(data) > max(data)[1] TRUE\n1/2 + 1/3 == 5/6[1] FALSE\n1/2 + 1/3 - 5/6[1] -1.110223e-16\nsqrt(2)^2 == 2[1] FALSE\nsqrt(2)^2 - 2[1] 4.440892e-16\nall.equal(1/2 + 1/3, 5/6)[1] TRUE\nall.equal(sqrt(2)^2, 2)[1] TRUE"},{"path":"rstudio-intro.html","id":"packages","chapter":"2 Intro to R/Rstudio","heading":"2.3.8 Packages","text":"Now, let’s briefly discuss packages. One best features R ability anyone easily write distribute packages CRAN (Comprehensive R Archive Network). Currently, 23032 packages available CRAN. also 2361 packages bioinformatics-specific package archive Bioconductor, well countless GitHub.course, primarily make use Tidyverse suite packages, contains several important packages data science: readr reading data, ggplot2 plotting data, dplyr tidyr cleaning data, lubridate stringr working dates strings. learn course progresses.important thing need remember packages :install ; load dailyI.e. need install package computer, need load every time reopen Rstudio want use (unless ’re one people never closes programs). course want setup R/Rstudio new computer, need install well.","code":""},{"path":"rstudio-intro.html","id":"install-a-package","chapter":"2 Intro to R/Rstudio","heading":"2.3.8.1 Install a package","text":"Unless ’re extremely niche work, generally packages want use CRAN can easily installed one time running following.’s important check output messages see install successful, , find important “Error:…” keywords use troubleshooting. Sometimes R ask different things install:R asks use “personal library”, say yes. just means can’t store package files system directory due system permissions, store somewhere else (typically user directory).R asks install “source”, try first; fails, retry yes. just means want R prioritize using precompiled executable files installing, generally much faster.R asks update existing packages installing new package, entirely . like update packages regularly, ’s usually harm don’t want update immediately.\ntry update packages currently loaded, R may ask first “restart R”, usually good idea.\ntry update packages currently loaded, R may ask first “restart R”, usually good idea.","code":"\n# you should have already installed tidyverse from last chapter\n# note the package name MUST be in quotes\ninstall.packages(\"tidyverse\")"},{"path":"rstudio-intro.html","id":"loading-a-package","chapter":"2 Intro to R/Rstudio","heading":"2.3.8.2 Loading a package","text":"can load package either library() require(), basically .6 package names actually load group packages, e.g. library(tidyverse) load “core” Tidyverse packages, include ggplot2, dplyr, tidyr, readr, purrr, tibble, stringr, forcats.Upon loading, many packages print various diagnostic messages console. generally completely ignorable. Sometimes warn “Conflicts”, standard just means overridden default functions. E.g. can see filter() function package dplyr overwritten pre-loaded filter() function stats package.may already guessed message output , can also access function another package without loading entire package using syntax package::function(). often done either avoid name conflicts clarify reader functions come packages.","code":"\nlibrary(tidyverse)── Attaching core tidyverse packages ────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.6\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.1     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.2.0     \n── Conflicts ──────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors"},{"path":"rstudio-intro.html","id":"whitespace","chapter":"2 Intro to R/Rstudio","heading":"2.3.9 Whitespace","text":"final topic, let’s briefly discuss spacing. “Whitespace” refers sequence space-type characters, can mix spaces, tabs, line breaks (.e. hit ENTER).R ignores whitespace variable names, functions, punctuation characters. E.g. following equivalent:long line code often broken across several lines readability. see many examples shortly data visualization chapter.However, make sure break line finish line eventually, otherwise ’ll get error. example, type mean(data console forget close parenthesis (try !) see prompt character > replaced +. continue , patiently waiting finish line, either close typing ) cancel line hitting ESC.","code":"\n# these are all the same\nmean(data)[1] 3.857143\nmean ( data )[1] 3.857143\nmean (\n  data\n)[1] 3.857143"},{"path":"rstudio-intro.html","id":"r-cheat-sheets","chapter":"2 Intro to R/Rstudio","heading":"2.3.10 R cheat sheets","text":"probably important R commands need know now. curated short selection R “cheat sheets” reference need , rough order useful think first time R learner.Matt Baggott’s R Reference Card v2.0 nice complete one-stop-shop R’s built-functions.IQSS’s Base R Cheat Sheet Alexey Shipunov’s One Page R Reference Card slightly shorter curated, offer nice, tighter set critical R commands, along useful examples syntax.slightly longer complete reference manual R, especially details R works different object types data structures, Emmanuel Paradis’s R Beginners may helpful.","code":""},{"path":"rstudio-intro.html","id":"r-markdown","chapter":"2 Intro to R/Rstudio","heading":"2.4 R Markdown","text":"next section, introduce R Markdown, document format allows seamlessly organize integrate text R code/output easily readable editable way. supports many output file types including HTML, PDF, DOCX, can used write reports, articles, presentations, ebooks, even websites (fact, entire website written R Markdown, GitHub repo even maintained using Rstudio; can view source code page using “View source” button right sidebar).Let’s start example! basic R Markdown demo file called demo.Rmd, produces demo.html output. use example learn work Rmd files.","code":"---\ntitle: \"Demo Rmd file\"\nauthor: \"Jane Doe\"\ndate: \"2024-06-20\"\noutput: html_document\n---\n\n```{r setup, include=FALSE}\n# this is a standard \"setup\" chunk usually found at the top of Rmd files,\n# often used for setting options, loading files, and importing libraries\nknitr::opts_chunk$set(echo = TRUE)\nlibrary(tidyverse)\n```\n\n# Section 1\n\n## Subsection A\n\nHere's some ordinary text. You can use Markdown syntax to add more features, e.g. here's a [link](https://markdownguide.org/cheat-sheet), here's some **bold text**, and here's some `inline code`. You can also add images, footnotes, blockquotes, and more. See linked cheat sheet above for more.\n\n 1. Lists are also east to add!\n 2. Here's a second item.\n 3. You can even add sublists:\n    - Here's a sublist with bullets.\n    - Another bullet?\n\n## Subsection B\n\nYou can easily incorporate R code into an Rmd file, with outputs and plots that auto-update. Here's an example code chunk named \"chunk1\".\n\n```{r example-chunk, fig.height=5, fig.align=\"center\"}\ndata <- c(3, 6, 6, 2, 4, 1, 5)\nmean(data)\nhist(data)\n```\n\nYou can even refer to R objects inside text, e.g. the sample mean and standard deviation are `r mean(data)` and `r sd(data)`.\n\n# Section 2\n\nHere's a second section.\n\n<!-- comments in an Rmd file must use HTML-style syntax -->"},{"path":"rstudio-intro.html","id":"source-window","chapter":"2 Intro to R/Rstudio","heading":"2.4.1 Source window","text":"Download demo.Rmd example file open ; automagically open Rstudio new panel top left called source window, actually just basic text editor like Notepad TextEdit, additional R-aware features (later).","code":""},{"path":"rstudio-intro.html","id":"knitting","chapter":"2 Intro to R/Rstudio","heading":"2.4.2 Knitting","text":"first thing learn R Markdown “Knit”, generate output document. Think Rmd file “recipe” tells Rstudio create format nice output audience.top source window, find Knit button click . ’ll see bunch messages scroll new tab called “Render” Rstudio executes processes document. errors, Rstudio produce output document “demo.html” directory saved “demo.Rmd” open preview file “Viewer” tab.can also knit pressing CTRL+SHIFT+K Windows, either CTRL+SHIFT+K, ⌘+SHIFT+K Mac.run errors, look line keyword “Error: …”. Usually, searching error message favorite search engine good way diagnose problem.continue learning R Markdown , feel free play around demo Rmd file re-knit see resulting changes.","code":""},{"path":"rstudio-intro.html","id":"yaml-header","chapter":"2 Intro to R/Rstudio","heading":"2.4.3 YAML header","text":"Rmd files usually start YAML header important metadata file:Title, author, date self explanatory. output: option sets output format R uses knitting. highly recommend using default html_document output format since lightweight, portable, easy us view Canvas grading.lots YAML options can explore, minimally, always 4: title, author, date, output: html_document set beginning Rmd document.","code":"---\ntitle: \"Demo Rmd file\"\nauthor: \"Jane Doe\"\ndate: \"2024-06-20\"\noutput: html_document\n---"},{"path":"rstudio-intro.html","id":"markdown","chapter":"2 Intro to R/Rstudio","heading":"2.4.4 Markdown","text":"R Markdown based Markdown7\nsimple syntax “marking ” text additional formatting. can see mixed paragraphs ordinary text, # Section ## Subsection headings, [links](url) **bold text**, lists sublists, inline separate “chunks” source code.expect learn markdown, minimally learn use section subsection headings, links lists, inline code code chunks.","code":""},{"path":"rstudio-intro.html","id":"code","chapter":"2 Intro to R/Rstudio","heading":"2.4.5 Code","text":"two main ways include code R Markdown file: inline chunks.","code":""},{"path":"rstudio-intro.html","id":"inline-code","chapter":"2 Intro to R/Rstudio","heading":"2.4.5.1 Inline code","text":"want quote R code inside paragraph text, surround backtick character `, can found keyboards top left next 1 key. Note character single quote character ’. example, : `mean(data)` render : mean(data).can also easily refer R variables substitute values, apply functions display output inside text. example, remember data vector defined section 2.3.6? : `r median(data)`\nrender : 4. Note `r prefix , triggers evaluation substitute code. helps avoid “hard-coding”, letting values references update always stay sync.value many digits decimal, e.g. `r mean(data)`\nbecomes: 3.8571429, highly recommended round result reasonable number digits using either round() signif(). case, `r round(mean(data),2)`\nbecomes 3.86 much better.also important round END, present analysis. round original dataset intermediate value used another computation, introduce errors can compound.Generally, recommend rounding either precision data, 2-3 significant figures; picky exact number digits. See page discussion precision rounding.","code":""},{"path":"rstudio-intro.html","id":"code-chunks","chapter":"2 Intro to R/Rstudio","heading":"2.4.5.2 Code chunks","text":"prominent way include R code inside R Markdown document using -called code “chunks” “blocks”. basic structure :can quickly insert chunk using CTRL+ALT+Windows, either CTRL+⌥+, ⌘+⌥+Mac.code chunk basic structure:Chunks always start ```{r r indicates contain R code executed.Chunks always start ```{r r indicates contain R code executed.optionally followed space name chunk, e.g. example-chunk. name necessary, recommend 2 reasons:\ncode errors name chunks, R tell name chunk error, can help troubleshoot faster.\nR also use chunk names (along section headings) generate document outline bottom left source window. can click outline button quickly navigate another part long Rmd file.\noptionally followed space name chunk, e.g. example-chunk. name necessary, recommend 2 reasons:code errors name chunks, R tell name chunk error, can help troubleshoot faster.R also use chunk names (along section headings) generate document outline bottom left source window. can click outline button quickly navigate another part long Rmd file.name can also optionally followed comma , followed additional “chunk options”, extra settings can set control behavior chunk output. long list available options, short list important:\n\nOption\nPossible values (default values bold)\nDescription\neval\nTRUE, FALSE\nControls whether code chunk evaluated.\necho\nTRUE, FALSE\nControls whether code chunk echoed (.e. displayed). Note code can evaluated without echoed, echoed without evaluated, /neither.\ninclude\nTRUE, FALSE\nSetting FALSE run chunk, hide code output. often used “setup chunk” near top document import packages, load datasets, “setup tasks” want hide.\nerror\nTRUE, FALSE\nControls whether allow errors continue knitting. Note option FALSE default, meaning R halt produce output encounters errors.\nfig.width, fig.height\nnumber; default: 7, 5\ncontrol size plot output, one.\nfig.align\n“default”, “left”, “right”, “center”\ncontrols alignment plot output, one. Note option MUST set quotes. “default” set alignment.\ncache\nTRUE, FALSE\nchunk time consuming, can “cache” . Cached chunks rerun unless code inside modified. Note set FALSE default. option used caution! Improper usage may cause code chunks update properly.8\n\nOne last note: remember “setup” chunk top demo file? :\n\n```{r setup, include=FALSE}\n# standard \"setup\" chunk usually found top Rmd files,\n# often used setting options, loading files, importing libraries\nknitr::opts_chunk$set(echo = TRUE)\nlibrary(tidyverse)\n```\n\nfunction knitr::opts_chunk$set() can used set default chunk options chunks document, e.g. can center figures adding fig.align = \"center\" instead copying every chunk header.name can also optionally followed comma , followed additional “chunk options”, extra settings can set control behavior chunk output. long list available options, short list important:One last note: remember “setup” chunk top demo file? :function knitr::opts_chunk$set() can used set default chunk options chunks document, e.g. can center figures adding fig.align = \"center\" instead copying every chunk header.closing header } starting new line, can now put whatever code want inside chunk. code gets run one line time output displayed.closing header } starting new line, can now put whatever code want inside chunk. code gets run one line time output displayed.end, chunk closed another set ```.end, chunk closed another set ```.Remember can also use TAB autocomplete Rmd code chunks save keystrokes avoid typos! can even TAB autocomplete chunk options.concludes discussion code chunks R Markdown.","code":"```{r example-chunk, fig.height=5, fig.align=\"center\"}\ndata <- c(3, 6, 6, 2, 4, 1, 5)\nmean(data)\nhist(data)\n``````{r setup, include=FALSE}\n# this is a standard \"setup\" chunk usually found at the top of Rmd files,\n# often used for setting options, loading files, and importing libraries\nknitr::opts_chunk$set(echo = TRUE)\nlibrary(tidyverse)\n```"},{"path":"rstudio-intro.html","id":"aside-latex","chapter":"2 Intro to R/Rstudio","heading":"2.4.6 Aside: \\(\\LaTeX\\)","text":"also outside scope course, need learn , R Markdown natively supports \\(\\LaTeX\\) code well. ’s rendered using MathJax, open-source Javascript engine rendering equations online. example, $$x=\\frac{-b\\pm\\sqrt{b^2-4ac}}{2a}$$ becomes:\\[x=\\frac{-b\\pm\\sqrt{b^2-4ac}}{2a}\\]see lots \\(\\LaTeX\\) later notes need write math, just wanted mention . can right click equations see notes change MathJax display options, see source code (can course also see source code entire page using link sidebar mentioned previously).wish read \\(\\LaTeX\\), start Rong Zhuang’s MathJax cheat sheet David Richeson’s quick guide lots great beginner-friendly examples. slightly complete list symbols, Eric Torrence’s cheat sheet may also useful.","code":""},{"path":"rstudio-intro.html","id":"cheat-sheet","chapter":"2 Intro to R/Rstudio","heading":"2.4.7 Cheat sheet","text":"need good R Markdown cheat sheet, recommend reference guide published developers Rstudio. Page 1 Markdown syntax guide, pages 2-3 highlight useful chunk options, pages 4-5 additional info different output formats well additional YAML header options.","code":""},{"path":"rstudio-intro.html","id":"workflow","chapter":"2 Intro to R/Rstudio","heading":"2.5 Workflow","text":"final section, briefly discuss workflow considerations working Rstudio R Markdown important know troubleshooting purposes.","code":""},{"path":"rstudio-intro.html","id":"working-directory","chapter":"2 Intro to R/Rstudio","heading":"2.5.1 Working directory","text":"“working directory” concept first-time R users always struggle . Simply put, R always runs ’s inside directory. current directory R running inside called “working directory”. can check current working directory getwd() function:can see current Rstudio session (writing notes) running stat240-revamp directory located C:/Users/bi/Desktop.Generally, start new Rstudio session, working directory default C:/Users/username/ Windows /Users/username/ Mac (/home/username/ Linux), username account name.default working directory actually presents problem, usually different current Rmd file . example, suppose ’re working homework 1. organized files properly—!—Rmd file probably located .../STAT240/homework/hw01/hw01.Rmd. reasons explained next section, working directory always match location current Rmd file.can either methods:Recommended: Using top menu bar, go “Session” > “Set Working Directory” > “Source File Location”. sets working directory location current file edited.\nWindows users, shortcut ALT+S, release keys type W S one time.\nWindows Mac, can also setup custom shortcut action. top menu bar, go “Tools” > “Modify Keyboard Shortcuts…”, find “Set Working Directory Current Document’s Directory” set preferred shortcut. Mine set CTRL+SHIFT+D feel free choose .\nWindows users, shortcut ALT+S, release keys type W S one time.Windows Mac, can also setup custom shortcut action. top menu bar, go “Tools” > “Modify Keyboard Shortcuts…”, find “Set Working Directory Current Document’s Directory” set preferred shortcut. Mine set CTRL+SHIFT+D feel free choose .can also set automagically current Rmd file location try(setwd(dirname(rstudioapi::getSourceEditorContext()$path)),silent=T) can run either console, copied Rmd file (e.g. setup chunk) run whenever open file.can also set manually running setwd() console prefer.Whenever open Rstudio, switch different file, ALWAYS following:Set working directory,Load necessary packages,Read necessary datasets.","code":"\n# check current working directory\ngetwd()[1] \"C:/Users/bi/Desktop/stat240-revamp\""},{"path":"rstudio-intro.html","id":"knitting-v.-console-execution","chapter":"2 Intro to R/Rstudio","heading":"2.5.2 Knitting v. console execution","text":"need match working directory Rmd file location? difference knitting execution comes play.turns , code runs differently Rstudio console knit Rmd file. Code console always run current working directory, objects created added current session Environment. stay clear Environment.However, Knit document, create new R session background working directory set location Rmd file, run entire document scratch, top bottom produce output file. means working directory Rstudio set place, can break file references need load datasets.may seem overly complicated right now, quickly become intuitive practice .","code":""},{"path":"rstudio-intro.html","id":"tips","chapter":"2 Intro to R/Rstudio","heading":"2.5.3 Tips","text":"end chapter, want offer tips workflow find repeating nearly every student, especially errors arise.important tip avoiding/fixing errors knit often, check output! ?\nKnitting best way catch errors, often knit, easier identify source error (since ’s less new code check).\nKnitting automatically save document , helping avoid lost work due crashes.\ncan check formatting new document elements, whether bodies text, markdown features, plot outputs, code, etc..\nrun unexpected computer Rstudio problems, ’ll recently-knit output can submit interim troubleshoot.\nKnitting best way catch errors, often knit, easier identify source error (since ’s less new code check).Knitting automatically save document , helping avoid lost work due crashes.can check formatting new document elements, whether bodies text, markdown features, plot outputs, code, etc..run unexpected computer Rstudio problems, ’ll recently-knit output can submit interim troubleshoot.Another tip general, think Rstudio’s console place “test ” new line code ’re trying add. Continue test ’re satisfied , immediately copy Rmd. Work console saved! Rmd file work saved knit final output.\ncan easily run line code Rmd file console putting cursor anywhere line using CTRL+ENTER Windows either CTRL+ENTER ⌘+ENTER Mac. run entire chunk, add SHIFT previous shortcut combo. can also use top-right chunk shortcut buttons.\ncan easily run line code Rmd file console putting cursor anywhere line using CTRL+ENTER Windows either CTRL+ENTER ⌘+ENTER Mac. run entire chunk, add SHIFT previous shortcut combo. can also use top-right chunk shortcut buttons.Remember knitting always creates NEW, empty background R session, sets working directory Rmd file location, runs entire file top bottom. means:\nrun line console without copying Rmd file, line run knit.\ndefine object console, forgot copy Rmd file, try use somewhere else file, get error.\nObjects must defined used Rmd file. define data line 20 try computing mean(data) line 10, get error.9\nworking directory match location current Rmd file, may also cause errors need load datasets. Remember always set working directory!\nrun line console without copying Rmd file, line run knit.define object console, forgot copy Rmd file, try use somewhere else file, get error.Objects must defined used Rmd file. define data line 20 try computing mean(data) line 10, get error.9If working directory match location current Rmd file, may also cause errors need load datasets. Remember always set working directory!error? tried following?\nRead error message! Look “Error:…” read search browser! ’re knitting, also line number chunk name (remember name chunks!) help find problem code .\nCheck ’re using function correctly. Read built-help page function, search online example usages.\nCheck input object (often output earlier line chunk) actually correct. E.g. data wasn’t properly created chunk-1, might show error try compute mean(data) chunk-2 . paranoid! Check input output go along!\nimported lots packages, check function names conflicts. E.g. package1 package2 function func1, may accidentally using wrong one. can check also opening help page func1 checking see help page directs .\nstill can’t identify problem, restart R session going top menu bar > “Session” > “Restart R”, run entire Rmd ONE function time, checking function’s output along way. may take longer, almost always work.\ntried everything STILL can’t figure , ask us help.\nRead error message! Look “Error:…” read search browser! ’re knitting, also line number chunk name (remember name chunks!) help find problem code .Check ’re using function correctly. Read built-help page function, search online example usages.Check input object (often output earlier line chunk) actually correct. E.g. data wasn’t properly created chunk-1, might show error try compute mean(data) chunk-2 . paranoid! Check input output go along!imported lots packages, check function names conflicts. E.g. package1 package2 function func1, may accidentally using wrong one. can check also opening help page func1 checking see help page directs .still can’t identify problem, restart R session going top menu bar > “Session” > “Restart R”, run entire Rmd ONE function time, checking function’s output along way. may take longer, almost always work.tried everything STILL can’t figure , ask us help.Phew, lot, wraps introduction working R/Rstudio! , linked bonus readings want learn bit control looping R, need class may good know plan take data science classes build career data science.next chapter, explore data types structures R, well learn read write datasets.","code":""},{"path":"rstudio-intro.html","id":"optional-bonus-topic-controllooping","chapter":"2 Intro to R/Rstudio","heading":"Optional bonus topic: control/looping","text":"includes /else statements /loops. ’re interested, see suggested readings:page Yihui Xie bookdown documentation offers good & quick overview control looping R.-depth discussion, check page Hadley Wickham Advanced R book.","code":""},{"path":"data-vectors.html","id":"data-vectors","chapter":"3 Data Vectors","heading":"3 Data Vectors","text":"chapter, introduce handling data R, starting vectors. Vectors arguably fundamental data structure R. briefly saw example vectors last chapter section 2.3.6 summary functions:Last chapter, mostly used vectors demonstrate summary functions like sum(), mean(), sd(), just tip iceberg. fact, functions R run vectors directly, one value time, actually efficient used way.R, vector can ONE “type” (“class”) object time, e.g. vector numbers, vector characters, vector dates, etc. Vectors mixed-type allowed R.Also, actually working vectors along. ’s single values R fact vectors length 1. E.g. take number, let’s say 5; can use .vector() show fact vector length 1.vectors fundamental structure. may useful going forward think numbers instead numeric vectors, logicals (TRUE/FALSE) logical vectors, characters (.e. strings) character vectors, etc. Everything runs vectors!","code":"\n# create an example dataset of a small sample of numbers\ndata <- c(3, 6, 6, 2, 4, 1, 5)\ndata[1] 3 6 6 2 4 1 5\nx <- 5\nis.vector(x)[1] TRUE\nlength(x)[1] 1"},{"path":"data-vectors.html","id":"types-of-vectors","chapter":"3 Data Vectors","heading":"3.1 Types of vectors","text":"LOTS types data vectors can hold, real complex numbers, characters raw byte-data, even dates times.10 class, deal following 4 types vectors:Numeric vectors, contain real numbers. Generally, R functions don’t distinguish integers decimal numbers (also called “doubles” “floats”) treat numbers decimal-valued real numbers.11Logical vectors, contain TRUE/FALSE values. Usually, arise logical comparison operators functions check condition satisfied. Remember computations, TRUE becomes 1 FALSE becomes 0.12Character vectors, contain characters (often also called “strings”). basically categorical text data. E.g. may groups “” “B”, sex “Male” “Female”. can even sentences, paragraphs, entire bodies text character. briefly touch processing text data class.13Lastly, date vectors also covered. actually closely related numeric vectors (later). ubiquitous data science thus deserving inclusion.14\nNote: cover dates , date + time values (also called datetime) since actually quite different data types many lectures.15\nNote: cover dates , date + time values (also called datetime) since actually quite different data types many lectures.15","code":""},{"path":"data-vectors.html","id":"numeric-vectors","chapter":"3 Data Vectors","heading":"3.2 Numeric vectors","text":"Let’s start numeric vectors. example, suppose want double, square, take arc-tangent, find rounded base-2 logarithm data value, can just directly vector, runs one value time:Note can also use data side operators, argument certain functions:can combine summary functions neat things. example, suppose want manually calculate standard deviation—.e. average deviation mean—sample. , discuss detail later course, now formula:\\[SD = \\sqrt{\\frac1{n-1}\\sum_{=1}^n(x_i-\\bar x)^2}\\qquad\\text{$\\bar{x}=\\frac1n\\sum_{=1}^nx_i$}\\]words, standard deviation square root 1/(n-1) times sum squared differences sample mean data value. easy vector arithmetic R:Let’s break . inside, data - mean(data) subtracts mean data value one time:squared ( ... )^2 , operates one time:Finally, vector summed, scaled 1/(n-1), square rooted get standard deviation. can check correct comparing built-sd() function.","code":"\ndata * 2[1]  6 12 12  4  8  2 10\ndata^2[1]  9 36 36  4 16  1 25\natan(data)[1] 1.2490458 1.4056476 1.4056476 1.1071487 1.3258177 0.7853982 1.3734008\nround(log2(data))[1] 2 3 3 1 2 0 2\n2^data[1]  8 64 64  4 16  2 32\n# log(10) with various bases; note the Inf due to base 1\nlog(10, base = data)[1] 2.095903 1.285097 1.285097 3.321928 1.660964      Inf 1.430677\n# implementing sd() using vector arithmetic syntax\nn <- length(data)\nsqrt((1 / (n - 1)) * sum((data - mean(data))^2))[1] 1.9518\ndata - mean(data)[1] -0.8571429  2.1428571  2.1428571 -1.8571429  0.1428571 -2.8571429  1.1428571\n(data - mean(data))^2[1] 0.73469388 4.59183673 4.59183673 3.44897959 0.02040816 8.16326531 1.30612245\nsd(data)[1] 1.9518"},{"path":"data-vectors.html","id":"logical-vectors","chapter":"3 Data Vectors","heading":"3.3 Logical vectors","text":"“vectorized” operations also work logical comparisons, produce logical vectors. example, can ask R observations even:can ask values within 1 standard deviation mean:Remember section 2.3.4 math turns TRUE 1 FALSE 0? turns extremely useful. example, can use sum() count many values even:can ask proportion data within 1 standard deviation mean, involves taking sum logical comparison dividing length, .e. computing mean:Remember: whenever vector TRUE/FALSE values—usually result logical comparison—can use sum() count many TRUE, mean() compute proportion TRUE values. can course also use numeric operations, just remember TRUE\\(\\rightarrow1\\), FALSE\\(\\rightarrow0\\).","code":"\n# recall %% gives the division remainder\ndata %% 2 == 0[1] FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE\n# recall & is the AND operator\n# note the inequality checks EACH value of data against the other side\n(mean(data) - sd(data) <= data) & (data <= mean(data) + sd(data))[1]  TRUE FALSE FALSE  TRUE  TRUE FALSE  TRUE\n# sum(logical vector) counts the number of TRUEs\n# here, we find 4 data values are even\nsum(data %% 2 == 0)[1] 4\n# mean(logical vector) = sum(logical vector) / length(logical vector)\n# thus, it's a shortcut for calculating proportion of TRUEs\n# here, we find 57% of the data is within 1 sd of the mean\nmean(\n  (mean(data) - sd(data) <= data) & (data <= mean(data) + sd(data))\n)[1] 0.5714286"},{"path":"data-vectors.html","id":"other-constructors","chapter":"3 Data Vectors","heading":"3.4 Other constructors","text":"far, ’ve learned construct vectors using c() function, example data<-c(3,6,6,2,4,1,5). common ways construct .One easiest, just need sequence integers, use : operator:seq() function something similar, except also additional arguments specify step size length.specifies many numbers total (note: ONE arguments can set time).Vectors can also created rep() function lets repeat contents. two arguments: times controls many times repeat entire input, controls many times repeat element input vector. can specify either arguments.16 Note rep() can used repeat objects , just numbers.Finally, can mix match constructors, using combination c(), :, seq(), rep() heart’s content.","code":"\n1:5[1] 1 2 3 4 5\n10:-10 [1]  10   9   8   7   6   5   4   3   2   1   0  -1  -2  -3  -4  -5  -6  -7  -8  -9\n[21] -10\nseq(1, 5)[1] 1 2 3 4 5\nseq(0, 100, by = 10) [1]   0  10  20  30  40  50  60  70  80  90 100\nseq(0, 1, length.out = 101)  [1] 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.10 0.11 0.12 0.13 0.14 0.15\n [17] 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.23 0.24 0.25 0.26 0.27 0.28 0.29 0.30 0.31\n [33] 0.32 0.33 0.34 0.35 0.36 0.37 0.38 0.39 0.40 0.41 0.42 0.43 0.44 0.45 0.46 0.47\n [49] 0.48 0.49 0.50 0.51 0.52 0.53 0.54 0.55 0.56 0.57 0.58 0.59 0.60 0.61 0.62 0.63\n [65] 0.64 0.65 0.66 0.67 0.68 0.69 0.70 0.71 0.72 0.73 0.74 0.75 0.76 0.77 0.78 0.79\n [81] 0.80 0.81 0.82 0.83 0.84 0.85 0.86 0.87 0.88 0.89 0.90 0.91 0.92 0.93 0.94 0.95\n [97] 0.96 0.97 0.98 0.99 1.00\n# repeat a single number\nrep(2, 5)[1] 2 2 2 2 2\n# repeat a vector, specifying both times and each\nrep(1:3, times = 3, each = 4) [1] 1 1 1 1 2 2 2 2 3 3 3 3 1 1 1 1 2 2 2 2 3 3 3 3 1 1 1 1 2 2 2 2 3 3 3 3\n# you can also let times be a vector, as well as repeat other objects\nrep(c(TRUE, FALSE), times = c(2, 4))[1]  TRUE  TRUE FALSE FALSE FALSE FALSE\nrep(c(1, 3, 7:9, seq(10, 12, by = 0.5)), each = 2) [1]  1.0  1.0  3.0  3.0  7.0  7.0  8.0  8.0  9.0  9.0 10.0 10.0 10.5 10.5 11.0 11.0\n[17] 11.5 11.5 12.0 12.0"},{"path":"data-vectors.html","id":"multiple-vectors-vector-recycling","chapter":"3 Data Vectors","heading":"3.5 Multiple vectors + vector recycling","text":"may surprise learn vectorized operations also work multiple vectors! vectors aren’t length, shorter vectors repeated match length longest vector. called recycling. Example:class, never need loop exercises (hence ’s considered bonus topic covered notes instead left optional additional reading).Instead, always look solution using vectorized operations. R, vectorized operations basically always MUCH faster loops, due low-level parallelization optimizations.","code":"\n# define some vectors for demo\n# x1, x2 are both length 6 vectors\n# y and z have lengths 3 and 2\nx1 <- 0:5         # x1:  0, 1, 2, 3, 4, 5\nx2 <- -2:3        # x2: -2,-1, 0, 1, 2, 3\ny  <- 1:3         #  y:  1, 2, 3\nz  <- c(-1, 1)    #  z: -1, 1\n# sum vectors one element at a time\nx1 + x2[1] -2  0  2  4  6  8\n# take powers, again one element at a time from each vector\nx2^x1[1]   1  -1   0   1  16 243\n# take differences, one element at a time, recycling y\nx1 - y[1] -1 -1 -1  2  2  2\n# log y with x1+2 as base, again recycling y\nlog(y, base = x1 + 2)[1] 0.0000000 0.6309298 0.7924813 0.0000000 0.3868528 0.5645750\n# more complex operation that recycles multiple vectors,\n# as well as some numbers (which are just length 1 vectors)\n2^abs(x1 * z) - x2^y - median(data)[1] -1 -3  0  3  8  1\n# these also work with other numeric/logical functions we've seen so far\n# here, left side is a length 6 vector, right side is a length 2 vector,\n# so right side is recycled three times then compared with left\nx2 <= atan(z) * mean(x1)[1]  TRUE  TRUE FALSE  TRUE FALSE FALSE"},{"path":"data-vectors.html","id":"in-membership","chapter":"3 Data Vectors","heading":"3.6 %in% (Membership)","text":"One notable exception %% operator, “vectorizes” left side. example, suppose want know elements z x1. ’s :want ask elements z x1, must prepend expression ! negate :Note difference z == x1, recycles z, checks element-wise equality:another common point confusion first time R users. Make sure understand difference checking vector membership, .e. element one vector also contained somewhere another vector, vs checking element--element equality, .e. checking 1st elements , 2nd elements , 3rd elements , etc. (possibly recycling). See StackOverflow page examples.Going forward, continue use vectorized functions vector recycling code examples, sometimes without drawing attention , sake brevity. Pretty soon, concepts also feel like second nature !","code":"\nz %in% x1[1] FALSE  TRUE\n!z %in% x1[1]  TRUE FALSE\nz == x1[1] FALSE  TRUE FALSE FALSE FALSE FALSE"},{"path":"data-vectors.html","id":"vector-subsetting","chapter":"3 Data Vectors","heading":"3.7 Vector subsetting","text":"Let’s also quickly cover vector subsetting. R, many ways extract subset (.e. just portion) vector. 2 important things remember throughout R subsetting objects:R indexes 1, 0. words, R starts counting position objects 1.Bounds inclusive. words, R generally includes start end bounds subsetting.Knowing , let’s learn subsetting examples. pair useful built-objects vectors letters LETTERS, contain respectively 26 lowercase uppercase letters English alphabet. letters make character vector (discuss detail next section).can extract elements vector [] operator, giving either vector numeric positions, vector TRUE/FALSE values, negative vector exclusions (.e. anything except). Examples:","code":"\nletters [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" \"r\" \"s\" \"t\"\n[21] \"u\" \"v\" \"w\" \"x\" \"y\" \"z\"\n# giving numeric positions of desired elements\n# remember numbers are numeric vectors of length 1\nletters[1][1] \"a\"\n# of course this also works with longer vectors\nletters[5:10][1] \"e\" \"f\" \"g\" \"h\" \"i\" \"j\"\n# naturally you can use more complex syntax if needed,\n# as long as the result is a numeric vector,\n# repeating indices give duplicate values\nletters[c(1, 24:26, rep(5, 8))] [1] \"a\" \"x\" \"y\" \"z\" \"e\" \"e\" \"e\" \"e\" \"e\" \"e\" \"e\" \"e\"\n# you can also use a logical vector, here we check if\n# each position is even, returning every second letter\nletters[1:26 %% 2 == 0] [1] \"b\" \"d\" \"f\" \"h\" \"j\" \"l\" \"n\" \"p\" \"r\" \"t\" \"v\" \"x\" \"z\"\n# logical vectors will be recycled if necessary, so this also works\nletters[c(FALSE, TRUE)] [1] \"b\" \"d\" \"f\" \"h\" \"j\" \"l\" \"n\" \"p\" \"r\" \"t\" \"v\" \"x\" \"z\"\n# using negative vectors is like saying \"anything EXCEPT\"\nletters[-1] [1] \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" \"r\" \"s\" \"t\" \"u\"\n[21] \"v\" \"w\" \"x\" \"y\" \"z\"\n# again, this also works with vectors of negatives\nletters[-1:-10] [1] \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" \"r\" \"s\" \"t\" \"u\" \"v\" \"w\" \"x\" \"y\" \"z\"\n# of course this is equivalent to\nletters[-(1:10)] [1] \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" \"r\" \"s\" \"t\" \"u\" \"v\" \"w\" \"x\" \"y\" \"z\"\n# note the parentheses there, without it you get -1,0,1,...,10\n# which raises an error (what we want is -1,-2,...,-10)\n# this is because positive and negative position syntax cannot be mixed\nletters[-1:10]Error in letters[-1:10] : only 0's may be mixed with negative subscripts"},{"path":"data-vectors.html","id":"sortingreordering","chapter":"3 Data Vectors","heading":"3.8 Sorting/reordering","text":"Sometimes, may need sort/reorder vectors. already saw previous section can reorder vectors subsetting vector positions. vector positions exhausts vector without repeating (.e. returns element vector exactly ), result reordering vector.can course also R sort vector . two main functions sorting: sort(), expect returns vector elements rearranged lowest highest (unless set argument decreasing = TRUE opposite); order(), simply returns order elements go (.e. vector positions belong) sorted lowest highest (, unless set decreasing = TRUE).final function sometimes handy rev() function, reverses vector.general, R operations change input object place. E.g. sort(data2) returns COPY data2 elements sorted; actually change data2. true functions R, exceptions. example, observe:want object updated place, explicitly tell R overwrite assignment <- operator, like :’s often considered bad practice overwrite input like , since can destructive , used caution, can easily lead errors road. recommend whenever possible writing output new object instead, like :general, R inputs outputs totally independent objects special “connections”.17. want operation saved, make sure remember assign output something!","code":"\n# defining a new data2 vector to use for examples,\n# trust me, this will REALLY help clarify what's happening in a minute\ndata2 <- data * 10\ndata2[1] 30 60 60 20 40 10 50\n# ok, now let's proceed with the demos, first up:\n# manual reordering, e.g. swapping the first and last elements\nn <- length(data2)\ndata2[c(n, 2:(n-1), 1)][1] 50 60 60 20 40 10 30\n# simply sort the data in ascending order\nsort(data2)[1] 10 20 30 40 50 60 60\n# sort in descending order\nsort(data2, decreasing = TRUE)[1] 60 60 50 40 30 20 10\n# return the order of positions that WOULD sort it\norder(data2)[1] 6 4 1 5 7 2 3\n# passing this as a subsetting vector gives the sorted vector\ndata2[order(data2)][1] 10 20 30 40 50 60 60\n# reverse the vector\nrev(data2)[1] 50 10 40 20 60 60 30\n# original data2 vector\ndata2[1] 30 60 60 20 40 10 50\n# sort data2\nsort(data2)[1] 10 20 30 40 50 60 60\n# is it changed?\ndata2[1] 30 60 60 20 40 10 50\n# overwrite data2 with the sorted copy (discouraged syntax)\ndata2 <- sort(data2)\n# now it's changed\ndata2[1] 10 20 30 40 50 60 60\n# save sorted data2 to new object (encouraged syntax)\ndata2_sorted <- sort(data2)\ndata2_sorted[1] 10 20 30 40 50 60 60"},{"path":"data-vectors.html","id":"character-vectors","chapter":"3 Data Vectors","heading":"3.9 Character vectors","text":"letters vector last section one example character vector. can create character vector also c() rep() ’ve seen . creating characters, can use either single ' double \" quote character, difference.","code":"\n# creating a demo character vector, e.g. these are my friends!\nfriends <- c(\"Alice\", \"Bob\", \"Charlie\", \"Donny\", \"Emmy\",\n             \"Francine\", \"Genevieve\", \"Heinemann\")\nfriends[1] \"Alice\"     \"Bob\"       \"Charlie\"   \"Donny\"     \"Emmy\"      \"Francine\" \n[7] \"Genevieve\" \"Heinemann\"\n# you can also use rep, e.g. I can assign my friends into 2 groups\ngroups <- rep(LETTERS[1:2], time = 4)\ngroups[1] \"A\" \"B\" \"A\" \"B\" \"A\" \"B\" \"A\" \"B\""},{"path":"data-vectors.html","id":"basic-string-functions","chapter":"3 Data Vectors","heading":"3.9.1 Basic string functions","text":"Base R number common functions working strings: nchar() getting number characters, tolower()/toupper() convert case, substr() extracting substrings, paste()/paste0() concatenate (e.g. “glue” together) strings, strrep() repeating characters string.","code":"\n# get the number of characters in each name\nnchar(friends)[1] 5 3 7 5 4 8 9 9\n# convert names to all upper or all lower\ntoupper(friends)[1] \"ALICE\"     \"BOB\"       \"CHARLIE\"   \"DONNY\"     \"EMMY\"      \"FRANCINE\" \n[7] \"GENEVIEVE\" \"HEINEMANN\"\ntolower(friends)[1] \"alice\"     \"bob\"       \"charlie\"   \"donny\"     \"emmy\"      \"francine\" \n[7] \"genevieve\" \"heinemann\"\n# get the first 3 characters of each name\nsubstr(friends, 1, 3)[1] \"Ali\" \"Bob\" \"Cha\" \"Don\" \"Emm\" \"Fra\" \"Gen\" \"Hei\"\n# get the last 3 characters of each name;\n# remember R always includes bounds, so to get the last three,\n# we want to get n-2,n-1,n where n is the number of characters\n# note this is done once again with our old friend, vectorization!\nsubstr(friends, nchar(friends) - 2, nchar(friends))[1] \"ice\" \"Bob\" \"lie\" \"nny\" \"mmy\" \"ine\" \"eve\" \"ann\"\n# remove the first and last characters of each name\nsubstr(friends, 2, nchar(friends) - 1)[1] \"lic\"     \"o\"       \"harli\"   \"onn\"     \"mm\"      \"rancin\"  \"eneviev\" \"eineman\"\n# paste can \"glue\" on single or (recycled) vectors of strings\npaste(friends, \"is my friend\")[1] \"Alice is my friend\"     \"Bob is my friend\"       \"Charlie is my friend\"  \n[4] \"Donny is my friend\"     \"Emmy is my friend\"      \"Francine is my friend\" \n[7] \"Genevieve is my friend\" \"Heinemann is my friend\"\npaste(\"My friend\", friends, \"is in group\", groups)[1] \"My friend Alice is in group A\"     \"My friend Bob is in group B\"      \n[3] \"My friend Charlie is in group A\"   \"My friend Donny is in group B\"    \n[5] \"My friend Emmy is in group A\"      \"My friend Francine is in group B\" \n[7] \"My friend Genevieve is in group A\" \"My friend Heinemann is in group B\"\n# paste0(...) is a shortcut for paste(..., sep=\"\")\n# sep sets the separator between each string (default: a space \" \")\npaste0(friends, \"123\")[1] \"Alice123\"     \"Bob123\"       \"Charlie123\"   \"Donny123\"     \"Emmy123\"     \n[6] \"Francine123\"  \"Genevieve123\" \"Heinemann123\"\npaste(friends, \"123\", sep = \"_\")[1] \"Alice_123\"     \"Bob_123\"       \"Charlie_123\"   \"Donny_123\"     \"Emmy_123\"     \n[6] \"Francine_123\"  \"Genevieve_123\" \"Heinemann_123\"\n# paste also has an argument called collapse, which sets a separator,\n# then uses that separator to collapse the vector into a single string\npaste(friends, collapse = \", \")[1] \"Alice, Bob, Charlie, Donny, Emmy, Francine, Genevieve, Heinemann\"\n# repeat characters in each string a set number of times\nstrrep(friends, 3)[1] \"AliceAliceAlice\"             \"BobBobBob\"                  \n[3] \"CharlieCharlieCharlie\"       \"DonnyDonnyDonny\"            \n[5] \"EmmyEmmyEmmy\"                \"FrancineFrancineFrancine\"   \n[7] \"GenevieveGenevieveGenevieve\" \"HeinemannHeinemannHeinemann\"\n# of course, this can also be vectorized!\nstrrep(friends, 1:8)[1] \"Alice\"                                                                   \n[2] \"BobBob\"                                                                  \n[3] \"CharlieCharlieCharlie\"                                                   \n[4] \"DonnyDonnyDonnyDonny\"                                                    \n[5] \"EmmyEmmyEmmyEmmyEmmy\"                                                    \n[6] \"FrancineFrancineFrancineFrancineFrancineFrancine\"                        \n[7] \"GenevieveGenevieveGenevieveGenevieveGenevieveGenevieveGenevieve\"         \n[8] \"HeinemannHeinemannHeinemannHeinemannHeinemannHeinemannHeinemannHeinemann\""},{"path":"data-vectors.html","id":"pattern-string-functions","chapter":"3 Data Vectors","heading":"3.9.2 Pattern string functions","text":"also functions working patterns. don’t need master functions, basic demos may prove helpful. Primarily, grep()/grepl() pattern matching, sub()/gsub() pattern replacing. prefix/suffix matching startsWith()/endsWith() also occasionally useful.noted patterns fairly short (mostly one single character) simplicity example, patterns can many characters long necessary.functions (well several listed grep() help page), actually accept complex pattern syntax search pattern. advanced search pattern syntax called “regular expressions” “regex” short. can things like match groups characters, match repeated characters groups, match specific locations words sentences, .class cover regular expressions real detail due limited time, feel free explore cheat sheet well two additional articles matter.","code":"\n# which friends (by position) have a lowercase \"e\" in their name?\ngrep(\"e\", friends)[1] 1 3 6 7 8\n# alternatively, return a TRUE/FALSE vector result instead for each element\ngrepl(\"e\", friends)[1]  TRUE FALSE  TRUE FALSE FALSE  TRUE  TRUE  TRUE\n# you can use either one to subset the original vector to get actual names\nfriends[grep(\"e\", friends)][1] \"Alice\"     \"Charlie\"   \"Francine\"  \"Genevieve\" \"Heinemann\"\n# you can disable case sensitivity, which adds Emmy to the results\nfriends[grep(\"e\", friends, ignore.case = TRUE)][1] \"Alice\"     \"Charlie\"   \"Emmy\"      \"Francine\"  \"Genevieve\" \"Heinemann\"\n# you can use sub() to replace patterns\n# here we can create a set of variant spellings, changing -y to -ie\nsub(\"y\", \"ie\", friends)[1] \"Alice\"     \"Bob\"       \"Charlie\"   \"Donnie\"    \"Emmie\"     \"Francine\" \n[7] \"Genevieve\" \"Heinemann\"\n# sub() can only replace once (inside each element),\n# but gsub() can replace ALL occurrences\nsub(\"n\", \"m\", friends)[1] \"Alice\"     \"Bob\"       \"Charlie\"   \"Domny\"     \"Emmy\"      \"Framcine\" \n[7] \"Gemevieve\" \"Heimemann\"\ngsub(\"n\", \"m\", friends)[1] \"Alice\"     \"Bob\"       \"Charlie\"   \"Dommy\"     \"Emmy\"      \"Framcime\" \n[7] \"Gemevieve\" \"Heimemamm\"\n# which friends have a name that endsWith() \"y\"?\n# (startsWith() does the opposite)\nendsWith(friends, \"y\")[1] FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE"},{"path":"data-vectors.html","id":"additional-stringr-functions","chapter":"3 Data Vectors","heading":"3.9.3 Additional stringr functions","text":"stringr, one core Tidyverse packages, contains alternative set functions working strings. Many similar purpose base R versions (although subtle differences). E.g. str_length() nchar(), str_to_lower()/str_to_upper() replicate tolower()/toupper(), str_replace() similar sub(), str_sub() extends substr(), etc. ’s full list doppelgänger stringr functions.However, useful stringr functions counterparts base R (least whose counterparts require much complex expressions). small curation .","code":"\n# since stringr is a \"core\" tidyverse package,\n# you can load it (+other core packages) with library(tidyverse)\n# you can also just load stringr by itself if that's all you need\nlibrary(stringr)\n# count how many times a pattern occurs\nstr_count(friends, \"e\")[1] 1 0 1 0 0 1 4 2\n# change strings to title case, i.e. first letter uppercase, all else lower\nstr_to_title(toupper(friends))[1] \"Alice\"     \"Bob\"       \"Charlie\"   \"Donny\"     \"Emmy\"      \"Francine\" \n[7] \"Genevieve\" \"Heinemann\"\n# \"pad\" a vector of strings to a constant length\nstr_pad(friends, width = 12, side = \"right\", pad = \".\")[1] \"Alice.......\" \"Bob.........\" \"Charlie.....\" \"Donny.......\" \"Emmy........\"\n[6] \"Francine....\" \"Genevieve...\" \"Heinemann...\""},{"path":"data-vectors.html","id":"comparing-strings","chapter":"3 Data Vectors","heading":"3.9.4 Comparing strings","text":"Strings, like numbers, can also logically compared R using ==, !=, <, <=, >, >= operators (also vectorized course). Checking equality self-explanatory, inequalities evaluate dictionary sorting order, .e. order might appear dictionary, except generalized include just letters also number symbols.snippet code (worry learning right now) prints “dictionary” sorting order ordinary keyboard characters typable standard US English keyboard layout ascending order.ordering depends platform; order Windows systems. *nix systems (.e. Mac/Linux) order  _-,;:!?.’\"()[]{}@*/\\&#%`^+<=>|~$0123…yYzZ instead.Characters sorted order sequence characters, characters earlier sequence “less ” later characters. Strings first character sorted second character; second character also , sorted third, (just like normal dictionaries)Note however nothingness, .e. absence character (string ends), comes character, makes sense otherwise “app” appear “apple”. series examples demonstrating string sorting.Using newfound wisdom, can now conclusively settle age old debates!","code":"\n# print out the result\ncat(\n  # sort the characters\n  sort(\n    # flatten list output to a big vector of characters\n    unlist(\n      # split symbols into individual characters\n      strsplit(\n        # make a vector of all ordinary keyboard characters\n        c(letters, LETTERS, 0:9, \"`~!@#$%^&*()_+-=[]\\\\{}|;':\\\",./<>? \"),\n        \"\"\n      )\n    )\n  ),\n  sep = \"\"\n)\n'- !\"#$%&()*,./:;?@[\\]^_`{|}~+<=>0123456789aAbBcCdDeEfFgGhHiIjJkKlLmMnNoOpPqQrRsStTuUvVwWxXyYzZ\n\n\"apple\" == \"apple\"[1] TRUE\n# note equality also implies both >= and <=\n\"apple\" >= \"apple\"[1] TRUE\n# remember R is case sensitive, so these are different\n\"apple\" != \"Apple\"[1] TRUE\n# comparing apples and oranges\n\"apple\" == \"orange\"[1] FALSE\n\"apple\" < \"orange\"[1] TRUE\n# numbers and symbols can also dictionary-sort like letters\n# note these numeral characters have NO numeric meaning!\n\"42\" < \"43\"[1] TRUE\n\"1 pm\" < \"2 pm\"[1] TRUE\n\"STAT240\" < \"STAT340\"[1] TRUE\n# however symbols come before numbers, thus\n\"1,000\" < \"1000\"[1] TRUE\n# remember: these have NO numberic meaning, so \"-\" is just a character\n\"-3.00\" < \"-3.14\"[1] TRUE\n# and remember ending a string comes before any other character\n\"-3\" < \"-3.14\"[1] TRUE\n# what is the greatest state? (head prints the first 6)\nhead(sort(state.name, decreasing = TRUE))[1] \"Wyoming\"       \"Wisconsin\"     \"West Virginia\" \"Washington\"    \"Virginia\"     \n[6] \"Vermont\"      \n# note by definition of inequalities, we can also use min/max\n# which will find the first and last alphabetically\nc(min(state.name), max(state.name))[1] \"Alabama\" \"Wyoming\"\n# is the pen mightier than the sword?\n\"pen\" > \"sword\"[1] FALSE\n# is Windows better than Mac?\n\"Windows\" > \"Mac\"[1] TRUE\n# and most importantly, did the chicken come before the egg?\n\"chicken\" < \"egg\"[1] TRUE"},{"path":"data-vectors.html","id":"ordered-data","chapter":"3 Data Vectors","heading":"3.9.5 Bonus: ordered data","text":"Ordered categorical data (also called ordinals) extremely common, ’s worth briefly mentioning , even ’s considered mostly outside scope course. characters natural ordering present, ’s recommended use factor() convert , setting ordered = TRUE using levels = c(...) specify ordering levels ascending order (low high).example, suppose following data vector dosage level data subjects:Naturally, ordered Low < Medium < High currently ordering structure. can fix like :can see now ordering structure R understands levels compare . ordering also automatically respected later plots analysis, ’ll soon see.","code":"\ndoses <- c(\"Low\", \"High\", \"Medium\", \"Low\", \"High\")\ndoses <- factor(doses, levels = c(\"Low\", \"Medium\", \"High\"), ordered = TRUE)\ndoses[1] Low    High   Medium Low    High  \nLevels: Low < Medium < High"},{"path":"data-vectors.html","id":"coercion","chapter":"3 Data Vectors","heading":"3.10 Coercion (converting types)","text":"Sometimes, read data may need converted ’s usable. E.g. let’s say read list prices catalog get following character vector:Since R doesn’t natively understand dollar signs comma grouping, must start character vector. can check type vector using .numeric(), .logical(), .character() functions.data “cleaned-” can coerce (.e. convert) vector types corresponding .numeric(), .logical(), .character() functions. case however, data yet “cleaned-” coercion operation give error.R, ’s important remember reinvent wheel; actions already associated package/function, ’s probably better can write (’re beginner). , may tempting write parsing function using sub()/gsub() replace dollar commas coerce, ’s better option.readr another one core Tidyverse packages. ’s designed make ingesting data easy possible.One set readr functions useful parse_number() parse_logical() functions (converting string trivial can basically always done using .character() function). functions quite smart can ignore extra characters just extract relevant numerical info.also work things like percent symbols, note simply ignores symbol instead treating dividing 100.See help page parse_number() examples usage notes.","code":"\nprices_raw <- c(\"$1,000\", \"$1,500\", \"$850\", \"$2,000\")\nprices_raw[1] \"$1,000\" \"$1,500\" \"$850\"   \"$2,000\"\nis.character(prices_raw)[1] TRUE\nas.numeric(prices_raw)Warning: NAs introduced by coercion[1] NA NA NA NA\n# since readr is also a \"core\" tidyverse package,\n# you can use library(tidyverse) or library(readr)\nlibrary(readr)\nprices <- parse_number(prices_raw)\nprices[1] 1000 1500  850 2000\nis.numeric(prices)[1] TRUE\n# demonstrate parsing percentages\nparse_number(c(\"30%\", \"100%\", \"1,500%\"))[1]   30  100 1500"},{"path":"data-vectors.html","id":"date-vectors","chapter":"3 Data Vectors","heading":"3.11 Date vectors","text":"Finally, let’s talk date vectors (note talking date+time values, just dates). R, dates actually stored number, representing number days January 1st 1970, used reference date called Epoch.run examples, ’re going load lubridate package, designed make working dates super easy also core Tidyverse package.Ok, let’s start demo creating date object. Let’s use year’s 4th July example.can see even though date object \"Date\" class, actually \"double\" type, means behind scenes, ’s secretly stored number.18 unclass() object, .e. strip away \"Date\" property, can see ’s just number 20273 underneath, can check fact Jul 4, 2025 indeed 20273 days Jan 1 1970.R conforms ISO-8601 standards, .e. dates ALWAYS show \"YYYY-MM-DD\" (even though ’re stored numerically). arguably best format dates, ’s unique format chronological order lexicographical order identical, extremely useful property.Also note despite date appearing character, character. Using identical() (compares two objects ) show false. Furthermore, .numeric() confirms date converts 20273 expected, whereas string \"2025-07-04\" converted returns NA.just warn even though may print similarly, date objects date-like strings , avoid errors unexpected behavior, make sure properly convert date data true date objects.","code":"\nlibrary(lubridate)\n# create example date object (more on this later)\ndate <- ymd(\"2025-07-04\")\ndate[1] \"2025-07-04\"\n# looks like a date\nclass(date)[1] \"Date\"\nis.Date(date) & !is.numeric(date)[1] TRUE\n# but it's secretly a number underneath!\nunclass(date)[1] 20273\n# we can reverse this too, start with a number,\n# then change the class to \"Date\", and voila!\nx <- 20273\nclass(x) <- \"Date\"\nx[1] \"2025-07-04\"\ndate[1] \"2025-07-04\"\nis.character(date)[1] FALSE\nidentical(date, \"2025-07-04\")[1] FALSE\nc(as.numeric(date), as.numeric(\"2025-07-04\"))Warning: NAs introduced by coercion[1] 20273    NA"},{"path":"data-vectors.html","id":"parsing-dates","chapter":"3 Data Vectors","heading":"3.11.1 Parsing dates","text":"cases (specifically, dates dataset already represented standard ISO-8601 format) R automagically parse (.e. convert) dates . cases, may need manually parse . continue use lubridate, since robust user-friendly functions working dates.lubridate, parser functions mdy(), dmy(), ymd() (well rarer siblings ydm(), myd(), dym()) used parse date data proper date objects. difference functions order expect see date components, e.g. mdy() used data ordered month, day, year (common US), dmy() used date data data ordered day, month, year (generally preferred outside US). functions extremely robust automagically recognize wide range formats, course ’re vectorized! ’s examples:can see, just need tell R order expect date components handle rest! demonstrated mdy() dmy() functions since far common formats, functions behave .One last parser. Sometimes data gives dates decimal, e.g. 2025-07-04 2025.504 since ’s 185th day year means ’s (185-1)/365*100%=50.4% way year.19 R also dedicated function . date_decimal() converts decimal date+time object, can round nearest date round_date(...,unit=\"day\") drop time component date() converts date+time objects pure date objects (, covering date+time objects due complexity & limited time).also reverse function decimal_date() converts date back decimal.","code":"\nmdy(c(\n  \"7/4/25\", \"07-04-2025\", \"070425\", \"Jul 4 '25\", \"Friday, July 4th, 2025\"\n))[1] \"2025-07-04\" \"2025-07-04\" \"2025-07-04\" \"2025-07-04\" \"2025-07-04\"\ndmy(c(\n  \"4/7/25\", \"04-07-2025\", \"040725\", \"4 Jul '25\", \"Friday, 4th of July, 2025\"\n))[1] \"2025-07-04\" \"2025-07-04\" \"2025-07-04\" \"2025-07-04\" \"2025-07-04\"\n# generate a vector of elapsed 21st century dates\n# in decimal format for demo purposes\n# (here, runif uniformly samples 4 numbers from 2000 to 2025.504)\ndates2 <- runif(4, 2000, 2025.504)\ndates2[1] 2006.772 2009.491 2014.610 2023.163\n# convert decimals to dates\ndates2 <- date(round_date(date_decimal(dates2), unit = \"day\"))\ndates2[1] \"2006-10-10\" \"2009-06-29\" \"2014-08-12\" \"2023-03-02\"\ndecimal_date(dates2)[1] 2006.773 2009.490 2014.611 2023.164"},{"path":"data-vectors.html","id":"aside-rs-calendar","chapter":"3 Data Vectors","heading":"3.11.2 Aside: R’s calendar","text":"Quick aside. R extremely robust calendar, don’t need worry “babysitting” R. Notably, R knows exactly years leap aren’t.Fun fact: R’s calendar rigorous Excel’s calendar, since correctly treats 1900 non-leap, unlike Excel. course probably immaterial 21st century, just think ’s amusing bit trivia.","code":"\n# the most recent leap year is 2024, since it's divisible by 4\nmdy(\"Feb 29, 2024\")[1] \"2024-02-29\"\n# however, years like 1900 or 2100 are not leap,\n# since they're also divisible by 100\nmdy(c(\"Feb 29, 1900\", \"Feb 29, 2100\"))Warning: 2 failed to parse.[1] NA NA\n# but 2000 is leap, since it's also divisible by 400\nmdy(\"Feb 29, 2000\")[1] \"2000-02-29\"\n# you can also use the leap_year() function instead\n# is the year that a given date is in leap?\nleap_year(date)[1] FALSE\n# which of these given years are leap?\nleap_year(c(1900, 2000, 2024, 2100))[1] FALSE  TRUE  TRUE FALSE"},{"path":"data-vectors.html","id":"getset-components","chapter":"3 Data Vectors","heading":"3.11.3 Get/set components","text":"Lubridate provides many get/set functions (often called getters setters) getting setting different components (.e. properties) associated date. common ones include year(), month(), day(), wday() (day week), quarter().Let’s continue using generated dates2 object , except add example date earlier 2025-07-04 vector first element.functions (makes sense) like month() wday() additional arguments like label abbr control output format option output names instead numbers. , recommend briefly check help page every new function learn additional options.system different language, may see code output non-English names months days week. R uses system’s locale determine output values. extremely easy fix; simply run line code console, close restart Rstudio changes permanently take effect.work? adds line invisible(Sys.setlocale(\"LC_TIME\",\"C\")) \"~/.Rprofile\" file, run every time R starts . added line invisibly sets \"LC_TIME\" locale variable \"C\", tells lubridate use English outputs.first line output actual output names. list “levels” second line just shows set possible values outputted. return object another example “ordered factor”. purposes can mostly treated similar character/string vector. (want string operations output, make sure convert fully character first .character()!)getters extremely useful data cleaning well data visualization, since can much pleasant , example, see monthly breakdown Jan, Feb, …, Dec instead 1, 2, …, 12.getters can also used “setters”, .e. used set components. example:works getters , feel free experiment . also several getter/setter functions qday() day quarter, week() week number, semester() 1st 2nd semester year.","code":"\n# add in Jul 4, then print (to remind us what it contains)\ndates2 <- c(date, dates2)\ndates2[1] \"2025-07-04\" \"2006-10-10\" \"2009-06-29\" \"2014-08-12\" \"2023-03-02\"\n# extract the year, month, day, wday, quarter\nyear(dates2)[1] 2025 2006 2009 2014 2023\nmonth(dates2)[1]  7 10  6  8  3\nday(dates2)[1]  4 10 29 12  2\n# wday starts counting from Sunday, i.e. 1=Sunday, 2=Monday, etc.\nwday(dates2)[1] 6 3 2 3 5\nquarter(dates2)[1] 3 4 2 3 1\n# output month as abbreviated names instead (abbr = TRUE by default)\nmonth(dates2, label = TRUE)[1] Jul Oct Jun Aug Mar\n12 Levels: Jan < Feb < Mar < Apr < May < Jun < Jul < Aug < Sep < Oct < ... < Dec\n# output day of week as full, unabbreviated names\nwday(dates2, label = TRUE, abbr = FALSE)[1] Friday   Tuesday  Monday   Tuesday  Thursday\n7 Levels: Sunday < Monday < Tuesday < Wednesday < Thursday < ... < Saturday\nwrite('\\ninvisible(Sys.setlocale(\"LC_TIME\",\"C\"))\\n',\"~/.Rprofile\",1,T)\n# make a copy to use here, since this destroys the original vector\nnew_dates2 <- dates2\n# change year of all dates to 2000\nyear(new_dates2) <- 2000\nnew_dates2[1] \"2000-07-04\" \"2000-10-10\" \"2000-06-29\" \"2000-08-12\" \"2000-03-02\"\n# of course this is also vectorized!\nyear(new_dates2) <- 2000:2004\nnew_dates2[1] \"2000-07-04\" \"2001-10-10\" \"2002-06-29\" \"2003-08-12\" \"2004-03-02\""},{"path":"data-vectors.html","id":"date-math","chapter":"3 Data Vectors","heading":"3.11.4 Date math","text":"Since dates represented internally number days since reference point, math dates turns extremely easy. can add/subtract days, make sequences, run logical comparisons, even use statistical summary functions.","code":"\n# get day after by adding +1 to our example date\ndate + 1[1] \"2025-07-05\"\n# what date was 1000 days ago?\ndate - 1000[1] \"2022-10-08\"\n# how many days has it been since y2k?\n# note that subtracting dates gives a \"difftime\" class object\n# you can convert this to a number using the familiar as.numeric()\nas.numeric(date - mdy(\"1/1/00\"))[1] 9316\n# make a sequence of dates from Jul 4 to the end of the month\nseq(date, mdy(\"7/31/25\"), by = 1) [1] \"2025-07-04\" \"2025-07-05\" \"2025-07-06\" \"2025-07-07\" \"2025-07-08\" \"2025-07-09\"\n [7] \"2025-07-10\" \"2025-07-11\" \"2025-07-12\" \"2025-07-13\" \"2025-07-14\" \"2025-07-15\"\n[13] \"2025-07-16\" \"2025-07-17\" \"2025-07-18\" \"2025-07-19\" \"2025-07-20\" \"2025-07-21\"\n[19] \"2025-07-22\" \"2025-07-23\" \"2025-07-24\" \"2025-07-25\" \"2025-07-26\" \"2025-07-27\"\n[25] \"2025-07-28\" \"2025-07-29\" \"2025-07-30\" \"2025-07-31\"\n# make a sequence of every Friday from Jul 4 to the end of the year\nseq(date, mdy(\"12/31/25\"), by = 7) [1] \"2025-07-04\" \"2025-07-11\" \"2025-07-18\" \"2025-07-25\" \"2025-08-01\" \"2025-08-08\"\n [7] \"2025-08-15\" \"2025-08-22\" \"2025-08-29\" \"2025-09-05\" \"2025-09-12\" \"2025-09-19\"\n[13] \"2025-09-26\" \"2025-10-03\" \"2025-10-10\" \"2025-10-17\" \"2025-10-24\" \"2025-10-31\"\n[19] \"2025-11-07\" \"2025-11-14\" \"2025-11-21\" \"2025-11-28\" \"2025-12-05\" \"2025-12-12\"\n[25] \"2025-12-19\" \"2025-12-26\"[1] TRUE\n# what is the earliest date in the dates2 vector?\nmin(dates2)[1] \"2006-10-10\"\n# organize dates2 in chronological order\nsort(dates2)[1] \"2006-10-10\" \"2009-06-29\" \"2014-08-12\" \"2023-03-02\" \"2025-07-04\"\n# is today in dates2?\ntoday() %in% dates2[1] FALSE\n# some statistical functions can also be run on dates\n# e.g. we can find the mean and median date of dates2\nmean(dates2)[1] \"2015-11-29\"\nmedian(dates2)[1] \"2014-08-12\"\n# we can even find the standard deviation of dates,\n# though this does NOT return a date itself but rather a number of days\nsd(dates2)[1] 3006.747"},{"path":"data-vectors.html","id":"printing-dates","chapter":"3 Data Vectors","heading":"3.11.5 Printing dates","text":"final note, let’s briefly discuss printing dates. can use format() print dates pretty way. Different ways printing component represented using %... codes. Examples:full list percent codes can found help page strptime(), base R function parsing date/time objects.","code":"\n# print today as mm/dd/yy which is common in the US\nformat(date, \"%m/%d/%y\")[1] \"07/04/25\"\n# another way, slightly more \"natural\" feeling\nformat(date, \"%b %e, %Y\")[1] \"Jul  4, 2025\"\n# fully written out, including weekday\nformat(date, \"%A, %B %e, %Y\")[1] \"Friday, July  4, 2025\"\n# some shorthands also exist and may depend on your computer's locale\nformat(date, \"ISO prefers %F, but the US uses %D or %x\")[1] \"ISO prefers 2025-07-04, but the US uses 07/04/25 or 7/4/2025\""},{"path":"data-frames.html","id":"data-frames","chapter":"4 Data Frames","heading":"4 Data Frames","text":"Moving vectors, next important data structure R data frame. Think data frame similar matrix, (ideally) column vector single type representing variable attribute, row observation sample.’s actually really helpful think data frame collection parallel vectors length, column type. E.g. suppose survey sample college students; maybe ’d sex column character type, GPA column numeric type, birthday column date type, column declared major logical type.","code":""},{"path":"data-frames.html","id":"creating-dfs","chapter":"4 Data Frames","heading":"4.1 Creating data frames","text":"2 common ways creating new data frame manually: data.frame() base R, tibble() tibble package, another core Tidyverse packages. extremely similar, recommend tibble() due nice extra features better printing, referencing columns creation, stricter subsetting rules. Example:Note following:syntax inside tibble() always column_name = vector_of_data, next_column_name = next_vector_of_data, ... vector must length.vectors pre-created; can create go along.can reference another column immediately creating inside function, e.g. date_of_birth created, immediately used next line help create age (way age approximately computed number days since birth divided 365.24, approximate number days year, rounded following convention).Data frames can, almost always contain many columns different type. However, usual single column—still vector!—can contain SINGLE type data inside , e.g. column numbers characters simultaneously.Printing df either just writing new line, print() function (thing), show first rows, also info like:\ncolumn (row) names,\nnumber rows columns (displayed rows x cols),\ntype column (dbl, chr, lgl, date, others beyond scope).\ncolumn (row) names,number rows columns (displayed rows x cols),type column (dbl, chr, lgl, date, others beyond scope).can create column constants recycling single value.\nNote: design, tibble() recycle length-1 vectors (help avoid errors improve syntax legibility).\nNote: design, tibble() recycle length-1 vectors (help avoid errors improve syntax legibility).","code":"\n# import the tibble and lubridate libraries\n# again, tibble is core tidyverse, so library(tidyverse) will also work\n# but lubridate is not core so needs to be imported manually\nlibrary(tibble)\nlibrary(lubridate)\n# manually create an example data frame\ndf <- tibble(\n  name = c(\"Alice\", \"Bob\", \"Charlie\"),\n  sex = c(\"F\", \"M\", \"M\"),\n  date_of_birth = mdy(c(\"12/7/04\", \"7/4/99\", \"10/31/06\")),\n  age = floor(as.numeric(today() - date_of_birth) / 365.24),\n  declared_major = c(TRUE, TRUE, FALSE),\n  school = \"UW-Madison\"\n)\n# print df\ndf# A tibble: 3 × 6\n  name    sex   date_of_birth   age declared_major school    \n  <chr>   <chr> <date>        <dbl> <lgl>          <chr>     \n1 Alice   F     2004-12-07       21 TRUE           UW-Madison\n2 Bob     M     1999-07-04       26 TRUE           UW-Madison\n3 Charlie M     2006-10-31       19 FALSE          UW-Madison"},{"path":"data-frames.html","id":"importing-data-frames","chapter":"4 Data Frames","heading":"4.2 Importing data frames","text":"course, practice don’t usually create data frames manually like , rather import data files. always, base R ways , continue recommend Tidyverse syntax due better features design.million different data formats, cover 3 common basic ones: CSV, comma separated value files; TSV, tab separated files; XLS(X), Excel (similar spreadsheet software) data files. Notably, cover databases (like SQL derivatives) non-rectangular data formats (like JSON XML), due limitations time/space.","code":""},{"path":"data-frames.html","id":"aside-file-formats-extensions","chapter":"4 Data Frames","heading":"4.2.1 Aside: file formats & extensions","text":"First, small aside. File formats (types) file extensions commonly conflated, distinction important.File format refers internal structure contents. Common formats include simple text (can encoded using variety different encodings ASCII Unicode UTF-8 common), complex documents like PDFs DOCs, images videos, compressed archives, binary executables, specialized (often proprietary) formats.File extensions, contrast just characters added end name file convenience hint computers (users) might expect find inside contents file. bearing actual file format contained inside.Many extensions may fact file format, e.g. .Rmd, .html, .csv, .txt, many examples extensions actually just simple text files (encoding), can opened text editor. Conversely, formats can stored variety different extensions, e.g. MPEG-4 versatile multimedia “container” format may stored .mp4 also .m4a, .m4b, .m4p, .m4r, .m4v depending context., extension exists “hint” contents file. can store text file .mp4 extension want. computer suggest open video player fail, can force open text editor work just fine. Remember file names file contents totally separate things need bearing .important takeaways :data “formats” (like CSV, TSV, JSON, XML) really just simple text files (similar .txt files often created text editor programs). class, say “CSV” generally refer specific way text formatted (.e. values separated commas) inside file, just extension.data formats (like XLS(X) databases) simple text files specialized formats, often need different treatment.Just changing extension file change contents. E.g. changing .csv extension .zip create valid zip file, painting stripes horse turns zebra.Today, many systems default hide file extensions, e.g. file ’s actually named data.csv may appear user just named data. can cause problems, user isn’t aware tries rename file data.csv may actually become data.csv.csv. common cause knit-fail see.highly recommended force device always show extensions can help avoid problems. Instructions Windows Macs.","code":""},{"path":"data-frames.html","id":"importing-functions","chapter":"4 Data Frames","heading":"4.2.2 Importing functions","text":"text-format data files, turn readr suite functions importing , focus :read_csv() used read CSV files columns data separated commas,read_tsv() used read files columns data separated tabs,read_delim() general form read_... functions can used read files type separator.One additional non-text format covered course: XLS(X) spreadsheet data, commonly generated Excel similar spreadsheet software. , different function Tidyverse’s non-core readxl package:read_excel() can used read XLS XLSX spreadsheet dataNote underscores function names. E.g. read_csv() readr read.csv() base R function. similar, readr’s read_csv() minor improvements speed consistency recommended class.Also note readr (tidyverse) loaded, attempting TAB autocomplete read_csv() function instead give read.csv() , remember set working directory load necessary libraries whenever (re)opening Rstudio starting/resuming work.","code":""},{"path":"data-frames.html","id":"eruptions-example","chapter":"4 Data Frames","heading":"4.2.3 Example: US Eruptions","text":"demonstrate basic functionality different functions, ’ve prepared exported dataset 21st century volcanic eruptions (recorded start end date prior 2025) United States Smithsonian formats listed can practice reading initial format:eruptions_recent.csveruptions_recent.tsveruptions_recent.delimeruptions_recent.xlsx","code":""},{"path":"data-frames.html","id":"csv-file","chapter":"4 Data Frames","heading":"4.2.4 CSV file","text":"example, ’s first lines eruptions_recent.csv CSV file (eruption, volcano name, start stop dates, duration days, certainty confirmed, VEI volcano explosivity index).link dataset, can directly pass read_csv() automagically download file system’s temp directory read . Make sure save data frame sensible name. ’s also usually good idea print first lines check result see everything worked without error.Several things note :diagnostic messages printed reading, well warnings/errors encounters anything unusual (errors/warnings observed ).reading , R try intelligently guess data types column ’re standard format. can see since columns CSV already neat written standard format (e.g. dates YYYY-MM-DD, numbers logicals written common syntax, missing values written NA), everything automagically converted: name left character, start stop parsed dates, duration vei parsed numeric, confirmed became logical.\ncolumns written standard format, may work well () may need data cleaning , touch later.\ncolumns written standard format, may work well () may need data cleaning , touch later.can run just data frame name print first rows. equivalent running print(eruptions_recent).\nPrinting often useful way double check errors. default, first 10 lines printed save space.\nPrinting often useful way double check errors. default, first 10 lines printed save space.look Environment tab now, see loaded data frame.\n, can click arrow see list columns, well names, types, first values.\ncan also click object name open new tab full spreadsheet-like view entire data frame, can inspect data frame, even search values sort columns (note: sorting just preview affect underlying object).\n, can click arrow see list columns, well names, types, first values.can also click object name open new tab full spreadsheet-like view entire data frame, can inspect data frame, even search values sort columns (note: sorting just preview affect underlying object).","code":"volcano,start,stop,duration,confirmed,vei\nKīlauea,2024-09-15,2024-09-20,5,TRUE,NA\nKīlauea,2024-06-03,2024-06-03,0,TRUE,NA\nAtka Volcanic Complex,2024-03-27,2024-03-27,0,TRUE,NA\nAhyi,2024-01-01,2024-03-27,86,TRUE,NA\nKanaga,2023-12-18,2023-12-18,0,TRUE,1\nRuby,2023-09-14,2023-09-15,1,TRUE,1\n# import readr\nlibrary(readr)\n# read in CSV file from link\neruptions_recent <- read_csv(\n  \"https://bwu62.github.io/stat240-revamp/data/eruptions_recent.csv\"\n)Rows: 75 Columns: 6\n── Column specification ──────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): volcano\ndbl  (2): duration, vei\nlgl  (1): confirmed\ndate (2): start, stop\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n# print first few lines of result to check\neruptions_recent# A tibble: 75 × 6\n   volcano               start      stop       duration confirmed   vei\n   <chr>                 <date>     <date>        <dbl> <lgl>     <dbl>\n 1 Kīlauea               2024-09-15 2024-09-20        5 TRUE         NA\n 2 Kīlauea               2024-06-03 2024-06-03        0 TRUE         NA\n 3 Atka Volcanic Complex 2024-03-27 2024-03-27        0 TRUE         NA\n 4 Ahyi                  2024-01-01 2024-03-27       86 TRUE         NA\n 5 Kanaga                2023-12-18 2023-12-18        0 TRUE          1\n 6 Ruby                  2023-09-14 2023-09-15        1 TRUE          1\n 7 Shishaldin            2023-07-11 2023-11-03      115 TRUE          3\n 8 Mauna Loa             2022-11-27 2022-12-10       13 TRUE          0\n 9 Ahyi                  2022-11-18 2023-06-11      205 TRUE          1\n10 Kīlauea               2021-09-29 2023-09-16      717 TRUE          0\n# ℹ 65 more rows"},{"path":"data-frames.html","id":"tsv-file","chapter":"4 Data Frames","heading":"4.2.5 TSV file","text":"functions similar. ’s first lines TSV-version dataset, eruptions_recent.tsv (way notes built doesn’t display tabs properly, view TSV file directly, can see ).read read_tsv() function. time, save space, ’ve disabled diagnostic messages setting show_col_types = FALSE reduced final print checking 6 lines. Otherwise, can see exact result.","code":"volcano start   stop    duration    confirmed   vei\nKīlauea 2024-09-15  2024-09-20  5   TRUE    NA\nKīlauea 2024-06-03  2024-06-03  0   TRUE    NA\nAtka Volcanic Complex   2024-03-27  2024-03-27  0   TRUE    NA\nAhyi    2024-01-01  2024-03-27  86  TRUE    NA\nKanaga  2023-12-18  2023-12-18  0   TRUE    1\nRuby    2023-09-14  2023-09-15  1   TRUE    1\n# read in TSV file from link\neruptions_recent <- read_tsv(\n  \"https://bwu62.github.io/stat240-revamp/data/eruptions_recent.tsv\",\n  show_col_types = FALSE\n)\n# print first 6 lines instead of 10 to still check, but save space\nprint(eruptions_recent, n = 6)# A tibble: 75 × 6\n  volcano               start      stop       duration confirmed   vei\n  <chr>                 <date>     <date>        <dbl> <lgl>     <dbl>\n1 Kīlauea               2024-09-15 2024-09-20        5 TRUE         NA\n2 Kīlauea               2024-06-03 2024-06-03        0 TRUE         NA\n3 Atka Volcanic Complex 2024-03-27 2024-03-27        0 TRUE         NA\n4 Ahyi                  2024-01-01 2024-03-27       86 TRUE         NA\n5 Kanaga                2023-12-18 2023-12-18        0 TRUE          1\n6 Ruby                  2023-09-14 2023-09-15        1 TRUE          1\n# ℹ 69 more rows"},{"path":"data-frames.html","id":"arbitrary-delimited-file","chapter":"4 Data Frames","heading":"4.2.6 Arbitrary delimited file","text":"data file columns delimited (.e. separated) characters, can use read_delim() function, generalization previous two read . Just set delim argument whatever delimiter , ’re good go. ’s first lines eruptions_recent.delim columns separated vertical bar | characters, followed line code import check result.","code":"volcano|start|stop|duration|confirmed|vei\nKīlauea|2024-09-15|2024-09-20|5|TRUE|\nKīlauea|2024-06-03|2024-06-03|0|TRUE|\nAtka Volcanic Complex|2024-03-27|2024-03-27|0|TRUE|\nAhyi|2024-01-01|2024-03-27|86|TRUE|\nKanaga|2023-12-18|2023-12-18|0|TRUE|1\nRuby|2023-09-14|2023-09-15|1|TRUE|1\n# read in | delimited file from link\neruptions_recent <- read_delim(\n  \"https://bwu62.github.io/stat240-revamp/data/eruptions_recent.delim\",\n  delim = \"|\",\n  show_col_types = FALSE\n)\n# print first 6 lines\nprint(eruptions_recent, n = 6)# A tibble: 75 × 6\n  volcano               start      stop       duration confirmed   vei\n  <chr>                 <date>     <date>        <dbl> <lgl>     <dbl>\n1 Kīlauea               2024-09-15 2024-09-20        5 TRUE         NA\n2 Kīlauea               2024-06-03 2024-06-03        0 TRUE         NA\n3 Atka Volcanic Complex 2024-03-27 2024-03-27        0 TRUE         NA\n4 Ahyi                  2024-01-01 2024-03-27       86 TRUE         NA\n5 Kanaga                2023-12-18 2023-12-18        0 TRUE          1\n6 Ruby                  2023-09-14 2023-09-15        1 TRUE          1\n# ℹ 69 more rows"},{"path":"data-frames.html","id":"xlsx-file","chapter":"4 Data Frames","heading":"4.2.7 XLS(X) file","text":"Data also commonly encountered XLS/XLSX spreadsheet file, can read readxl’s read_excel() function. eruptions_recent.xlsx file dataset exported XLSX. Since XLSX text format, can’t embedded , ’s first rows look like opened Excel:Unfortunately, readxl support URLs data must downloaded loading.Oops, looks like start/stop read datetime instead date. ’ll learn later fix , now ’re moving .","code":"\n# load readxl, which is NOT core tidyverse, so must be imported explicitly\nlibrary(readxl)\n# I already have the file downloaded to data/\n# inside my current working directory\ndir.exists(\"data/\")[1] TRUE\nfile.exists(\"data/eruptions_recent.xlsx\")[1] TRUE\neruptions_recent <- read_xlsx(\"data/eruptions_recent.xlsx\")\n# print first 6 lines\nprint(eruptions_recent, n = 6)# A tibble: 75 × 6\n  volcano            start               stop                duration confirmed   vei\n  <chr>              <dttm>              <dttm>                 <dbl> <chr>     <dbl>\n1 Kīlauea            2024-09-15 00:00:00 2024-09-20 00:00:00        5 TRUE         NA\n2 Kīlauea            2024-06-03 00:00:00 2024-06-03 00:00:00        0 TRUE         NA\n3 Atka Volcanic Com… 2024-03-27 00:00:00 2024-03-27 00:00:00        0 TRUE         NA\n4 Ahyi               2024-01-01 00:00:00 2024-03-27 00:00:00       86 TRUE         NA\n5 Kanaga             2023-12-18 00:00:00 2023-12-18 00:00:00        0 TRUE          1\n6 Ruby               2023-09-14 00:00:00 2023-09-15 00:00:00        1 TRUE          1\n# ℹ 69 more rows"},{"path":"data-frames.html","id":"extra-arguments","chapter":"4 Data Frames","heading":"4.2.8 Extra arguments","text":"files prepared easily imported without needing additional arguments, general ’s common need set arguments functions get import properly. BRIEF selection useful arguments available, loosely ordered order importance.arguments can used several ways, e.g. may accept either TRUE/FALSE vector numbers strings, etc. may different behavior depending input. highlight common usages , always see help page details!read_csv(), read_tsv(), read_delim() functions readr share single help page, many arguments common (, see help page). useful additional arguments include:col_names controls handling column names.\ndefault value TRUE, first row file used column names,\nset FALSE, placeholder names used, first line file treated data,\nset character vector, vector used column names, first row file treated data.\ndefault value TRUE, first row file used column names,set FALSE, placeholder names used, first line file treated data,set character vector, vector used column names, first row file treated data.col_types controls handling column types.\nbest way set compact, single-word string letter represents order left right column type use:\nd = double (.e. “normal” numeric value)\nn = number, special readr format parses “human readable” non-standard numbers “$1,000” “150%” (closely related parse_number() function section 3.10)\nl = logical, .e. TRUE/FALSE\nD = date, works dates standard format like \"YYYY-MM-DD\"; parse non-standard formats\nc = character, text data well data non-standard format, parsed later\n_ - skip column\n\n\nexample, suppose data frame order left right numeric column, date column, character column, column want skip, non-standard column needs parsed later; set col_types = \"dDc_c\" specify .best way set compact, single-word string letter represents order left right column type use:\nd = double (.e. “normal” numeric value)\nn = number, special readr format parses “human readable” non-standard numbers “$1,000” “150%” (closely related parse_number() function section 3.10)\nl = logical, .e. TRUE/FALSE\nD = date, works dates standard format like \"YYYY-MM-DD\"; parse non-standard formats\nc = character, text data well data non-standard format, parsed later\n_ - skip column\nd = double (.e. “normal” numeric value)n = number, special readr format parses “human readable” non-standard numbers “$1,000” “150%” (closely related parse_number() function section 3.10)l = logical, .e. TRUE/FALSED = date, works dates standard format like \"YYYY-MM-DD\"; parse non-standard formatsc = character, text data well data non-standard format, parsed later_ - skip columnna sets vector values treated missing, default c(\"\", \"NA\"), .e. empty strings \"NA\" treated missing.comment data files comment lines, usually (always) beginning hashtag # character. lines can ignored setting comment = \"#\" example.skip let’s skip set number lines beginning file.n_max allows setting maximum number lines read file.id useful filename contains important information (common importing data split many files). Setting id = TRUE saves name id column.show_col_types can set FALSE silence diagnostic messages shown importing.read_excel() function readxl also useful extra arguments. , similar slightly different, unique (, see help page). Brief selection important arguments:sheet range unique read_excel() control sheet (.e. tabs bottom) range (.e. rectangular region spreadsheet) read data .\nsheet (defaults first sheet) can either name, number indicating position, even included range specification.\nrange (defaults entire range) can specified variety different ways, commonly might something like \"A2:D6\" includes cells columns -D rows 2-6. See page examples syntax.\nsheet (defaults first sheet) can either name, number indicating position, even included range specification.range (defaults entire range) can specified variety different ways, commonly might something like \"A2:D6\" includes cells columns -D rows 2-6. See page examples syntax.col_names behaves exactly : default TRUE uses first row names, FALSE uses generic placeholder names, can also directly set names character vectorcol_types similar, instead compact string notation, must use character vector specify column type, “numeric”, “logical”, “date”, “text”, “skip” possible valuesna: also behaves accepts vector values represent missing data; difference defaults \"\"skip behaves , let’s skip lines beginning.n_max also behaves sets maximum number lines read.","code":""},{"path":"data-frames.html","id":"paths-file-management","chapter":"4 Data Frames","heading":"4.2.9 Paths & file management","text":"also need briefly discuss paths revisit file management. Previously, download data file import local storage. many first time R users, nontrivial task.R, import downloaded file, must provide valid file path, just reference file’s location system. Paths always relative current working directory. remembered set working directory correctly, Rstudio session runs place current Rmd file (knits ), path also correct, everything work time, errors.data file directory Rmd file, can reference just using name. example, suppose ’re working hw01.Rmd directories look like :Since hw01_data.csv directory hw01.Rmd, can import simply read_csv(\"hw01_data.csv\"), assuming working directory set correctly. However, data file subdirectory called data/, .e. like :, import need write read_csv(\"data/hw01_data.csv\") R know first go data/ directory searching hw01_data.csv load. instead, data file one level , like :, import need write read_csv(\"../hw01_data.csv\") ../ means go directory level (.e. exit current folder) searching hw01_data.csv load.’s important note single correct way manage files, long organized can easily find need. However, strong preference, recommend follow file organization structure introduced section 1.3, .e. setup directories like :, long always following, things always just work:Always put homework/discussion Rmd files homework/hw##/ discussion/ds##/ ## assignment number.Always put data data/ directory, exactly 2 levels hw## ds## directories.Always reference data files like \"../../data/data_file.csv\" tell R go 2 levels current directory, take main STAT240/ directory, descend data/ search data_file.csv.’re trouble finding importing file, additional tips may help:R, can also TAB autocomplete paths. Make sure working directory set, start path \"\", place cursor quotes, hit TAB. see popup menu showing files current directory. , either select subdirectory TAB , type ../ go directory level, repeat steps necessary find file, hit ENTER confirm selection.’re desperate, can also use graphical readr import tool found Environment tab, opens dialog box can browse file, set arguments convenient dropdown menus, see preview data look like settings, best : corner can see code generated can can copy Rmd file. always, make sure working directory set beforehand!Paths R always use forward / slashes, NEVER back \\ slashes, even though back slashes used Windows file systems. just R’s syntax.","code":"..\n└── STAT240/\n    └── homework/\n        └── hw01/\n            ├── hw01.Rmd\n            └── hw01_data.csv..\n└── STAT240/\n    └── homework/\n        └── hw01/\n            ├── hw01.Rmd\n            └── data/\n                 └── hw01_data.csv└── STAT240/\n    └── homework/\n        ├── hw01_data.csv\n        └── hw01/\n            └── hw01.Rmd..\n└── STAT240/\n    │\n    ├── data/\n    │   ├── data_A.csv\n    │   ├── data_B.tsv\n    │   ├── data_C.xlsx\n    │   :    :\n    │\n    ├── discussion/\n    │   │\n    │   ├── ds01/\n    │   │   └── ds01.Rmd\n    │   │\n    │   ├── ds02/\n    │   :   └── ds02.Rmd\n    │\n    ├── homework/\n    │   │\n    │   ├── hw01/\n    │   │   └── hw01.Rmd\n    │   │\n    │   ├── hw02/\n    │   :   └── hw02.Rmd\n    │\n    ├── notes/\n    ├── project/\n    ├── other/\n    :"},{"path":"data-frames.html","id":"working-with-data-frames","chapter":"4 Data Frames","heading":"4.3 Working with data frames","text":"using data frames extensively throughout class. Let’s start learning basic manipulations . First, ’m going reload eruptions_recent dataset using CSV file, correct start/stop columns.","code":"\n# set R to print fewer rows by default, to save space in demos below\noptions(pillar.print_min = 6)\n\n# reload dataset\neruptions_recent <- read_csv(\n  \"https://bwu62.github.io/stat240-revamp/data/eruptions_recent.csv\",\n  show_col_types = FALSE\n)\n# print first few rows\neruptions_recent# A tibble: 75 × 6\n  volcano               start      stop       duration confirmed   vei\n  <chr>                 <date>     <date>        <dbl> <lgl>     <dbl>\n1 Kīlauea               2024-09-15 2024-09-20        5 TRUE         NA\n2 Kīlauea               2024-06-03 2024-06-03        0 TRUE         NA\n3 Atka Volcanic Complex 2024-03-27 2024-03-27        0 TRUE         NA\n4 Ahyi                  2024-01-01 2024-03-27       86 TRUE         NA\n5 Kanaga                2023-12-18 2023-12-18        0 TRUE          1\n6 Ruby                  2023-09-14 2023-09-15        1 TRUE          1\n# ℹ 69 more rows"},{"path":"data-frames.html","id":"basic-df","chapter":"4 Data Frames","heading":"4.3.1 Basic operations","text":"basic operations working data frames: nrow(), ncol(), dim() can show number rows /columns; summary() can show quick summary column; names()/colnames() can get set column names; rownames() can get set row names.","code":"\n# get number of rows and columns\nnrow(eruptions_recent)[1] 75\nncol(eruptions_recent)[1] 6\n# get both together using dim()\ndim(eruptions_recent)[1] 75  6\n# show different summary of each column, depending on the column type\nsummary(eruptions_recent)   volcano              start                 stop               duration     \n Length:75          Min.   :2001-02-02   Min.   :2001-04-15   Min.   :   0.0  \n Class :character   1st Qu.:2006-06-18   1st Qu.:2006-11-17   1st Qu.:   5.5  \n Mode  :character   Median :2011-04-23   Median :2011-09-01   Median :  62.0  \n                    Mean   :2012-08-24   Mean   :2013-02-12   Mean   : 172.0  \n                    3rd Qu.:2019-07-19   3rd Qu.:2019-10-15   3rd Qu.: 184.0  \n                    Max.   :2024-09-15   Max.   :2024-09-20   Max.   :1491.0  \n                                                                              \n confirmed            vei       \n Mode :logical   Min.   :0.000  \n FALSE:4         1st Qu.:1.000  \n TRUE :71        Median :2.000  \n                 Mean   :1.881  \n                 3rd Qu.:3.000  \n                 Max.   :4.000  \n                 NA's   :8      \n# show names of the variable columns\n# note names() and colnames() are completely identical for data frames\nnames(eruptions_recent)[1] \"volcano\"   \"start\"     \"stop\"      \"duration\"  \"confirmed\" \"vei\"      \n# you can also set individual, specific, or even all names\nnames(eruptions_recent)[2] <- \"START\"\nnames(eruptions_recent)[c(1, 4:6)] <- c(\"VOLCANO\", \"DURATION\", \"CONFIRMED\", \"VEI\")\nnames(eruptions_recent)[1] \"VOLCANO\"   \"START\"     \"stop\"      \"DURATION\"  \"CONFIRMED\" \"VEI\"      \n# let's reset the names back to their original values\nnames(eruptions_recent) <- c(\n  \"volcano\", \"start\", \"stop\", \"duration\", \"confirmed\", \"vei\"\n)\n# data frames may also have row names, though most don't\n# if there are no row names, they just show as numbers\n# (this is not generally a commonly used feature)\nrownames(eruptions_recent) [1] \"1\"  \"2\"  \"3\"  \"4\"  \"5\"  \"6\"  \"7\"  \"8\"  \"9\"  \"10\" \"11\" \"12\" \"13\" \"14\" \"15\" \"16\"\n[17] \"17\" \"18\" \"19\" \"20\" \"21\" \"22\" \"23\" \"24\" \"25\" \"26\" \"27\" \"28\" \"29\" \"30\" \"31\" \"32\"\n[33] \"33\" \"34\" \"35\" \"36\" \"37\" \"38\" \"39\" \"40\" \"41\" \"42\" \"43\" \"44\" \"45\" \"46\" \"47\" \"48\"\n[49] \"49\" \"50\" \"51\" \"52\" \"53\" \"54\" \"55\" \"56\" \"57\" \"58\" \"59\" \"60\" \"61\" \"62\" \"63\" \"64\"\n[65] \"65\" \"66\" \"67\" \"68\" \"69\" \"70\" \"71\" \"72\" \"73\" \"74\" \"75\""},{"path":"data-frames.html","id":"subsetting-data-frames","chapter":"4 Data Frames","heading":"4.3.2 Subsetting data frames","text":"can extract manipulate subsets data frame along either dimension. commonly, may want use $ either pull single column vector, modify existing column -place, even create new column.can also use [] [[]] subset columns name position, difference [] returns data frame [[]] returns vector directly.[] operator additional usage [rows,cols] rows, cols can vectors specifying subsets name position. Leaving one empty means return .commonly used data science split dataset. example, suppose wanted randomly partition data 80% training 20% testing set. can first use sample(n,x) randomly select x rows n, use positive negative row subsetting syntax get partitions:ever need recombine , just use rbind() bind rows together multiple data frames, long exact columns (name type).","code":"\n# extract the duration column\neruptions_recent$duration [1]    5    0    0   86    0    1  115   13  205  717  489   39   36  822  154    0\n[17]  195   30  286   39    6   53  110   62  253 1009  125    3    6   23  519  121\n[33]   44 1021    0    0 1491  131    1    3    2  100   71    0  108   19    2   38\n[49]    8    0  256   29  422    4   98    0   98  188  264  139   58  259   71   41\n[65] 1213   11  509  150  202   63  180    1   11    1   72\n# change the confirmed column to 1s and 0s in-place\neruptions_recent$confirmed <- as.numeric(eruptions_recent$confirmed)\neruptions_recent# A tibble: 75 × 6\n  volcano               start      stop       duration confirmed   vei\n  <chr>                 <date>     <date>        <dbl>     <dbl> <dbl>\n1 Kīlauea               2024-09-15 2024-09-20        5         1    NA\n2 Kīlauea               2024-06-03 2024-06-03        0         1    NA\n3 Atka Volcanic Complex 2024-03-27 2024-03-27        0         1    NA\n4 Ahyi                  2024-01-01 2024-03-27       86         1    NA\n5 Kanaga                2023-12-18 2023-12-18        0         1     1\n6 Ruby                  2023-09-14 2023-09-15        1         1     1\n# ℹ 69 more rows\n# add a new column giving just the year the eruption started in\neruptions_recent$start_year <- year(eruptions_recent$start)\neruptions_recent# A tibble: 75 × 7\n  volcano               start      stop       duration confirmed   vei start_year\n  <chr>                 <date>     <date>        <dbl>     <dbl> <dbl>      <dbl>\n1 Kīlauea               2024-09-15 2024-09-20        5         1    NA       2024\n2 Kīlauea               2024-06-03 2024-06-03        0         1    NA       2024\n3 Atka Volcanic Complex 2024-03-27 2024-03-27        0         1    NA       2024\n4 Ahyi                  2024-01-01 2024-03-27       86         1    NA       2024\n5 Kanaga                2023-12-18 2023-12-18        0         1     1       2023\n6 Ruby                  2023-09-14 2023-09-15        1         1     1       2023\n# ℹ 69 more rows\n# extract the vei column, keeping the result as a data frame\neruptions_recent[\"vei\"]# A tibble: 75 × 1\n    vei\n  <dbl>\n1    NA\n2    NA\n3    NA\n4    NA\n5     1\n6     1\n# ℹ 69 more rows\n# extract the same column but by position and directly as a vector\neruptions_recent[[6]] [1] NA NA NA NA  1  1  3  0  1  0  2  2  1  2  0  3  1 NA  3  1  1  2  2  1  3  3  3\n[28]  3  3  2  1  3  3  0  2  2  2  2 NA  2  3  1  2  2  3  2  4  4  1 NA  2  2  2  1\n[55]  1  2  2  1  3  3  1  2  1  2  2  1  3  2  2  3  1 NA  1  0  3\n# extract just the first 5 start/stop times\neruptions_recent[1:5, c(\"start\", \"stop\")]# A tibble: 5 × 2\n  start      stop      \n  <date>     <date>    \n1 2024-09-15 2024-09-20\n2 2024-06-03 2024-06-03\n3 2024-03-27 2024-03-27\n4 2024-01-01 2024-03-27\n5 2023-12-18 2023-12-18\n# extract the entire 10th row\neruptions_recent[10, ]# A tibble: 1 × 7\n  volcano start      stop       duration confirmed   vei start_year\n  <chr>   <date>     <date>        <dbl>     <dbl> <dbl>      <dbl>\n1 Kīlauea 2021-09-29 2023-09-16      717         1     0       2021\n# you can also use negative indices to remove specific items\n# e.g. this removes rows 1-10 and also removes the 7th column (start_year)\neruptions_recent[-(1:10), -7]# A tibble: 65 × 6\n  volcano       start      stop       duration confirmed   vei\n  <chr>         <date>     <date>        <dbl>     <dbl> <dbl>\n1 Pavlof        2021-08-05 2022-12-07      489         1     2\n2 Pagan         2021-07-29 2021-09-06       39         1     2\n3 Veniaminof    2021-02-28 2021-04-05       36         1     1\n4 Semisopochnoi 2021-02-02 2023-05-05      822         1     2\n5 Kīlauea       2020-12-20 2021-05-23      154         1     0\n6 Cleveland     2020-06-01 2020-06-01        0         1     3\n# ℹ 59 more rows\n# define n as total number of rows\nn <- nrow(eruptions_recent)\n# randomly sample 20% of numbers from 1 to n as test rows\n# (sample auto-rounds inputs down to integers if they're not whole)\ntest_rows <- sample(n, 0.2*n)\ntest_rows [1] 68 39  1 34 43 14 59 51 21 54  7  9 15 67 37\n# split dataset using the subsetting syntax we just learned\neruptions_recent_test  <- eruptions_recent[ test_rows, ]\neruptions_recent_train <- eruptions_recent[-test_rows, ]\neruptions_recent_test# A tibble: 15 × 7\n   volcano                  start      stop       duration confirmed   vei start_year\n   <chr>                    <date>     <date>        <dbl>     <dbl> <dbl>      <dbl>\n 1 Shishaldin               2004-02-17 2004-07-16      150         1     2       2004\n 2 Cleveland                2010-09-11 2010-09-12        1         0    NA       2010\n 3 Kīlauea                  2024-09-15 2024-09-20        5         1    NA       2024\n 4 Mariana Back-Arc Segmen… 2013-02-13 2015-12-01     1021         1     0       2013\n 5 Cleveland                2009-10-02 2009-12-12       71         1     2       2009\n 6 Semisopochnoi            2021-02-02 2023-05-05      822         1     2       2021\n 7 Cleveland                2006-02-06 2006-10-28      264         1     3       2006\n 8 Anatahan                 2007-11-27 2008-08-09      256         1     2       2007\n 9 Great Sitkin             2019-06-01 2019-06-07        6         1     1       2019\n10 Pagan                    2006-12-04 2006-12-08        4         1     1       2006\n11 Shishaldin               2023-07-11 2023-11-03      115         1     3       2023\n12 Ahyi                     2022-11-18 2023-06-11      205         1     1       2022\n13 Kīlauea                  2020-12-20 2021-05-23      154         1     0       2020\n14 Anatahan                 2004-04-12 2005-09-03      509         1     3       2004\n15 Cleveland                2011-07-19 2015-08-18     1491         1     2       2011\neruptions_recent_train# A tibble: 60 × 7\n  volcano               start      stop       duration confirmed   vei start_year\n  <chr>                 <date>     <date>        <dbl>     <dbl> <dbl>      <dbl>\n1 Kīlauea               2024-06-03 2024-06-03        0         1    NA       2024\n2 Atka Volcanic Complex 2024-03-27 2024-03-27        0         1    NA       2024\n3 Ahyi                  2024-01-01 2024-03-27       86         1    NA       2024\n4 Kanaga                2023-12-18 2023-12-18        0         1     1       2023\n5 Ruby                  2023-09-14 2023-09-15        1         1     1       2023\n6 Mauna Loa             2022-11-27 2022-12-10       13         1     0       2022\n# ℹ 54 more rows\n# note the resulting rows will be in a different order,\n# but it's the same data frame we started out with\neruptions_recent_recombined <- rbind(eruptions_recent_test, eruptions_recent_train)\neruptions_recent_recombined# A tibble: 75 × 7\n  volcano                   start      stop       duration confirmed   vei start_year\n  <chr>                     <date>     <date>        <dbl>     <dbl> <dbl>      <dbl>\n1 Shishaldin                2004-02-17 2004-07-16      150         1     2       2004\n2 Cleveland                 2010-09-11 2010-09-12        1         0    NA       2010\n3 Kīlauea                   2024-09-15 2024-09-20        5         1    NA       2024\n4 Mariana Back-Arc Segment… 2013-02-13 2015-12-01     1021         1     0       2013\n5 Cleveland                 2009-10-02 2009-12-12       71         1     2       2009\n6 Semisopochnoi             2021-02-02 2023-05-05      822         1     2       2021\n# ℹ 69 more rows"},{"path":"data-exploration.html","id":"data-exploration","chapter":"Data Exploration","heading":"Data Exploration","text":"One first important things obtaining dataset explore data thoroughly using combination summary statistics visualizations. Careful data exploration help following:Understand dataset,Discover interesting, sometimes unexpected patterns trends,Identify potential sources problems (e.g. errors, biases, obstacles later analysis),Formulate meaningful questions ask using data, andChoose appropriate path analysis.next section, cover variety data exploration techniques, starting descriptive (.e. summary) statistics, move data visualization (.e. graphing/plotting).","code":""},{"path":"descriptive.html","id":"descriptive","chapter":"5 Descriptive Statistics","heading":"5 Descriptive Statistics","text":"Descriptive statistics people think hear word “statistics”, .e. collection numbers summarize data. often good starting point exploring newly encountered dataset.statistic just number computed sample data, often intended summarize data specific way. Virtually function ingests sample outputs number can referred statistic.","code":""},{"path":"descriptive.html","id":"measures-of-central-tendency","chapter":"5 Descriptive Statistics","heading":"5.1 Measures of central tendency","text":"MANY statistics aim quantify “center” sample. collectively referred measures central tendency, also often called averages short. 3 common averages mean, median, mode.“Average” can refer measure central tendency, .e. average can refer either mean, median, mode (measures). Thus, ’s generally recommended specify measure ’re referring avoid using word “average” (unless ’re strategically ambiguous).","code":""},{"path":"descriptive.html","id":"sample-mean","chapter":"5 Descriptive Statistics","heading":"5.1.1 Mean","text":"arithmetic mean sample, commonly referred just “mean”, people think hear “average”. sum sample divided sample size. Formally, given sample \\(x_1,x_2,\\dots,x_n\\) mean \\(\\bar{x}\\) defined :\\[\\bar{x}=\\frac1n\\sum_{=1}^nx_i=\\frac{x_1+x_2+\\cdots+x_n}n\\]mean may seem natural intuitive central tendency measure, number drawbacks:mean sensitive “outliers” recommended heavily skewed data.mean generally used truly numeric data, .e. data values intrinsically tied quantitative observable.\ncommon example data “truly” numeric ordinal data, commonly generated Likert scales (e.g. strongly disagree, disagree, neutral, agree, strongly agree), star-based review systems (e.g. ★★★★☆).20\ncommon example data “truly” numeric ordinal data, commonly generated Likert scales (e.g. strongly disagree, disagree, neutral, agree, strongly agree), star-based review systems (e.g. ★★★★☆).20Continuing previous 21st century US volcanic eruptions dataset, can use mean() , example, find mean length eruptions days:vector missing values, .e. NA, mean() statistical functions return NA. safety measure, help remind handle missing values appropriately attempting analysis. can tell R ignore NAs proceed setting na.rm = TRUE function.","code":"\n# import all core tidyverse packages, since we will need several\n# (again, this imports readr, tibble, and stringr, as well as several others)\nlibrary(tidyverse)\n# reload dataset\neruptions_recent <- read_csv(\n  \"https://bwu62.github.io/stat240-revamp/data/eruptions_recent.csv\",\n  show_col_types = FALSE\n)\n# compute mean duration of eruptions\nmean(eruptions_recent$duration)[1] 172.0133\n# we can check this agrees with our mathematical definition\nsum(eruptions_recent$duration) / length(eruptions_recent$duration)[1] 172.0133\n# example of NAs causing mean to fail\nmean(c(1, 6, NA, 2))[1] NA\n# setting na.rm = TRUE tells R to safely ignore NAs\nmean(c(1, 6, NA, 2), na.rm = TRUE)[1] 3"},{"path":"descriptive.html","id":"median","chapter":"5 Descriptive Statistics","heading":"5.1.2 Median","text":"median, common alternative mean, defined “middle” number sorted sample. Formally, ’s value greater equal half sample, also less equal half sample. even number observations, median isn’t uniquely defined commonly taken mean middle two numbers.median generally recommended mean following situations:“outliers”, data significantly skewed.data truly numeric, e.g. ordinal data.can use median() find median length eruptions days:Note median length eruptions, 62, significantly smaller mean, 172.0133333, data extremely skewed, .e. extremely long eruptions, pull mean much higher (since ’s sensitive extreme values).’s worth mentioning word “outliers” isn’t well defined formally actually surprisingly tricky subject statistics. , “outlier” just loosely refers observations differ dramatically compared rest data. ’s important remember “outliers” errors; may fact point new information ’re aware .Statistics less sensitive “outliers” called robust. E.g. median robust mean.","code":"\n# compute median duration of eruptions\nmedian(eruptions_recent$duration)[1] 62\n# check to see if it satisfies the formal definition\n# recall from the logical vectors section from Chapter 3 that\n# mean() of a logical vector gives the proportion of TRUEs\nc(\n  mean(\n    eruptions_recent$duration >= median(eruptions_recent$duration)\n  ),\n  mean(\n    eruptions_recent$duration <= median(eruptions_recent$duration)\n  )\n)[1] 0.5066667 0.5066667"},{"path":"descriptive.html","id":"mode","chapter":"5 Descriptive Statistics","heading":"5.1.3 Mode","text":"mode oft-maligned black sheep central tendency family. defined common observation, .e. observation occurs number times sample. ’s primarily intended categorical data (e.g. male vs female) though also relevance distributions (much later).mode course also form “average”. example, statistics show roughly 95% lumberjacks US male. therefore accurate say “average American lumberjack male”. fact, categorical data—ubiquitous—possible measure central tendency.Unfortunately, base R convenient function computing mode (function mode() completely unrelated), can easily either define import Mode() DescTools package","code":"\n# this defines a simple Mode function using mostly commands we already know\n# explanation: table tabulates observations, then\n#              sort(- ...) sorts by descending order, then\n#              names(...)[1] extracts the name of the first item\n# note this function doesn't return multiple modes if there are more than 1\nMode <- \\(x) names(sort(-table(x)))[1]\n# find the volcano with the most number of eruptions\nMode(eruptions_recent$volcano)[1] \"Cleveland\"\n# how many eruptions has Cleveland had since 2001?\nsum(eruptions_recent$volcano == \"Cleveland\")[1] 13\n# the DescTools package also has a Mode function,\n# which correctly handles ties and also gives the frequency\n# make sure to install it before using: install.packages(\"DescTools\")\nDescTools::Mode(eruptions_recent$volcano)[1] \"Cleveland\"\nattr(,\"freq\")\n[1] 13"},{"path":"descriptive.html","id":"modality","chapter":"5 Descriptive Statistics","heading":"5.1.4 Aside: Modality","text":"common distributions 1 mode, .e. unimodal, distributions may 2 modes, case ’re called bimodal (2 modes) multimodal (≥2 modes). ’s example bimodal distribution:","code":""},{"path":"descriptive.html","id":"aside-2-visualizing-meanmedianmode","chapter":"5 Descriptive Statistics","heading":"5.1.5 Aside 2: Visualizing mean/median/mode","text":"’s good diagram showing visual interpretations mean, median, mode skewed unimodal distribution.21Mode: point “peak” distribution .Median: point splits distribution two equal areas.Mean: point distribution balance , cut .","code":""},{"path":"descriptive.html","id":"other-measures","chapter":"5 Descriptive Statistics","heading":"5.1.6 Other measures","text":"mean, median, mode far common measures central tendency, ones need know course. However, thought might worth briefly mentioning averages interesting applications just fun:quadratic mean, also known root mean square, defined sample \\(x_1,\\dots,x_n\\) \\(\\sqrt{\\frac1n(x_1^2+\\cdots+x_n^2)}\\). shows statistical contexts, e.g. standard deviation almost quadratic mean difference observation arithmetic mean, except dividing \\(n-1\\) instead \\(n\\) (corrects small bias called Bessel’s correction). also applications model evaluation, statistical physics, electronics engineering, signal analysis, .geometric mean defined \\(\\sqrt[n]{x_1x_2\\cdots x_n}\\) valid positive-valued data. useful data multiplicative rather additive nature, e.g. growth rates, interest rates, comparisons relative performance benchmarks, etc. many applications finance economics, areas optical engineering, even cinematography.\nNote logarithm geometric mean sample equal arithmetic mean logarithm sample. words, geometric mean viewed log scale “looks like” arithmetic mean linear scale. Log transforms important tool certain contexts, see page brief overview.\nNote logarithm geometric mean sample equal arithmetic mean logarithm sample. words, geometric mean viewed log scale “looks like” arithmetic mean linear scale. Log transforms important tool certain contexts, see page brief overview.harmonic mean defined \\(\\left(\\frac{x_1^{-1}+\\,\\cdots\\,+x_n^{-1}}{n}\\right)^{\\!-1}\\), .e. reciprocal arithmetic mean reciprocals data, also typically used positive-valued data. turns correct mean use certain applications involving rate, ratio, time values. also applications machine learning, physics, finance, even baseball.Collectively, arithmetic, geometric, harmonic means also known Pythagorean means.just short list; host means exist. , need know advanced means; expected know arithmetic mean, median, mode.","code":""},{"path":"descriptive.html","id":"measures-of-spread","chapter":"5 Descriptive Statistics","heading":"5.2 Measures of spread","text":"Arguably next important set summary statistics measures central tendency measures spread, aim quantify “spread ” dataset . Variance standard deviation far common measures, IQR range also sometimes useful.Unlike measures central tendency, measures spread typically location-agnostic, .e. don’t change entire dataset shifted constant. Formally, sample \\(x_1,\\dots,x_n\\) sample \\(x_1+c,\\,\\dots,\\,x_n+c\\) spread \\(c\\).","code":""},{"path":"descriptive.html","id":"sample-var-sd","chapter":"5 Descriptive Statistics","heading":"5.2.1 Variance (and standard deviation)","text":"Let’s get easy one way first. Standard deviation always defined (positive) square root variance. ’s variance ? variance sample defined :\\[s^2=\\frac1{n-1}\\sum_{=1}^n(x_i-\\bar{x})^2=\\frac{(x_1-\\bar{x})^2+\\cdots+(x_n-\\bar{x})^2}{n-1}\\]Basically, ’s mean squared-distance mean, \\(\\bar{x}\\), except use \\(n-1\\) instead \\(n\\) correct small bias. example, can compute variance duration eruptions, days squared:words, “average” squared difference eruption’s duration mean duration 87k days2.Note units variance squared data units. makes inconvenient work , since means directly compared data. instead often work square root, .e. standard deviation:\\[s=\\sqrt{\\frac1{n-1}\\sum_{=1}^n(x_i-\\bar{x})^2}\\]can thought “average” distance mean given observation. can computed using sd():words, “average” distance days duration eruptions mean 294.9 days.Since standard deviation defined square root variance (thus variance always square standard deviation), knowing one quantities enables also easily compute .variance standard deviation common measures spread, like arithmetic mean, also sensitive outliers may suitable highly skewed data.","code":"\n# compute variance of duration\nvar(eruptions_recent$duration)[1] 86978.66\n# compute the standard deviation of duration\nsd(eruptions_recent$duration)[1] 294.9215\n# you can check this is equal to sqrt(var(...))\nsqrt(var(eruptions_recent$duration))[1] 294.9215"},{"path":"descriptive.html","id":"iqr","chapter":"5 Descriptive Statistics","heading":"5.2.2 Interquartile range","text":"interquartile range, also called IQR, distance 1st 3rd quartile. understand , let’s first briefly review percentiles.","code":""},{"path":"descriptive.html","id":"percentiles","chapter":"5 Descriptive Statistics","heading":"5.2.3 Percentiles","text":"Percentiles generalization median. Recall median data point just barely greater equal 50% data. Similarly, \\(p\\)th percentile data point just barely greater equal \\(p\\)% data. Percentiles computed using quantile(x, probs = p) function, x data vector, p desired percentile (vector thereof). example, can compute 0th, 25th, 50th, 75th, 100th percentiles eruption duration :5 numbers correspond min, 1st quartile (\\(Q_1\\)), median, 3rd quartile (\\(Q_3\\)), max respectively, often collectively known five-number summary. \\(Q_1\\) \\(Q_3\\) also frequently called upper lower hinges dataset.difference \\(Q_3-Q_1\\) called interquartile range, often used instead variance/standard deviation outliers/skewness significant concern dataset due increased robustness. IQR can computed IQR() function:","code":"\nquantile(eruptions_recent$duration, probs = c(0, 0.25, 0.5, 0.75, 1))    0%    25%    50%    75%   100% \n   0.0    5.5   62.0  184.0 1491.0 \n# compute the interquartile range of eruption durations\nIQR(eruptions_recent$duration)[1] 178.5"},{"path":"descriptive.html","id":"range","chapter":"5 Descriptive Statistics","heading":"5.2.4 Range","text":"range crudest measures spread, defined difference minimum maximum sample. even sensitive outliers variance/standard deviation, thus less commonly used formal statistical settings. sometimes practical measures extremely small datasets.R, range() function gives minimum maximum vector; get actual range, can use diff() take difference two:","code":"\n# range() gives the min/max vector\nrange(eruptions_recent$duration)[1]    0 1491\n# diff() of range() gives the true statistical range\ndiff(range(eruptions_recent$duration))[1] 1491"},{"path":"descriptive.html","id":"other-measures-1","chapter":"5 Descriptive Statistics","heading":"5.2.5 Other measures","text":"variance, standard deviation, IQR, range measures spread need know course, measures just fun:median absolute deviation MAD exactly sounds like, median absolute value deviation median. ’s robust (.e. outlier-resistant) version standard deviation.Gini coefficient another interesting measure spread (one favorites). ’s one list dimensionless (.e. always “pure” number units) always 0 1 non-negative data. ’s defined half relative mean absolute difference, defined average absolute difference \\(|x_i-x_j|\\) pairs observations divided arithmetic mean. ’s commonly used economics characterize inequality, 0 total equality (e.g. everyone exactly amount wealth) 1 total inequality (e.g. one person wealth everyone else none)., just subset possible measures spread, need know measures; expected know variance, standard deviation, IQR, range.","code":""},{"path":"descriptive.html","id":"skew","chapter":"5 Descriptive Statistics","heading":"5.3 Skew","text":"Another important summary statistic skew dataset. Skew precise mathematical definition beyond scope course. need know difference positive (also called right) skew negative (also called left) skew dataset.side skewness always side longer tail. longer tail positive (right) side, ’s called positive (right) skew. longer tail negative (left) side, ’s called negative (left) skew., 3 example (unimodal) distributions showing kinds skewness, modes aligned.unskewed, symmetric distribution, mode \\(=\\) median \\(=\\) mean.positive, right skewed distribution, mode \\(<\\) median \\(<\\) mean.negative, left skewed distribution, mean \\(<\\) median \\(<\\) mode.shows , already learned, median robust vs mean “outliers”/skew, words, mean affected “outliers”/skew gets “dragged away” skewness.Visually, mode always “peak”, median splits distribution 2 equal areas, mean center mass shape along horizontal axis (.e. “balancing point”).","code":""},{"path":"data-visualization.html","id":"data-visualization","chapter":"6 Data Visualization","heading":"6 Data Visualization","text":"Descriptive statistics good place start, usually plotting data visually best way fully understand dataset core. next chapter, cover variety common plot types useful exploratory analysis.studying plot type, ’s important keep following questions mind:kind data appropriate plot?create plot?interpret plot?","code":""},{"path":"data-visualization.html","id":"ggplot2","chapter":"6 Data Visualization","heading":"6.1 ggplot2","text":"making plots using ggplot2 package also core Tidyverse package. offers robust syntax easily creating modifying plots (link cheat sheet).making ggplot2 plot, ’s important remember everything layer add onto base object using + just like adding numbers. Whether ’re adding plot, faceting structure, changing axes, adding annotations (e.g. title/labels), etc. ’re layers added. may seem strange first, ’ll quickly grasp examples follow.Let’s first import necessary packages. need readr ggplot2 reading plotting data. convenience, ’ll just load core Tidyverse packages.","code":"\n# if you need to, reimport all core tidyverse packages\nlibrary(tidyverse)\n\n# optional: disable showing col_types by default in readr import functions\noptions(readr.show_col_types = FALSE)\n\n# optional: set a prettier theme and colorblind-friendly palette for plots\n#           (also looks better if printed with most printers, even in b/w)\ntheme_set(theme_bw())\noptions(ggplot2.discrete.fill = \\(...) scale_fill_brewer(..., palette = \"Set2\"),\n        ggplot2.discrete.colour = \\(...) scale_color_brewer(..., palette = \"Dark2\"))"},{"path":"data-visualization.html","id":"palmer-penguins","chapter":"6 Data Visualization","heading":"6.2 Palmer penguins","text":"properly demonstrate plots, need slightly feature-rich dataset. Let’s import Palmer penguins dataset readily usable good set variables.22 ’ve removed rows NAs convenience, ’s link file: penguins.csv.column variables intuitively named able guess meaning; see penguins help page info variables well papers detailing data gathering process.","code":"\n# load in the penguins dataset\n# (note: a few rows with NAs have been removed for simplicity)\npenguins <- read_csv(\"https://bwu62.github.io/stat240-revamp/data/penguins.csv\")\n# print the first few rows of the data frame to check;\n# this data frame is now too wide for our screen,\n# you can see some columns are cut off\nprint(penguins, n = 5)# A tibble: 333 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n  <chr>   <chr>              <dbl>         <dbl>             <dbl>       <dbl> <chr> \n1 Adelie  Torgersen           39.1          18.7               181        3750 male  \n2 Adelie  Torgersen           39.5          17.4               186        3800 female\n3 Adelie  Torgersen           40.3          18                 195        3250 female\n4 Adelie  Torgersen           36.7          19.3               193        3450 female\n5 Adelie  Torgersen           39.3          20.6               190        3650 male  \n# ℹ 328 more rows\n# ℹ 1 more variable: year <dbl>\n# let's temporarily increase the width and reprint,\n# so you can see all columns in the data frame\noptions(width = 92)\npenguins# A tibble: 333 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex     year\n   <chr>   <chr>              <dbl>         <dbl>             <dbl>       <dbl> <chr>  <dbl>\n 1 Adelie  Torgersen           39.1          18.7               181        3750 male    2007\n 2 Adelie  Torgersen           39.5          17.4               186        3800 female  2007\n 3 Adelie  Torgersen           40.3          18                 195        3250 female  2007\n 4 Adelie  Torgersen           36.7          19.3               193        3450 female  2007\n 5 Adelie  Torgersen           39.3          20.6               190        3650 male    2007\n 6 Adelie  Torgersen           38.9          17.8               181        3625 female  2007\n 7 Adelie  Torgersen           39.2          19.6               195        4675 male    2007\n 8 Adelie  Torgersen           41.1          17.6               182        3200 female  2007\n 9 Adelie  Torgersen           38.6          21.2               191        3800 male    2007\n10 Adelie  Torgersen           34.6          21.1               198        4400 male    2007\n# ℹ 323 more rows\n# reset width to its original value\noptions(width = 85)\n# another option is the glimpse() function which prints sideways,\n# avoiding the hidden columns due to insufficient width issue\nglimpse(penguins)Rows: 333\nColumns: 8\n$ species           <chr> \"Adelie\", \"Adelie\", \"Adelie\", \"Adelie\", \"Adelie\", \"Adelie…\n$ island            <chr> \"Torgersen\", \"Torgersen\", \"Torgersen\", \"Torgersen\", \"Torg…\n$ bill_length_mm    <dbl> 39.1, 39.5, 40.3, 36.7, 39.3, 38.9, 39.2, 41.1, 38.6, 34.…\n$ bill_depth_mm     <dbl> 18.7, 17.4, 18.0, 19.3, 20.6, 17.8, 19.6, 17.6, 21.2, 21.…\n$ flipper_length_mm <dbl> 181, 186, 195, 193, 190, 181, 195, 182, 191, 198, 185, 19…\n$ body_mass_g       <dbl> 3750, 3800, 3250, 3450, 3650, 3625, 4675, 3200, 3800, 440…\n$ sex               <chr> \"male\", \"female\", \"female\", \"female\", \"male\", \"female\", \"…\n$ year              <dbl> 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 200…"},{"path":"data-visualization.html","id":"one-variable-plots","chapter":"6 Data Visualization","heading":"6.3 One-variable plots","text":"Ok, now ’re finally ready learn plots. start simple one-variable plots, .e. plots can made single column data frame. Depending type variable, may decide end choosing several different plot types. Note plot types can also enhanced visualize two variables, soon demonstrate.","code":""},{"path":"data-visualization.html","id":"histogram","chapter":"6 Data Visualization","heading":"6.3.1 Histogram","text":"Histograms plots numeric values grouped “bins” (.e. intervals) count bin plotted bar. extremely effective visualizing distribution single numeric column, allowing easily see shape, spread, even skewness dataset. Histograms one common plots numeric data.following code makes basic histogram R using ggplot.","code":"\nggplot(penguins, aes(x = flipper_length_mm)) + geom_histogram()"},{"path":"data-visualization.html","id":"interpretation","chapter":"6 Data Visualization","heading":"6.3.1.1 Interpretation","text":"Looking plot, can make key observations:distribution flipper length bimodal, .e. 2 peaks: around 190mm 215mm.peak around 190mm higher (.e. numerous) peak around 215mm, comparable spreads.\ncan mean either group observations prominent population studied, perhaps result kind selection sampling bias.\ncan mean either group observations prominent population studied, perhaps result kind selection sampling bias.two modes, 200-205mm, ’s noticeable “gap” comparatively much fewer observations.vast majority observations around 180-220mm, extremes almost low 170mm just slightly 230mm.","code":""},{"path":"data-visualization.html","id":"explanation-of-syntax","chapter":"6 Data Visualization","heading":"6.3.1.2 Explanation of syntax:","text":"code may seem strange first, ’s quick explanation:ggplot() function creates base “plot object”, kind like setting canvas preparation painting. ggplot() takes 2 arguments order:\nfirst argument penguins data frame used plot. general rule, always put data want plot SINGLE data frame pass ggplot().\nsecond argument aesthetic mapping. Think aesthetics choosing display column variables data frame. ’s brief list common aesthetics can map:\nx controls horizontal axis,\ny controls vertical axis,\ncolor fill control point/line/boundary color inside/fill colors respectively,\nshape size control point shapes sizes respectively,\nlinetype controls type line (.e. solid, dashed, dotted, etc.)\n\nfirst argument penguins data frame used plot. general rule, always put data want plot SINGLE data frame pass ggplot().second argument aesthetic mapping. Think aesthetics choosing display column variables data frame. ’s brief list common aesthetics can map:\nx controls horizontal axis,\ny controls vertical axis,\ncolor fill control point/line/boundary color inside/fill colors respectively,\nshape size control point shapes sizes respectively,\nlinetype controls type line (.e. solid, dashed, dotted, etc.)\nx controls horizontal axis,y controls vertical axis,color fill control point/line/boundary color inside/fill colors respectively,shape size control point shapes sizes respectively,linetype controls type line (.e. solid, dashed, dotted, etc.)base plot object setup data frame aesthetic mapping, simply need “add ” plot layer like geom_histogram() specifies type plot want drawn!\ncan also specify aesthetic mapping plot layer (e.g. inside geom_histogram()), override aesthetic mapping inherits base ggplot() object. Otherwise, aesthetic mapping base ggplot() object used. example, make exact plot:\nggplot(penguins, aes(x = flipper_length_mm)) + geom_histogram()\nggplot(penguins) + geom_histogram(aes(x = flipper_length_mm))\n\ncan also specify aesthetic mapping plot layer (e.g. inside geom_histogram()), override aesthetic mapping inherits base ggplot() object. Otherwise, aesthetic mapping base ggplot() object used. example, make exact plot:\nggplot(penguins, aes(x = flipper_length_mm)) + geom_histogram()\nggplot(penguins) + geom_histogram(aes(x = flipper_length_mm))\nggplot(penguins, aes(x = flipper_length_mm)) + geom_histogram()ggplot(penguins) + geom_histogram(aes(x = flipper_length_mm))Due slightly unusual nature syntax, number common failure modes observed. Make sure take note following:Plot layers ALWAYS added + like numbers. just design syntax. Attempting use anything else give errors!Plot layers ALWAYS added + like numbers. just design syntax. Attempting use anything else give errors!also MUST execute layer like function (). try just add + geom_histogram without (), layer generate correctly give errors!also MUST execute layer like function (). try just add + geom_histogram without (), layer generate correctly give errors!many layers, ’s recommended break multiple lines, incomplete line MUST either unclosed parenthetical ( end unfinished addition +, example:\n\n# ok; R sees incomplete lines\n# continues reading next line\nggplot(penguins,                       # unclosed ( parenthetical\n       aes(x = flipper_length_mm)) +   # unfinished + addition\n  geom_histogram()\n\n# error, since first line incomplete!!\nggplot(penguins, aes(x = flipper_length_mm))\n  + geom_histogram()\nError `+.gg`:\n! use `+` single argument.\nℹ accidentally put `+` new line?many layers, ’s recommended break multiple lines, incomplete line MUST either unclosed parenthetical ( end unfinished addition +, example:Don’t forget incomplete ( somewhere, didn’t finish + statement, R patiently wait replacing normal > prompt + prompt. Finish line hit ESC quit.","code":"\n# this is ok; R sees the incomplete lines\n# and continues reading the next line\nggplot(penguins,                       # unclosed ( parenthetical\n       aes(x = flipper_length_mm)) +   # unfinished + addition\n  geom_histogram()\n# but this will error, since the first line is NOT incomplete!!\nggplot(penguins, aes(x = flipper_length_mm))\n  + geom_histogram()Error in `+.gg`:\n! Cannot use `+` with a single argument.\nℹ Did you accidentally put `+` on a new line?"},{"path":"data-visualization.html","id":"adding-aesthetics","chapter":"6 Data Visualization","heading":"6.3.1.3 Adding aesthetics","text":"ggplot2, can easily add additional aesthetics plot, turning one-variable plots two-variable plots, allowing visualize vary together. Remember histogram shows bimodality? turns represent different species penguins. Let’s use fill aesthetic differentiate species.default, ggplot stack bars position along horizontal axis. Let’s unstack setting position = \"identity\" make bars 50% opaque setting alpha = 0.5 can better see group. set plot layer geom_histogram().already starting look pretty good! can now start easily make interesting observations:species penguin different average23 flipper length, Gentoo penguins largest, Adelie penguins smallest, Chinstrap penguins somewhere .seems far Adelie Gentoo penguins dataset Chinstrap penguins. can investigate later.Aesthetics always mapped columns data frame. Also note columns proper names (.e. letters, numbers, periods, underscores spaces symbols) need quotes \" \" used inside ggplot (well Tidyverse functions).plots can flipped (.e. changed horizontal vertical vice versa) swapping x y aesthetics, .e. using y = ... instead x = ... vice versa. cases may preferred (e.g. dataset long labels) generally ’s matter personal preference/style. Feel free experiment !","code":"\nggplot(penguins, aes(x = flipper_length_mm, fill = species)) +\n  geom_histogram()\nggplot(penguins, aes(x = flipper_length_mm, fill = species)) +\n  geom_histogram(position = \"identity\", alpha = 0.5)"},{"path":"data-visualization.html","id":"title-labels","chapter":"6 Data Visualization","heading":"6.3.1.4 Title & labels","text":"one final thing done plot: title label ! something EVERY plot make, just class throughout data science career.plot annotations (e.g. titles, axes/data labels, legends, etc.) meet following criteria:Accuracy: annotations contain accurate information.Precision: strive precise (e.g. instead “average”, specify mean, median, mode, something else).Concision: strive use words necessary convey important information given context.Grammar/spelling: proper grammar spelling used (abbreviations, needed, standard intuitive).Units: unless ’s extremely obvious (unitless), data units also specified!plots submitted class without annotations annotations meeting criteria may penalized!Annotations hard get right sometimes; practice adding every plot, think critically write read peoples’ plots ’ll get good fast.can add titles/labels adding labs() layer. Inside labs(), can simultaneously set /following:x = \"...\" y = \"...\" sets axes labels x ytitle sets plot titlelabels aesthetics can also set using aesthetic name\nexample, used fill, set legend label fill = \"...\"\nexample, used fill, set legend label fill = \"...\"less frequently used labels include subtitle, caption, alt alt-textFor example:plot now ready use!","code":"\nggplot(penguins, aes(x = flipper_length_mm, fill = species)) +\n  geom_histogram(position = \"identity\", alpha = 0.5) +\n  labs(x = \"Flipper length (mm)\", y = \"Count\", fill = \"Species\",\n    title = \"Flipper length histograms of species in Palmer penguins sample\")"},{"path":"data-visualization.html","id":"extra-options","chapter":"6 Data Visualization","heading":"6.3.1.5 Extra options","text":"90% time, steps went need completely prepare plot use. 10% time, may need configure plot . plot layer function additional specific options can set, usual check help page search online !cover every option every plot type, occasionally, may highlight important options experiment explore . geom_histogram(), besides unstacking bars position = \"identity\" argument showed , may also want control bins set. ’s several ways briefly outlined —note can choose ONE method!can set bins argument set many total bins, used evenly divide range data. E.g. default, bins = 30 used draw 30 bins, generally agreed sensible default, even though can create bins strange decimal bounds (like example, bins (170.98,173.02], (173.02,175.05], …, (229.98,232.02]).\ndata integer-valued (like flipper length ), default method can actually cause problems, bins contain whole numbers others, creating strange artifacts data. example, two consecutive bins (1.8,3.2] (3.2,4.6], even though 1.4 units wide, first covers 2 whole numbers (2 3) whereas second covers 1 whole number (just 4) distort histogram shape.\ndata integer-valued (like flipper length ), default method can actually cause problems, bins contain whole numbers others, creating strange artifacts data. example, two consecutive bins (1.8,3.2] (3.2,4.6], even though 1.4 units wide, first covers 2 whole numbers (2 3) whereas second covers 1 whole number (just 4) distort histogram shape.Alternatively, can also set binwidth boundary arguments start given boundary count given binwidth create bins.\nexample, want make bins (170,175], (175,180], …, (230,235], can set binwidth = 5 boundary = 170 (whole number divisible 5).\nexample, want make bins (170,175], (175,180], …, (230,235], can set binwidth = 5 boundary = 170 (whole number divisible 5).maximum control, can also set breaks equal numeric vector use bin boundaries.\nexample, breaks (170,175], (175,180], …, (230,235] can chosen setting breaks = seq(170, 235, = 5).\nexample, breaks (170,175], (175,180], …, (230,235] can chosen setting breaks = seq(170, 235, = 5).Generally, want choose bins easy visually interpret, try using whole numbers work well base-10 decimal system. also want avoid using many bins can cause problems.Let’s improve plot one final time setting sensible bin widths:now even easier interpret, artifacts previous plots gone. can easily identify average24 group, even identify specific counts specific bins (e.g. can tell example 39 penguins Adelie penguins observed (190,195] bin).","code":"\nggplot(penguins, aes(x = flipper_length_mm, fill = species)) +\n  geom_histogram(position = \"identity\", alpha = 0.5,\n                 binwidth = 5, boundary = 170) +\n  labs(x = \"Flipper length (mm)\", y = \"Count\", fill = \"Species\",\n    title = \"Flipper length histograms of species in Palmer penguins sample\")"},{"path":"data-visualization.html","id":"density-plots","chapter":"6 Data Visualization","heading":"6.3.2 Density plots","text":"common variation histogram density plot, can thought like smoothed-curve version histogram, area curve normalized 1. represents guess entire population distribution looks like based sample drawn. can created adding geom_density() plot layer.Similar histogram, can also add additional aesthetics differentiate species:Note looks similar previously made histogram, Chinstrap distribution longer overshadowed species, since area normalization process effectively removes effect sample size height distribution species.learn lot density plots later inference portion course, now move .can make plot even accessible readers certain vision impairments adding certain additional aesthetics. example, try adding linetype = species aesthetic mapping (don’t forget add label inside labs()), well increasing border thickness adding linewidth = 1 inside geom_density() observe output!","code":"\nggplot(penguins, aes(x = flipper_length_mm)) + geom_density()\nggplot(penguins, aes(x = flipper_length_mm, fill = species)) +\n  geom_density(alpha = 0.5) +\n  labs(x = \"Flipper length (mm)\", y = \"Density\", fill = \"Species\",\n    title = \"Flipper length densities of species in Palmer penguins sample\")"},{"path":"data-visualization.html","id":"box-plots","chapter":"6 Data Visualization","heading":"6.3.3 Box plots","text":"Another common plot numeric values box plot. Box plots simply way showing following 5 summary statistics number line:minimum sample,first quartile \\(Q_1\\), .e. 25th percentile,median,third quartile \\(Q_3\\), .e. 75th percentile, andThe maximum sample.\\(Q_1\\) \\(Q_3\\) form ends “box”, median shown line , min max form “whiskers” stretch either end. Note width box (.e. \\(Q_3-Q_1\\)) IQR.Compared histogram density plot, key advantages drawbacks:’s easier compare specific summary statistics like median quartiles using box plots,However ’s often less effective communicating complex features like modality skew.simplicity sometimes works better comparing many groups without appearing overly complex.Let’s show five number summary fivenum() well corresponding box plot flipper length variable:Note even though can easily identify median, quartiles, min/max, can longer observe bimodality like previously histogram density plot. tradeoff sometimes worth making sometimes .boxplot can also easily adapted highlight difference species, time adding y aesthetic:plot sacrifices distributional complexity density plot, return can easily compare summary statistics group, overall arguably just “looks nicer” opinion.Note 2 Adelie penguins plotted points instead, due common “rule thumb” box plots label points 1.5×IQR away quartile “outliers”. , simply default convention. See help page details, well disable .","code":"\n# get the min, q1, median, q3, and max:\nfivenum(penguins$flipper_length_mm)[1] 172 190 197 213 231\n# turn these into a box plot\nggplot(penguins, aes(x = flipper_length_mm)) + geom_boxplot()\nggplot(penguins, aes(x = flipper_length_mm, y = species)) +\n  geom_boxplot() +\n  labs(x = \"Flipper length (mm)\", y = \"Species\",\n       title = \"Flipper length box plots of species in Palmer penguins sample\")"},{"path":"data-visualization.html","id":"bar-plots","chapter":"6 Data Visualization","heading":"6.3.4 Bar plots","text":"far, discussed visualizing 1 numeric variable, several interesting options shown sections , pros/cons.1 categorical variable, bar plots bars varying heights plotted category real option. Generally, common thing plot (without involving columns) either count proportion category observed sample. Note proportion, pie charts also often used, fallen fashion longer recommended.Confusingly, ggplot2 offers 2 different functions making bar plots geom_bar() geom_col() appear similar !geom_bar() default accepts one aesthetic (either x y ) tally given column, counting number rows category plotting total counts. generally used full original dataset.geom_col() default demands two aesthetics (x y) performs computation simply plots one . generally used summaries dataset, full original dataset.example, can use geom_bar() compute plot total count species sample:wanted make plot using geom_col(), must FIRST summarize dataset computing counts manually, passing species computed values x y aesthetics, like :next chapter, learn efficiently summarize datasets similar “summary” data frames like , allow us fully appreciate versatility geom_col().","code":"\nggplot(penguins, aes(x = species)) + geom_bar() +\n  labs(x = \"Species\", y = \"Count\",\n       title = \"Count of each species in Palmer penguins sample\")\n# we will learn later how to do this more efficiently with tidyverse,\n# but for now we can summarize the counts using base R syntax\npenguins_species_counts <- tibble(\n  species = c(\"Adelie\", \"Chinstrap\", \"Gentoo\"), \n  count = c(\n    sum(penguins$species == \"Adelie\"),\n    sum(penguins$species == \"Chinstrap\"),\n    sum(penguins$species == \"Gentoo\")\n  )\n)\n# check the result\npenguins_species_counts# A tibble: 3 × 2\n  species   count\n  <chr>     <int>\n1 Adelie      146\n2 Chinstrap    68\n3 Gentoo      119\n# now, make the same plot using the summary data frame and geom_col()\nggplot(penguins_species_counts, aes(x = species, y = count)) + geom_col() +\n  labs(x = \"Species\", y = \"Count\",\n       title = \"Count of each species in Palmer penguins sample\")"},{"path":"data-visualization.html","id":"extra-options-1","chapter":"6 Data Visualization","heading":"6.3.4.1 Extra options","text":"geom_bar() two important arguments significantly improve utility. can set stat = \"summary\" fun = \"...\" ... name summary function (e.g. mean, median, sd, var, IQR, range, “summary” function, .e. something ingests vector outputs single value). allow set axis aesthetic well (e.g. y = ...) another column summarized using given function.example, supposed want use bar plot compare median flipper length species. can set stat = \"summary\", fun = \"median\", map y = flipper_length_mm make following plot:","code":"\nggplot(penguins, aes(x = species, y = flipper_length_mm)) +\n  geom_bar(stat = \"summary\", fun = \"median\") +\n  labs(x = \"Species\", y = \"Median flipper length (mm)\",\n       title = \"Median flipper length by species in Palmer penguins sample\")"},{"path":"data-visualization.html","id":"adding-aesthetics-1","chapter":"6 Data Visualization","heading":"6.3.4.2 Adding aesthetics","text":"Bar plots also often good candidates adding additional aesthetics like fill. ’ll show 2 examples : stacked unstacked bar version.default, adding fill creates stacked bar plot, good showing proportions bar respect second categorical variable. example, suppose want show island species came . can adding fill = island aesthetic mapping:can also unstack bars setting position = \"dodge\" inside geom_bar(), making bars appear side side. example, suppose want compare median flipper length species also sex. can easily adding fill = sex aesthetic mapping, well setting position mentioned :","code":"\nggplot(penguins, aes(x = species, fill = island)) + geom_bar() +\n  labs(x = \"Species\", y = \"Count\", fill = \"Island\",\n       title = \"Count of each species (by island) in Palmer penguins sample\")\nggplot(penguins, aes(x = species, y = flipper_length_mm, fill = sex)) +\n  geom_bar(stat = \"summary\", fun = \"median\", position = \"dodge\") +\n  labs(x = \"Species\", y = \"Median flipper length (mm)\", fill = \"Sex\",\n       title = \"Median flipper length by species & sex in Palmer penguins sample\")"},{"path":"data-visualization.html","id":"two-variable-plots","chapter":"6 Data Visualization","heading":"6.4 Two-variable plots","text":"contrast previous types, plot types MUST made least two variables; possible single column.","code":""},{"path":"data-visualization.html","id":"scatter-plot","chapter":"6 Data Visualization","heading":"6.4.1 Scatter plot","text":"Scatter plots perhaps famous plot type, one people well familiar . Scatter plots classic y vs x plot two numeric variables Cartesian coordinates 2-dimensional grid. make scatter plot ggplot2, just need map x y aesthetics two columns, add geom_point() layer.example, suppose want make scatter plot flipper length vs bill depth see correlation two. Note always say y vs x, never x vs y. code:","code":"\nggplot(penguins, aes(y = flipper_length_mm, x = bill_depth_mm)) + geom_point() +\n  labs(x = \"Bill depth (mm)\", y = \"Flipper length (mm)\",\n       title = \"Flipper length vs bill depth for Palmer penguins sample\")"},{"path":"data-visualization.html","id":"adding-aesthetics-2","chapter":"6 Data Visualization","heading":"6.4.1.1 Adding aesthetics","text":"may surprise see two negatively correlated, .e. increase bill depth seems correlate decrease flipper length, vice versa. However, add species plot, see interesting pattern emerge.improve readability, can set color shape aesthetics controlled species column, well slightly increase size points setting size = 2 inside geom_point(), like :Now can clearly see bill depth flipper length fact positively correlated within species, might expect. effect called Simpson’s paradox arises surprisingly often datasets, notably 1973 UC Berkeley accused gender discrimination almost sued.25When making scatter plots, ’s important remember correlation necessarily imply causation. flippers grow longer long bills vice versa? Obviously probably , penguins just grow bigger others species winning genetic lottery bigger features overall.26","code":"\nggplot(penguins, aes(y = flipper_length_mm, x = bill_depth_mm,\n                     color = species, shape = species)) +\n  geom_point(size = 2) +\n  labs(x = \"Bill depth (mm)\", y = \"Flipper length (mm)\",\n       color = \"Species\", shape = \"Species\",\n       title = \"Flipper length vs bill depth by species for Palmer penguins sample\")"},{"path":"data-visualization.html","id":"smoothed-trend-curveslines","chapter":"6 Data Visualization","heading":"6.4.2 Smoothed trend curves/lines","text":"can also add smoothed trend curve/line (depending context) plot highlight direction correlation within species group. adding additional geom_smooth() layer plot. default, plot smoothed trend curve (using LOESS smoothing) group:clearly right ; data shows strong signs linearity. can force geom_smooth() fit plot linear regression models species setting method = lm. can also turn unnecessarily cluttering gray error margins se = FALSE get much improved plot:Note since x, y, color set base ggplot() object, subsequent layers automagically inherit aesthetics; geom_point() geom_smooth() know use aesthetic mappings construct plot layers. Trend curves don’t make use shape aesthetic applies scatter plot layer, ’s simply ignored geom_smooth().good time point difference mapping aesthetic base object vs mapping plot layer.last example code chunk , note set x, y, color, shape aesthetics base ggplot() object, inherited geom_point() geom_smooth() layers. instead don’t want one inherit certain aesthetics, can set directly layer instead.example, suppose species points differing color shape, don’t want different trend line species, rather 1 single trend line across points. can easily achieve setting color shape inside geom_point() instead ggplot(). cause geom_smooth() inherit x y ggplot() single trend line made species:","code":"\nggplot(penguins, aes(y = flipper_length_mm, x = bill_depth_mm,\n                     color = species, shape = species)) +\n  geom_point(size = 2) + geom_smooth() +\n  labs(x = \"Bill depth (mm)\", y = \"Flipper length (mm)\",\n       color = \"Species\", shape = \"Species\",\n       title = \"Flipper length vs bill depth by species for Palmer penguins sample\")\nggplot(penguins, aes(y = flipper_length_mm, x = bill_depth_mm,\n                     color = species, shape = species)) +\n  geom_point(size = 2) + geom_smooth(method = lm, se = FALSE) +\n  labs(x = \"Bill depth (mm)\", y = \"Flipper length (mm)\",\n       color = \"Species\", shape = \"Species\",\n       title = \"Flipper length vs bill depth by species for Palmer penguins sample\")\nggplot(penguins, aes(y = flipper_length_mm, x = bill_depth_mm)) +\n  geom_point(aes(color = species, shape = species), size = 2) +\n  geom_smooth(method = lm, se = FALSE) +\n  labs(x = \"Bill depth (mm)\", y = \"Flipper length (mm)\",\n       color = \"Species\", shape = \"Species\",\n       title = \"Flipper length vs bill depth by species for Palmer penguins\")"},{"path":"data-visualization.html","id":"line-plots","chapter":"6 Data Visualization","heading":"6.4.3 Line plots","text":"specific datasets, especially chronological datasets variable plotted time, may make sense directly connect individual data point points form line (trace) plot. Note smoothed trend plot shown since line plot smoothing performed!Unfortunately, Palmer penguins dataset isn’t best example last plot type, ’m temporarily borrowing another dataset example. chunk imports enrollment.csv contains historic U.S. college enrollment data sex., row pair male female college enrollment counts (millions) specific year. can obviously just make scatter plot enrollment vs year, additional aesthetics differentiating male female data points, like :However, chronological nature data means data point specific predecessor successor, .e. point (except end points) specific points comes chronological order. Thus, makes sense connect points single continuous line sex. can done using geom_line() instead. can also drop shape aesthetic since doesn’t apply lines:default line appears thin may hard read people. can add linewidth = 1.2 geom_line() increase thickness. can also add another aesthetic linetype = sex additional disambiguation ’s extremely clear line corresponds sex:can see since late 70’s, college enrollment female students consistently outpaced male students.purposes course, “line plot” “trace plot” refer geom_line(), “smoothed line” “trend line” “straight line” refer geom_smooth(method = lm). Take care mix !","code":"\nenrollment <- read_csv(\"https://bwu62.github.io/stat240-revamp/data/enrollment.csv\")\n# show a glimpse of the dataset\nglimpse(enrollment)Rows: 148\nColumns: 3\n$ year              <dbl> 1947, 1947, 1948, 1948, 1949, 1949, 1950, 1950, 1951, 195…\n$ sex               <chr> \"male\", \"female\", \"male\", \"female\", \"male\", \"female\", \"ma…\n$ enrolled_millions <dbl> 1.659249, 0.678977, 1.709367, 0.694029, 1.721572, 0.72332…\nggplot(enrollment, aes(x = year, y = enrolled_millions,\n                       color = sex, shape = sex)) +\n  geom_point(size = 2) +\n  labs(x = \"Time\", y = \"Enrolled (millions)\",\n       color = \"Sex\", shape = \"Sex\",\n       title = \"U.S. College enrollment by sex\")\nggplot(enrollment, aes(x = year, y = enrolled_millions, color = sex)) +\n  geom_line() +\n  labs(x = \"Time\", y = \"Enrolled (millions)\", color = \"Sex\",\n       title = \"U.S. College enrollment by sex\")\nggplot(enrollment, aes(x = year, y = enrolled_millions,\n                       color = sex, linetype = sex)) +\n  geom_line(linewidth = 1.2) +\n  labs(x = \"Time\", y = \"Enrolled (millions)\",\n       color = \"Sex\", linetype = \"Sex\",\n       title = \"U.S. College enrollment by sex\")"},{"path":"data-visualization.html","id":"bonus-area-plots","chapter":"6 Data Visualization","heading":"6.4.4 Bonus: area plots","text":"wanted throw bonus plot type . kind composition--time data also commonly shown stacked area plot can made geom_area(). ’ve also added scale_fill_manual() configuration layer manually specify color palette use intuitive colors category.Compared line plots one line per category, stacked area plot advantage easily showing relative proportions categories well total sum categories, comes cost able easily compare individual categories . tradeoff, usual.","code":"\nggplot(enrollment, aes(x = year, y = enrolled_millions, fill = sex)) +\n  geom_area() + scale_fill_manual(values = c(\"#fb9a99\", \"#a6cee3\")) +\n  labs(x = \"Time\", y = \"Enrolled (millions)\", fill = \"Sex\",\n       title = \"U.S. College enrollment by sex\")"},{"path":"data-visualization.html","id":"aside-time-axis","chapter":"6 Data Visualization","heading":"6.4.5 Aside: time axis","text":"Note previous example used just year number horizontal axis (since data already summarized annual totals) can course also use dates axis. quick example, can load FRED U.S. unemployment rate dataset plot .superficially look previous plot, however zoom plot just last year data, can see horizontal axis fact special date type axis:choppiness data summarized monthly.","code":"\nunemployment <- read_csv(\"https://fred.stlouisfed.org/graph/fredgraph.csv?id=UNRATE\")\nglimpse(unemployment)Rows: 933\nColumns: 2\n$ observation_date <date> 1948-01-01, 1948-02-01, 1948-03-01, 1948-04-01, 1948-05-0…\n$ UNRATE           <dbl> 3.4, 3.8, 4.0, 3.9, 3.5, 3.6, 3.6, 3.9, 3.8, 3.7, 3.8, 4.0…\nggplot(unemployment, aes(x = observation_date, y = UNRATE)) + geom_line() +\n  labs(x = \"Time\", y = \"Unemployment rate (%)\",\n       title = \"U.S. Unemployment rate\")\n# plot just the last 12 months of data\nn = nrow(unemployment)\nggplot(unemployment[(n-11):n,], aes(x = observation_date, y = UNRATE)) + geom_line() +\n  labs(x = \"Time\", y = \"Unemployment rate (%)\",\n       title = \"U.S. Unemployment rate\")"},{"path":"data-visualization.html","id":"facet-subplots","chapter":"6 Data Visualization","heading":"6.5 Facet subplots","text":"Another way incorporate additional aesthetics use facets, .e. instead everything one plot, breaking multiple subplots visualize extra dimensions dataset.two primary functions :facet_wrap() creates series subplots along ONE additional categorical column one panel category, allows plots wrap onto multiple rows, way sentence can wrap onto multiple lines. best want incorporate single additional variable many levels wouldn’t fit onto one row.facet_grid() creates grid subplots along one two additional categorical column(s). can make either row subplots, column subplots, even matrix , incorporating two additional columns visualizations.Let’s demonstrate briefly.","code":""},{"path":"data-visualization.html","id":"facet_wrap","chapter":"6 Data Visualization","heading":"6.5.1 facet_wrap()","text":"Going back penguins dataset, suppose wanted look relationship body mass flipper length. start making plot:Maybe decide also incorporate species variable, can see differences different species:interesting patterns start emerge. Suppose want take step also incorporate sex, see adds anything interesting picture. change example mapping set shape = sex, think results plot ’s little complicated hard read:better way may facet_wrap() species variable, switch using color shape differentiate sex. syntax add faceting layer facet_wrap() argument ~species automagically split species facet subplot. can also set ncol = 2 control /wrap plots:","code":"\nggplot(penguins, aes(x = body_mass_g, y = flipper_length_mm)) + geom_point()\nggplot(penguins, aes(x = body_mass_g, y = flipper_length_mm,\n                     color = species, shape = species)) +\n  geom_point(size = 2)\nggplot(penguins, aes(x = body_mass_g, y = flipper_length_mm,\n                     color = species, shape = sex)) +\n  geom_point(size = 2) +\n  labs(x = \"Body mass (g)\", y = \"Flipper length (mm)\",\n       title = \"Flipper length vs body mass (by species & sex) for Palmer penguins\")\nggplot(penguins, aes(x = body_mass_g, y = flipper_length_mm,\n                     color = sex, shape = sex)) +\n  geom_point(size = 2) + facet_wrap(~species, ncol = 2) +\n  labs(x = \"Body mass (g)\", y = \"Flipper length (mm)\", color = \"Sex\", shape = \"Sex\",\n       title = \"Flipper length vs body mass (by species & sex) for Palmer penguins\")"},{"path":"data-visualization.html","id":"facet_grid","chapter":"6 Data Visualization","heading":"6.5.2 facet_grid()","text":"Suppose wanted look closely distribution flipper lengths species sex. can start plot made previously section 6.3.2:Suppose want also add sex variable interest, also want closely scrutinize distributions. One way can done add faceting layer facet_grid() argument sex ~ species construct matrix plots one row sex one column species:can also replace one side ~ b syntax period . facet direction. example, sex ~ . make matrix plots one row sex, single column together, whereas . ~ species make matrix plots one column species, one row together.can demonstrate . time can reassign variable faceted (.e. one replaced .) back fill aesthetic. second plot, also flipped x y make better use tall-orientation available plot space.default, facet_wrap() facet_grid() match x y axes across subplots. can turn either setting scales argument faceting function either free_x free_y free one axis, free free axes.","code":"\nggplot(penguins, aes(x = flipper_length_mm, fill = species)) +\n  geom_density(alpha = 0.5) +\n  labs(x = \"Flipper length (mm)\", y = \"Density\", fill = \"Species\",\n       title = \"Flipper length densities of species in Palmer penguins\")\nggplot(penguins, aes(x = flipper_length_mm)) + geom_density() +\n  facet_grid(sex ~ species) +\n  labs(x = \"Flipper length (mm)\", y = \"Density\",\n       title = \"Flipper length densities (by species & sex) in Palmer penguins\")\nggplot(penguins, aes(x = flipper_length_mm, fill = species)) +\n  geom_density(alpha = 0.5) + facet_grid(sex ~ .) +\n  labs(x = \"Flipper length (mm)\", y = \"Density\", fill = \"Species\",\n       title = \"Flipper length densities (by species & sex) in Palmer penguins\")\nggplot(penguins, aes(y = flipper_length_mm, fill = sex)) +\n  geom_density(alpha = 0.5) + facet_grid(. ~ species) +\n  labs(x = \"Flipper length (mm)\", y = \"Density\", fill = \"Sex\",\n       title = \"Flipper length densities (by species & sex) in Palmer penguins\")"},{"path":"data-visualization.html","id":"scales","chapter":"6 Data Visualization","heading":"6.6 Scales","text":"Generally, default axes fine, need can modify scale layers. Scales control every aesthetic displayed plot, scales every aesthetic. Aesthetics added plot automatically come scale layer default settings, can replace custom-tuned scale settings adding another scale .Every scale layer following name pattern: scale_aes_type aes type name aesthetic type scale used. example, ’re making density plot set x = flipper_length_mm controlled scale_x_continuous() flipper_length_mm column continuous, numeric type variable. However making bar plot set x = species controlled scale_x_discrete() since species discrete, categorical type variable.also apply aesthetics. set fill = species color = sex default controlled scale_fill_discrete() scale_color_discrete() since discrete. automatic color choosing functions like scale_fill_brewer() scale_color_brewer() use excellent Brewer color palettes, can also set scale_fill_manual() scale_color_manual().tidyverse loaded, try typing scale_ console use autocomplete popup (doesn’t appear automatically, use TAB trigger ) explore different scale layers available. Hover scale function read short summary well scan available arguments. need details, check help page!don’t time go detail every possible scale function; see page scales read help pages particular function info! ’s example scale layers used modify last plot made just faceted species:","code":"\nggplot(penguins, aes(y = flipper_length_mm, fill = sex)) +\n  geom_density(alpha = 0.5) + facet_grid(. ~ species) +\n  ggtitle(\"Flipper length densities (by species & sex) in Palmer penguins sample\") +\n  xlab(\"Density\") + ylab(\"Flipper length (mm)\") +\n  scale_x_continuous(\n    limits = c(0, 0.1),              # set limits\n    breaks = seq(0, 0.08, 0.02),     # set breaks\n    expand = 0                       # remove padding (extra spacing at either end)\n    # if you need to transform x, you can also set transform = something (see help!)\n  ) +\n  scale_y_continuous(\n    minor_breaks = seq(170, 230, 2), # set minor breaks\n    expand = 0                       # remove padding here as well\n  ) +\n  scale_fill_manual(\n    values = c(\"red\", \"blue\")        # set custom colors, full list of possible names:\n  )                                  # www.stat.columbia.edu/~tzheng/files/Rcolor.pdf"},{"path":"data-visualization.html","id":"other-geoms","chapter":"6 Data Visualization","heading":"6.7 Other geoms","text":"Many geoms exist (see ggplot2 cheat sheet full list). just extremely useful ones know .","code":""},{"path":"data-visualization.html","id":"straight-lines","chapter":"6 Data Visualization","heading":"6.7.1 Straight lines","text":"Sometimes may want draw specific lines annotate plot. can use geom_hline(yintercept = ...), geom_vline(xintercept = ...), geom_abline(slope = ..., intercept = ...) manually draw horizontal, vertical, arbitrary lines top another plot. can also directly set things like color, alpha, linetype, linewidth inside function control style. need multiple lines, can use vector inputs, add multiple layers. example:","code":"\nggplot(penguins, aes(x = body_mass_g, y = flipper_length_mm,\n                     color = species, shape = species)) +\n  geom_point(size = 2) +\n  geom_hline(yintercept = c(190, 210), color = \"navyblue\", linetype = \"dashed\") +\n  geom_vline(xintercept = 4500, linewidth = 2, alpha = 0.5) +\n  geom_abline(slope = 0.015, intercept = 140, color = \"magenta\", size = 1)"},{"path":"data-visualization.html","id":"functions","chapter":"6 Data Visualization","heading":"6.7.2 Functions","text":"Functions can easily plotted geom_function(fun = ...) ... target function. can added either top existing plot another layer, plotted new plot, case base object ggplot() requires additional arguments.function isn’t predefined, can easily define \\(x) ... example:function exists, need modify arguments, can use args = list(...) arguments specified inside directly passed chose function, example:","code":"\nggplot() + geom_function(\n  fun = \\(x) x^2 + 1,      # define x²+1\n  xlim = c(-2, 2),         # set limits\n  n = 1e3                  # increase number of points used in drawing\n)                          # (improves smoothness of resulting curve)\n# plot the normal distribution with mean 10 sd 2\nggplot() + geom_function(\n  fun = dnorm,                     # dnorm() is the normal distribution function\n  args = list(mean = 10, sd = 2),  # set mean and sd arguments inside dnorm()\n  xlim = c(4, 16),                 # set limits\n  n = 1e3                          # increase number of points\n)"},{"path":"data-visualization.html","id":"bonus-pairs-plot","chapter":"6 Data Visualization","heading":"6.8 Bonus: pairs plot","text":"didn’t know put ’m inserting end chapter . ’s bonus plot type called pairs plot, every variable data frame plotted every variable. primary purpose plot rapidly orient new dataset ’re just starting explore. ’s usually recommended limit 5-6 variables max pairs plot, lest become chaotic.many different implementations , one best GGally::ggpairs(). plot type automatically determined based variable types can density, histogram, point, bar, box, , additionally summary statistics like correlations given appropriate.plot recommended initial exploratory plot; used purposeful setting like report since lacks direction/focus.","code":"\n# to run this chunk, you need to have the package GGally installed\n# first, we subset out just a few columns (otherwise it gets too crazy)\n# aes(color = species) is optional but improves the plot greatly\npenguins_subset <- penguins[\n  c(\"species\", \"body_mass_g\", \"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\")\n]\nGGally::ggpairs(penguins_subset, aes(color = species))"},{"path":"data-visualization.html","id":"further-readings","chapter":"6 Data Visualization","heading":"6.9 Further readings","text":"beyond scope course, ’s links want learn :haven’t already, make sure check ggplot2 cheat sheet.need help picking plot, Data Viz nice flow chart links example R code.also recommend scanning Data Viz page caveats, .e. common pitfalls data science.can add additional text annotations plots necessary.can also modify coordinate systems, example make polar plots especially effective cyclical directional data.also plot themes can try .’s gallery R plots want learn advanced types plots see examples make .","code":""},{"path":"data-transformation.html","id":"data-transformation","chapter":"Data Transformation","heading":"Data Transformation","text":"next section, learn perform basic data transformation procedures commonly used clean “tidy” data convenient useful format whatever purpose needs serve. loosely broken several sections.First, learn basics dplyr, another core Tidyverse packages, including things like renaming variables, subsetting rows columns, adding editing columns, making summaries., proceed advanced topics like grouping operations, combining different data frames along rows columns, pivoting using tidyr, yet another core Tidyverse package.","code":""},{"path":"intro-to-dplyr.html","id":"intro-to-dplyr","chapter":"7 Intro to dplyr","heading":"7 Intro to dplyr","text":"dplyr core Tidyverse package transforming raw datasets clean usable format (link cheat sheet). functions versatile, performant, consistent user friendly syntax. traits make highly suitable data science levels.","code":"\n# import all core tidyverse packages\nlibrary(tidyverse)\n# optional: change default print to 5 rows to save vertical space, and\n#           disable showing col_types by default in readr import functions\noptions(pillar.print_min = 5, readr.show_col_types = FALSE)\n# optional: change default ggplot theme options (personal preference)\nsource(\"https://bwu62.github.io/stat240-revamp/ggplot_theme_options.R\")"},{"path":"intro-to-dplyr.html","id":"syntax-design","chapter":"7 Intro to dplyr","heading":"7.1 Syntax design","text":"First, think ’s important briefly comment syntax design dplyr avoid confusion later . functions covered chapter satisfy following design principles:Functions dplyr designed work data frames, objects (e.g. vectors).\nrun vector, must first wrapped inside data frame (see section 4.1).\nrun vector, must first wrapped inside data frame (see section 4.1).Functions dplyr able run pipes like %>% |> (soon).Functions dplyr, modify input, must always manually save output.","code":""},{"path":"intro-to-dplyr.html","id":"pipes","chapter":"7 Intro to dplyr","heading":"7.2 Pipes","text":"dplyr functions setup first argument input data frame. words, ’re always run like f(df, ...) f dplyr function, df data frame wish operate , ... can arguments. also applies Tidyverse functions, e.g. recall first argument ggplot() must also data frame.design makes easy chain many functions together pipes %>% |>, basically used pass left-side expression first argument function. class, stick %>% consistency, can use |> prefer. example,x %>% f equivalent f(x)x %>% f(y) equivalent f(x, y) 27Why useful? Suppose start data frame df want run functions f, g, h order. can course following:quickly becomes awkward, due many nested parentheticals. much cleaner syntax use %>% pipe df one function next, resulting much cleaner equivalent syntax:much neater also significantly easier modify debug. can also easily specify additional arguments. example, suppose f, g, h need additional arguments = 1, b = 2, c = 3 order. Compare two equivalent syntax options:long function chain accepts data frame first argument outputs data frame result, can string together many functions need perform several data operations single step.magrittr help page additional usage tips %>%, pass left side different argument position, using %>% create simple functions, advanced pipes. outside scope course, read discretion!","code":"h(g(f(df)))df %>% f %>% g %>% hh(g(f(df, a = 1), b = 2), c = 3)df %>% f(a = 1) %>% g(b = 2) %>% h(c = 3)"},{"path":"intro-to-dplyr.html","id":"data-example","chapter":"7 Intro to dplyr","heading":"7.3 Data example","text":"now, ’re going continue using Palmer penguins dataset penguins.csv example data frame. Let’s load data :","code":"\n# load in the familiar penguins dataset\npenguins <- read_csv(\"https://bwu62.github.io/stat240-revamp/data/penguins.csv\")\n# print the first few rows of the data frame to check\n# again this data frame is too wide, and some columns are cut off\nprint(penguins, n = 5)# A tibble: 333 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n  <chr>   <chr>              <dbl>         <dbl>             <dbl>       <dbl> <chr> \n1 Adelie  Torgersen           39.1          18.7               181        3750 male  \n2 Adelie  Torgersen           39.5          17.4               186        3800 female\n3 Adelie  Torgersen           40.3          18                 195        3250 female\n4 Adelie  Torgersen           36.7          19.3               193        3450 female\n5 Adelie  Torgersen           39.3          20.6               190        3650 male  \n# ℹ 328 more rows\n# ℹ 1 more variable: year <dbl>"},{"path":"intro-to-dplyr.html","id":"column-wise-functions","chapter":"7 Intro to dplyr","heading":"7.4 Column-wise functions","text":"begin column-wise dplyr functions, .e. functions primarily focus manipulating columns certain ways. many , 4 important following:select() selecting subset columns work ,rename() renaming columns,mutate() editing adding columns, without reducing number rows,summarize() computing data summaries, often using statistical functions like mean, median, sd, etc. results reducing number rows.","code":""},{"path":"intro-to-dplyr.html","id":"select","chapter":"7 Intro to dplyr","heading":"7.4.1 select()","text":"select() used subset columns data frame often one first operations used loading dataset (remove columns unnecessary analysis).flexible syntax: can use subset either numeric position, name, ranges, exclusion, even using special selector functions. can also used reorder columns. examples:’s worth reminding df %>% select(...) modify original input df, instead effectively makes copy df runs instead. Example:want save result df %>% select(...) must manually save output <-. general, always recommend saving NEW object instead overwriting original object non-destructive (.e. lose data) less likely create code errors later . ’s also easier debug.note applies functions page, .e. modify input, desired changes must always manually saved!","code":"\n# select just species, sex, flipper length, and body mass\npenguins %>%\n  select(species, sex, flipper_length_mm, body_mass_g)# A tibble: 333 × 4\n  species sex    flipper_length_mm body_mass_g\n  <chr>   <chr>              <dbl>       <dbl>\n1 Adelie  male                 181        3750\n2 Adelie  female               186        3800\n3 Adelie  female               195        3250\n4 Adelie  female               193        3450\n5 Adelie  male                 190        3650\n# ℹ 328 more rows\n# note this is syntactically equivalent to the following:\nselect(penguins, species, sex, flipper_length_mm, body_mass_g)# A tibble: 333 × 4\n  species sex    flipper_length_mm body_mass_g\n  <chr>   <chr>              <dbl>       <dbl>\n1 Adelie  male                 181        3750\n2 Adelie  female               186        3800\n3 Adelie  female               195        3250\n4 Adelie  female               193        3450\n5 Adelie  male                 190        3650\n# ℹ 328 more rows\n# you can also select by position, or with a range, or both\n# e.g. we can select the 1st, 3rd to 5th, and year columns:\npenguins %>%\n  select(1, 3:5, year)# A tibble: 333 × 5\n  species bill_length_mm bill_depth_mm flipper_length_mm  year\n  <chr>            <dbl>         <dbl>             <dbl> <dbl>\n1 Adelie            39.1          18.7               181  2007\n2 Adelie            39.5          17.4               186  2007\n3 Adelie            40.3          18                 195  2007\n4 Adelie            36.7          19.3               193  2007\n5 Adelie            39.3          20.6               190  2007\n# ℹ 328 more rows\n# you can also use ranges with names\n# e.g. select from 1st to island, then body mass to last col\npenguins %>%\n  select(1:island, body_mass_g:last_col())# A tibble: 333 × 5\n  species island    body_mass_g sex     year\n  <chr>   <chr>           <dbl> <chr>  <dbl>\n1 Adelie  Torgersen        3750 male    2007\n2 Adelie  Torgersen        3800 female  2007\n3 Adelie  Torgersen        3250 female  2007\n4 Adelie  Torgersen        3450 female  2007\n5 Adelie  Torgersen        3650 male    2007\n# ℹ 328 more rows\n# you can also select by excluding specific columns with -\n# e.g. select everything except island and everything after body mass\npenguins %>%\n  select(-island, -(body_mass_g:last_col()), body_mass_g)# A tibble: 333 × 5\n  species bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  <chr>            <dbl>         <dbl>             <dbl>       <dbl>\n1 Adelie            39.1          18.7               181        3750\n2 Adelie            39.5          17.4               186        3800\n3 Adelie            40.3          18                 195        3250\n4 Adelie            36.7          19.3               193        3450\n5 Adelie            39.3          20.6               190        3650\n# ℹ 328 more rows\n# select with selector functions, see ?starts_with help page for details,\n# you can also use & as AND, | as OR, ! as NOT during selection\n# e.g. get cols that start with \"s\", or end with \"mm\" or \"g\",\n#      but do NOT contain \"length\" anywhere in the name\npenguins %>%\n  select(\n    (starts_with(\"s\") | ends_with(c(\"mm\", \"g\"))) & !contains(\"length\")\n  )# A tibble: 333 × 4\n  species sex    bill_depth_mm body_mass_g\n  <chr>   <chr>          <dbl>       <dbl>\n1 Adelie  male            18.7        3750\n2 Adelie  female          17.4        3800\n3 Adelie  female          18          3250\n4 Adelie  female          19.3        3450\n5 Adelie  male            20.6        3650\n# ℹ 328 more rows\n# notice in all examples above, columns are always returned in order\n# select() can therefore also be used to reorder columns\n# e.g. move year, island, species, sex cols in front of everything else\n#      (here, everything() selects all the other cols in original order)\npenguins %>%\n  select(year, island, species, sex, everything())# A tibble: 333 × 8\n   year island    species sex    bill_length_mm bill_depth_mm flipper_length_mm\n  <dbl> <chr>     <chr>   <chr>           <dbl>         <dbl>             <dbl>\n1  2007 Torgersen Adelie  male             39.1          18.7               181\n2  2007 Torgersen Adelie  female           39.5          17.4               186\n3  2007 Torgersen Adelie  female           40.3          18                 195\n4  2007 Torgersen Adelie  female           36.7          19.3               193\n5  2007 Torgersen Adelie  male             39.3          20.6               190\n# ℹ 328 more rows\n# ℹ 1 more variable: body_mass_g <dbl>\n# show starting data frame\nprint(penguins)# A tibble: 333 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n  <chr>   <chr>              <dbl>         <dbl>             <dbl>       <dbl> <chr> \n1 Adelie  Torgersen           39.1          18.7               181        3750 male  \n2 Adelie  Torgersen           39.5          17.4               186        3800 female\n3 Adelie  Torgersen           40.3          18                 195        3250 female\n4 Adelie  Torgersen           36.7          19.3               193        3450 female\n5 Adelie  Torgersen           39.3          20.6               190        3650 male  \n# ℹ 328 more rows\n# ℹ 1 more variable: year <dbl>\n# select a subset of columns\npenguins %>% select(species, flipper_length_mm)# A tibble: 333 × 2\n  species flipper_length_mm\n  <chr>               <dbl>\n1 Adelie                181\n2 Adelie                186\n3 Adelie                195\n4 Adelie                193\n5 Adelie                190\n# ℹ 328 more rows\n# check original input is unchanged\nprint(penguins)# A tibble: 333 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n  <chr>   <chr>              <dbl>         <dbl>             <dbl>       <dbl> <chr> \n1 Adelie  Torgersen           39.1          18.7               181        3750 male  \n2 Adelie  Torgersen           39.5          17.4               186        3800 female\n3 Adelie  Torgersen           40.3          18                 195        3250 female\n4 Adelie  Torgersen           36.7          19.3               193        3450 female\n5 Adelie  Torgersen           39.3          20.6               190        3650 male  \n# ℹ 328 more rows\n# ℹ 1 more variable: year <dbl>\n# saving to a new object with a descriptive name is ALWAYS recommended!\npenguins_fewcols <- penguins %>% select(species, flipper_length_mm)\nprint(penguins_fewcols)# A tibble: 333 × 2\n  species flipper_length_mm\n  <chr>               <dbl>\n1 Adelie                181\n2 Adelie                186\n3 Adelie                195\n4 Adelie                193\n5 Adelie                190\n# ℹ 328 more rows\n# overwriting the original input is STRONGLY discouraged\n# because it's destructive and often causes problems later\npenguins <- penguins %>% select(species, flipper_length_mm)\nprint(penguins)# A tibble: 333 × 2\n  species flipper_length_mm\n  <chr>               <dbl>\n1 Adelie                181\n2 Adelie                186\n3 Adelie                195\n4 Adelie                193\n5 Adelie                190\n# ℹ 328 more rows\n# reload data frame since we need it for other examples\npenguins <- read_csv(\"https://bwu62.github.io/stat240-revamp/data/penguins.csv\")"},{"path":"intro-to-dplyr.html","id":"rename","chapter":"7 Intro to dplyr","heading":"7.4.2 rename()","text":"rename() used rename columns. another common operation right loading dataset. Examples:outside scope, can also apply function many/columns using rename_with(). example:","code":"\n# rename species as species_name, and island as island_name\npenguins %>%\n  rename(species_name = species, island_name = island)# A tibble: 333 × 8\n  species_name island_name bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  <chr>        <chr>                <dbl>         <dbl>             <dbl>       <dbl>\n1 Adelie       Torgersen             39.1          18.7               181        3750\n2 Adelie       Torgersen             39.5          17.4               186        3800\n3 Adelie       Torgersen             40.3          18                 195        3250\n4 Adelie       Torgersen             36.7          19.3               193        3450\n5 Adelie       Torgersen             39.3          20.6               190        3650\n# ℹ 328 more rows\n# ℹ 2 more variables: sex <chr>, year <dbl>\n# if you want to use irregular names, i.e. names with spaces or symbols,\n# you must surround them with \" \" quotes\npenguins %>%\n  rename(\"Bill Length (mm)\" = bill_length_mm)# A tibble: 333 × 8\n  species island `Bill Length (mm)` bill_depth_mm flipper_length_mm body_mass_g sex  \n  <chr>   <chr>               <dbl>         <dbl>             <dbl>       <dbl> <chr>\n1 Adelie  Torge…               39.1          18.7               181        3750 male \n2 Adelie  Torge…               39.5          17.4               186        3800 fema…\n3 Adelie  Torge…               40.3          18                 195        3250 fema…\n4 Adelie  Torge…               36.7          19.3               193        3450 fema…\n5 Adelie  Torge…               39.3          20.6               190        3650 male \n# ℹ 328 more rows\n# ℹ 1 more variable: year <dbl>\n# if you have a data frame with an extremely long and awkward name,\n# you can also use selector functions to help you rename it\n# (see ?starts_with help page for more details/examples)\ndf_badname <- tibble(\n  x = 1:3,\n  \"Really long (and awkward) name with !@#$% symbols\" = 4:6\n)\ndf_badname# A tibble: 3 × 2\n      x `Really long (and awkward) name with !@#$% symbols`\n  <int>                                               <int>\n1     1                                                   4\n2     2                                                   5\n3     3                                                   6\ndf_badname %>%\n  rename(y = starts_with(\"Really\") & ends_with(\"symbols\"))# A tibble: 3 × 2\n      x     y\n  <int> <int>\n1     1     4\n2     2     5\n3     3     6\npenguins %>%\n  rename_with(toupper)# A tibble: 333 × 8\n  SPECIES ISLAND    BILL_LENGTH_MM BILL_DEPTH_MM FLIPPER_LENGTH_MM BODY_MASS_G SEX   \n  <chr>   <chr>              <dbl>         <dbl>             <dbl>       <dbl> <chr> \n1 Adelie  Torgersen           39.1          18.7               181        3750 male  \n2 Adelie  Torgersen           39.5          17.4               186        3800 female\n3 Adelie  Torgersen           40.3          18                 195        3250 female\n4 Adelie  Torgersen           36.7          19.3               193        3450 female\n5 Adelie  Torgersen           39.3          20.6               190        3650 male  \n# ℹ 328 more rows\n# ℹ 1 more variable: YEAR <dbl>"},{"path":"intro-to-dplyr.html","id":"mutate","chapter":"7 Intro to dplyr","heading":"7.4.3 mutate()","text":"mutate() used either change existing column, add new columns. ’s easy function introduce tough one master. basic syntax df %>% mutate(col1 = expr1, col2 = expr2, ...) col1, col2, … columns changed/added (depending already exists) expr1, expr2, … R expressions.key thing remember expressions can vector computation using one columns data frame produces vector length single value (gets recycled).convenience, let’s select just columns continue demonstration:Now, ’s example applications mutate() using penguins2A notable function extremely useful inside mutate() case_when() can calculate different values depending certain conditions.28 basic syntax df %>% mutate(new_col = case_when(cond1 ~ expr1, cond2 ~ expr2, ...)) cond1, cond2, … logical condition vectors checked one one given order, expr1, expr2, … R expressions activated condition matches.example, suppose want create new column differently depending sex bill depth:’s worth restating vectorized operation columns can used inside mutate(), long result -length vector (single value recycled). includes essentially every function chapter 3!dplyr cheat sheet page 2 small list functions may useful inside mutate() advanced situations, cumsum() finding cumulative sums columns (.e. “running” sum), lag() lead() creating lagged leading vector useful computing changes time series data, na_if() selectively replacing specific values NA, several ranking functions like dense_rank() min_rank(), many .","code":"\npenguins2 <- penguins %>%\n  select(species, sex, bill_length_mm, bill_depth_mm)\nprint(penguins2)# A tibble: 333 × 4\n  species sex    bill_length_mm bill_depth_mm\n  <chr>   <chr>           <dbl>         <dbl>\n1 Adelie  male             39.1          18.7\n2 Adelie  female           39.5          17.4\n3 Adelie  female           40.3          18  \n4 Adelie  female           36.7          19.3\n5 Adelie  male             39.3          20.6\n# ℹ 328 more rows\n# we can easily add columns of constants\npenguins2 %>%\n  mutate(study = \"Palmer\", century = 21, true = TRUE)# A tibble: 333 × 7\n  species sex    bill_length_mm bill_depth_mm study  century true \n  <chr>   <chr>           <dbl>         <dbl> <chr>    <dbl> <lgl>\n1 Adelie  male             39.1          18.7 Palmer      21 TRUE \n2 Adelie  female           39.5          17.4 Palmer      21 TRUE \n3 Adelie  female           40.3          18   Palmer      21 TRUE \n4 Adelie  female           36.7          19.3 Palmer      21 TRUE \n5 Adelie  male             39.3          20.6 Palmer      21 TRUE \n# ℹ 328 more rows\n# we can also change existing columns using the same syntax,\n# e.g. we can capitalize and abbreviate sex to M and F\npenguins2 %>%\n  mutate(sex = substr(toupper(sex), 1, 1))# A tibble: 333 × 4\n  species sex   bill_length_mm bill_depth_mm\n  <chr>   <chr>          <dbl>         <dbl>\n1 Adelie  M               39.1          18.7\n2 Adelie  F               39.5          17.4\n3 Adelie  F               40.3          18  \n4 Adelie  F               36.7          19.3\n5 Adelie  M               39.3          20.6\n# ℹ 328 more rows\n# we can also use multiple columns in expressions\n# e.g. we can roughly estimate the volume of each bill,\n# see: https://allisonhorst.github.io/palmerpenguins/#bill-dimensions\npenguins2 %>%\n  mutate(bill_vol_mm3 = pi * (bill_depth_mm / 2)^2 * bill_length_mm)# A tibble: 333 × 5\n  species sex    bill_length_mm bill_depth_mm bill_vol_mm3\n  <chr>   <chr>           <dbl>         <dbl>        <dbl>\n1 Adelie  male             39.1          18.7       10739.\n2 Adelie  female           39.5          17.4        9393.\n3 Adelie  female           40.3          18         10255.\n4 Adelie  female           36.7          19.3       10737.\n5 Adelie  male             39.3          20.6       13098.\n# ℹ 328 more rows\n# you can also break this into steps, using intermediate variables\n# note intermediate variables can be used immediately in the same mutate() call\npenguins2 %>% mutate(\n  bill_cross_section_mm2 = pi * (bill_depth_mm / 2)^2,\n  bill_vol_mm3           = bill_cross_section_mm2 * bill_length_mm\n)# A tibble: 333 × 6\n  species sex    bill_length_mm bill_depth_mm bill_cross_section_mm2 bill_vol_mm3\n  <chr>   <chr>           <dbl>         <dbl>                  <dbl>        <dbl>\n1 Adelie  male             39.1          18.7                   275.       10739.\n2 Adelie  female           39.5          17.4                   238.        9393.\n3 Adelie  female           40.3          18                     254.       10255.\n4 Adelie  female           36.7          19.3                   293.       10737.\n5 Adelie  male             39.3          20.6                   333.       13098.\n# ℹ 328 more rows\n# you can even mix summary functions into your expression\n# e.g. standardize bill length and depth by subtracting mean and dividing by sd\npenguins2 %>% mutate(\n  bill_length_std = (bill_length_mm - mean(bill_length_mm)) / sd(bill_length_mm),\n  bill_depth_std  = (bill_depth_mm - mean(bill_depth_mm)) / sd(bill_depth_mm)\n)# A tibble: 333 × 6\n  species sex    bill_length_mm bill_depth_mm bill_length_std bill_depth_std\n  <chr>   <chr>           <dbl>         <dbl>           <dbl>          <dbl>\n1 Adelie  male             39.1          18.7          -0.895          0.780\n2 Adelie  female           39.5          17.4          -0.822          0.119\n3 Adelie  female           40.3          18            -0.675          0.424\n4 Adelie  female           36.7          19.3          -1.33           1.08 \n5 Adelie  male             39.3          20.6          -0.858          1.74 \n# ℹ 328 more rows\n# you can of course create columns of other data types\npenguins2 %>% mutate(\n  small_bill = bill_length_mm < 39 | bill_depth_mm < 18,\n  fake_dates = seq(mdy(\"1/1/25\"), mdy(\"1/1/25\") + nrow(penguins) - 1, by = 1)\n)# A tibble: 333 × 6\n  species sex    bill_length_mm bill_depth_mm small_bill fake_dates\n  <chr>   <chr>           <dbl>         <dbl> <lgl>      <date>    \n1 Adelie  male             39.1          18.7 FALSE      2025-01-01\n2 Adelie  female           39.5          17.4 TRUE       2025-01-02\n3 Adelie  female           40.3          18   FALSE      2025-01-03\n4 Adelie  female           36.7          19.3 TRUE       2025-01-04\n5 Adelie  male             39.3          20.6 FALSE      2025-01-05\n# ℹ 328 more rows\n# use case_when() inside a mutate() depending on some conditions\n# .default sets the \"default\" result, when no conditions match OR when we have NAs\npenguins2 %>% mutate(\n  new_col = case_when(\n    sex == \"male\" & bill_depth_mm <= 19 ~ bill_length_mm * 100,\n    sex == \"male\" & bill_depth_mm > 19 ~ bill_length_mm * -1,\n    sex == \"female\" ~ round(log((bill_length_mm / bill_depth_mm)^2), 2),\n    .default = 0\n  )\n)# A tibble: 333 × 5\n  species sex    bill_length_mm bill_depth_mm new_col\n  <chr>   <chr>           <dbl>         <dbl>   <dbl>\n1 Adelie  male             39.1          18.7 3910   \n2 Adelie  female           39.5          17.4    1.64\n3 Adelie  female           40.3          18      1.61\n4 Adelie  female           36.7          19.3    1.29\n5 Adelie  male             39.3          20.6  -39.3 \n# ℹ 328 more rows"},{"path":"intro-to-dplyr.html","id":"summarize","chapter":"7 Intro to dplyr","heading":"7.4.4 summarize()","text":"summarize() similar mutate() except MUST use summary functions, .e. function always reduce vector single value. , can use arbitrary function combination functions columns data frame, result can type (e.g. numeric, character, logical, date, etc.), long result singular.basic syntax df %>% summarize(col1 = expr1, col2 = expr2, ...) col1, col2, … names new summary columns, expr1, expr2, … R expressions reduce single value. Example:last function chunk n() special function takes arguments returns number rows.29A common applications summarize() data exploration:’s also common use summarize() compute statistical results. example can calculate 95% confidence intervals mean bill length depth (ignoring species/sex) something cover detail later course:, think ’s important stress expression involving columns results single value can used inside summarize(). dplyr cheat sheet page 2 examples useful summarizing functions first(), last(), nth() getting first, last, n-th observations group respectively. Feel free read .","code":"\n# let's compute several summary statistics of bill length\npenguins2 %>% summarize(\n  mean_length   = mean(bill_length_mm),\n  median_length = median(bill_length_mm),\n  sd_length     = sd(bill_length_mm),\n  iqr_length    = IQR(bill_length_mm),\n  max_length    = max(bill_length_mm),\n  min_length    = min(bill_length_mm),\n  n             = n()\n)# A tibble: 1 × 7\n  mean_length median_length sd_length iqr_length max_length min_length     n\n        <dbl>         <dbl>     <dbl>      <dbl>      <dbl>      <dbl> <int>\n1        44.0          44.5      5.47        9.1       59.6       32.1   333\n# compute a few other summaries to explore the data\n# note similar to mutate, we can immediately use a summarized column\npenguins2 %>% summarize(\n  n            = n(),\n  n_male       = sum(sex == \"male\"),\n  pct_male     = 100 * n_male / n,\n  n_female     = sum(sex == \"female\"),\n  pct_female   = 100 * n_female / n,\n  pct_NA       = 100 * mean(\n    is.na(species) | is.na(sex) | is.na(bill_length_mm) | is.na(bill_depth_mm)\n  ), # get proportion of rows with NA (mean of logicals gives proportion of TRUEs)\n  q90_len      = quantile(bill_length_mm, 0.90),\n  pmed_len     = mean(bill_length_mm <= median(bill_length_mm)),\n  correlation  = cor(bill_length_mm, bill_depth_mm)\n)# A tibble: 1 × 9\n      n n_male pct_male n_female pct_female pct_NA q90_len pmed_len correlation\n  <int>  <int>    <dbl>    <int>      <dbl>  <dbl>   <dbl>    <dbl>       <dbl>\n1   333    168     50.5      165       49.5      0    50.8    0.502      -0.229\n# we first compute some intermediate statistics,\n# then use those to compute the intervals\npenguins2 %>% summarize(\n  n            = n(),\n  mean_length  = mean(bill_length_mm),\n  sd_length    = sd(bill_length_mm),\n  mean_depth   = mean(bill_depth_mm),\n  sd_depth     = sd(bill_depth_mm),\n  length_95_ci = paste(\n    round(mean_length + c(-1, 1) * 1.96 * sd_length / sqrt(n), 2),\n    collapse = \",\"\n  ),\n  depth_95_ci  = paste(\n    round(mean_depth  + c(-1, 1) * 1.96 * sd_depth  / sqrt(n), 2),\n    collapse = \",\"\n  )\n)# A tibble: 1 × 7\n      n mean_length sd_length mean_depth sd_depth length_95_ci depth_95_ci\n  <int>       <dbl>     <dbl>      <dbl>    <dbl> <chr>        <chr>      \n1   333        44.0      5.47       17.2     1.97 43.41,44.58  16.95,17.38"},{"path":"intro-to-dplyr.html","id":"row-wise-functions","chapter":"7 Intro to dplyr","heading":"7.5 Row-wise functions","text":"Let’s move now row-wise functions, .e. functions primarily focus manipulating rows certain ways. , many , 3 important:filter() filtering rows keep,slice() (plus sibling functions) slicing specific rows,arrange() sorting rows.","code":""},{"path":"intro-to-dplyr.html","id":"filter","chapter":"7 Intro to dplyr","heading":"7.5.1 filter()","text":"filter() used filter rows keep. Note wording ; rows meet conditions KEPT, dropped. frequent point confusion beginners.Similar mutate() summarize(), can filter constructing logical expression using combination columns, long result vector TRUE/FALSE values, one row. end, rows TRUE returned output.Let’s return using original penguins data frame. ’s filtering examples:","code":"\n# filter to get penguins >=6kg\npenguins %>%\n  filter(body_mass_g >= 6000)# A tibble: 4 × 8\n  species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex  \n  <chr>   <chr>           <dbl>         <dbl>             <dbl>       <dbl> <chr>\n1 Gentoo  Biscoe           49.2          15.2               221        6300 male \n2 Gentoo  Biscoe           59.6          17                 230        6050 male \n3 Gentoo  Biscoe           51.1          16.3               220        6000 male \n4 Gentoo  Biscoe           48.8          16.2               222        6000 male \n# ℹ 1 more variable: year <dbl>\n# combining multiple filtering conditions using & and |\npenguins %>% filter(\n  (species == \"Adelie\" | species == \"Gentoo\") &\n    island %in% c(\"Biscoe\", \"Dream\") & year < 2008\n)# A tibble: 62 × 8\n  species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n  <chr>   <chr>           <dbl>         <dbl>             <dbl>       <dbl> <chr> \n1 Adelie  Biscoe           37.8          18.3               174        3400 female\n2 Adelie  Biscoe           37.7          18.7               180        3600 male  \n3 Adelie  Biscoe           35.9          19.2               189        3800 female\n4 Adelie  Biscoe           38.2          18.1               185        3950 male  \n5 Adelie  Biscoe           38.8          17.2               180        3800 male  \n# ℹ 57 more rows\n# ℹ 1 more variable: year <dbl>\n# you can of course use more complex functions and expressions,\n# you can also use , to separate multiple conditions in filter() instead of &\n# e.g. this code gets rows with no \"e\" in the species name, and also\n# bill depth is higher than median but flipper length is lower than median\npenguins %>% filter(\n  !grepl(\"e\", species),\n  bill_depth_mm > median(bill_depth_mm),\n  flipper_length_mm < median(flipper_length_mm)\n)# A tibble: 24 × 8\n  species   island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n  <chr>     <chr>           <dbl>         <dbl>             <dbl>       <dbl> <chr> \n1 Chinstrap Dream            46.5          17.9               192        3500 female\n2 Chinstrap Dream            50            19.5               196        3900 male  \n3 Chinstrap Dream            51.3          19.2               193        3650 male  \n4 Chinstrap Dream            45.4          18.7               188        3525 female\n5 Chinstrap Dream            46.1          18.2               178        3250 female\n# ℹ 19 more rows\n# ℹ 1 more variable: year <dbl>\n# an example of an even more complicated expression,\n# this line gets penguins with body mass and bill length\n# within a 1 SD circle centered around the mean of both,\n# then plots the result to visually inspect\n# note the result df can be directly piped into ggplot()\npenguins %>%\n  filter(\n    ((body_mass_g - mean(body_mass_g)) / sd(body_mass_g))^2 +\n      ((bill_length_mm - mean(bill_length_mm)) / sd(bill_length_mm))^2 <= 1\n  ) %>%\n  ggplot(aes(x = bill_length_mm, y = body_mass_g)) + geom_point() +\n  labs(title = \"Filter demo (points in a 1-SD circle around mean of x,y)\",\n       x = \"Bill length (mm)\", y = \"Body mass (g)\") +\n  coord_fixed(.0067) # make the plot window a square"},{"path":"intro-to-dplyr.html","id":"slice","chapter":"7 Intro to dplyr","heading":"7.5.2 slice()","text":"several slice...() functions dplyr slicing specific rows, similar filter() specialized. main one slice() used select position (.e. row number). Examples:slice_head() slice_tail() specifically slice rows top bottom, either number n proportion prop. Examples:slice_min() slice_max() used slice rows contain min/max values specific variable. example, suppose want get penguin smallest body mass:ties, default tied rows printed, e.g. suppose want 3 penguins longest flipper lengths:looks like 7 different Gentoo penguins tied 2nd place, returned! can disable argument, ’s recommended keep default behavior .can also set prop instead n return percent. E.g. let’s get top 1% penguins bill length:","code":"\n# slice rows 11-15\npenguins %>% slice(11:15)# A tibble: 5 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n  <chr>   <chr>              <dbl>         <dbl>             <dbl>       <dbl> <chr> \n1 Adelie  Torgersen           36.6          17.8               185        3700 female\n2 Adelie  Torgersen           38.7          19                 195        3450 female\n3 Adelie  Torgersen           42.5          20.7               197        4500 male  \n4 Adelie  Torgersen           34.4          18.4               184        3325 female\n5 Adelie  Torgersen           46            21.5               194        4200 male  \n# ℹ 1 more variable: year <dbl>\n# slice every 10th row\npenguins %>% slice(seq(0, nrow(penguins), 10))# A tibble: 33 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n  <chr>   <chr>              <dbl>         <dbl>             <dbl>       <dbl> <chr> \n1 Adelie  Torgersen           34.6          21.1               198        4400 male  \n2 Adelie  Biscoe              38.8          17.2               180        3800 male  \n3 Adelie  Dream               36.4          17                 195        3325 female\n4 Adelie  Dream               37            16.9               185        3000 female\n5 Adelie  Biscoe              41.4          18.6               191        3700 male  \n# ℹ 28 more rows\n# ℹ 1 more variable: year <dbl>\n# remove the first 200 rows\npenguins %>% slice(-(1:200))# A tibble: 133 × 8\n  species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n  <chr>   <chr>           <dbl>         <dbl>             <dbl>       <dbl> <chr> \n1 Gentoo  Biscoe           45            15.4               220        5050 male  \n2 Gentoo  Biscoe           43.8          13.9               208        4300 female\n3 Gentoo  Biscoe           45.5          15                 220        5000 male  \n4 Gentoo  Biscoe           43.2          14.5               208        4450 female\n5 Gentoo  Biscoe           50.4          15.3               224        5550 male  \n# ℹ 128 more rows\n# ℹ 1 more variable: year <dbl>\n# get the first 25 rows\npenguins %>% slice_head(n = 25)# A tibble: 25 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n  <chr>   <chr>              <dbl>         <dbl>             <dbl>       <dbl> <chr> \n1 Adelie  Torgersen           39.1          18.7               181        3750 male  \n2 Adelie  Torgersen           39.5          17.4               186        3800 female\n3 Adelie  Torgersen           40.3          18                 195        3250 female\n4 Adelie  Torgersen           36.7          19.3               193        3450 female\n5 Adelie  Torgersen           39.3          20.6               190        3650 male  \n# ℹ 20 more rows\n# ℹ 1 more variable: year <dbl>\n# get the last 10% of rows\npenguins %>% slice_tail(prop = 0.1)# A tibble: 33 × 8\n  species   island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n  <chr>     <chr>           <dbl>         <dbl>             <dbl>       <dbl> <chr> \n1 Chinstrap Dream            47.5          16.8               199        3900 female\n2 Chinstrap Dream            47.6          18.3               195        3850 female\n3 Chinstrap Dream            52            20.7               210        4800 male  \n4 Chinstrap Dream            46.9          16.6               192        2700 female\n5 Chinstrap Dream            53.5          19.9               205        4500 male  \n# ℹ 28 more rows\n# ℹ 1 more variable: year <dbl>\n# get smallest penguin by mass\npenguins %>% slice_min(body_mass_g, n = 1)# A tibble: 1 × 8\n  species   island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n  <chr>     <chr>           <dbl>         <dbl>             <dbl>       <dbl> <chr> \n1 Chinstrap Dream            46.9          16.6               192        2700 female\n# ℹ 1 more variable: year <dbl>\n# get 3 penguins with longest flippers\npenguins %>% slice_max(flipper_length_mm, n = 3)# A tibble: 8 × 8\n  species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex  \n  <chr>   <chr>           <dbl>         <dbl>             <dbl>       <dbl> <chr>\n1 Gentoo  Biscoe           54.3          15.7               231        5650 male \n2 Gentoo  Biscoe           50            16.3               230        5700 male \n3 Gentoo  Biscoe           59.6          17                 230        6050 male \n4 Gentoo  Biscoe           49.8          16.8               230        5700 male \n5 Gentoo  Biscoe           48.6          16                 230        5800 male \n6 Gentoo  Biscoe           52.1          17                 230        5550 male \n7 Gentoo  Biscoe           51.5          16.3               230        5500 male \n8 Gentoo  Biscoe           55.1          16                 230        5850 male \n# ℹ 1 more variable: year <dbl>\n# get top 1% of penguins by bill length\npenguins %>% slice_max(bill_length_mm, prop = 0.01)# A tibble: 3 × 8\n  species   island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n  <chr>     <chr>           <dbl>         <dbl>             <dbl>       <dbl> <chr> \n1 Gentoo    Biscoe           59.6          17                 230        6050 male  \n2 Chinstrap Dream            58            17.8               181        3700 female\n3 Gentoo    Biscoe           55.9          17                 228        5600 male  \n# ℹ 1 more variable: year <dbl>"},{"path":"intro-to-dplyr.html","id":"arrange","chapter":"7 Intro to dplyr","heading":"7.5.3 arrange()","text":"arrange() used sort rows. Note since sorts rows, change data frame “meaningful” way (generally, order rows/columns considered “meaningful” change). ’s primarily used visual appeal, .e. neater presentation dataset.syntax df %>% arrange(expr1, expr2, ...) expr1, expr2, … can simply column, vector expression using columns data frame (similar mutate() filter()) whose resultant values used sorting rows. important notes:next expression used break ties previous expressions, otherwise ’s ignored! E.g. two rows can sorted expr1, used. However two rows tied expr1, expr2 (exists) used try break tie, . also frequent point confusion beginners.Default order always ascending, .e. small large, Z, earlier later, FALSE TRUE. descending order, wrap expression desc().Rows completely tied may returned order.30Examples arrange():","code":"\n# sort penguins by ascending flipper length\npenguins %>% arrange(flipper_length_mm)# A tibble: 333 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n  <chr>   <chr>              <dbl>         <dbl>             <dbl>       <dbl> <chr> \n1 Adelie  Biscoe              37.9          18.6               172        3150 female\n2 Adelie  Biscoe              37.8          18.3               174        3400 female\n3 Adelie  Torgersen           40.2          17                 176        3450 female\n4 Adelie  Dream               39.5          16.7               178        3250 female\n5 Adelie  Dream               37.2          18.1               178        3900 male  \n# ℹ 328 more rows\n# ℹ 1 more variable: year <dbl>\n# sort penguins by descending flipper length\npenguins %>% arrange(desc(flipper_length_mm))# A tibble: 333 × 8\n  species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex  \n  <chr>   <chr>           <dbl>         <dbl>             <dbl>       <dbl> <chr>\n1 Gentoo  Biscoe           54.3          15.7               231        5650 male \n2 Gentoo  Biscoe           50            16.3               230        5700 male \n3 Gentoo  Biscoe           59.6          17                 230        6050 male \n4 Gentoo  Biscoe           49.8          16.8               230        5700 male \n5 Gentoo  Biscoe           48.6          16                 230        5800 male \n# ℹ 328 more rows\n# ℹ 1 more variable: year <dbl>\n# since mathematical expressions are allowed, this is also equivalent\npenguins %>% arrange(-flipper_length_mm)# A tibble: 333 × 8\n  species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex  \n  <chr>   <chr>           <dbl>         <dbl>             <dbl>       <dbl> <chr>\n1 Gentoo  Biscoe           54.3          15.7               231        5650 male \n2 Gentoo  Biscoe           50            16.3               230        5700 male \n3 Gentoo  Biscoe           59.6          17                 230        6050 male \n4 Gentoo  Biscoe           49.8          16.8               230        5700 male \n5 Gentoo  Biscoe           48.6          16                 230        5800 male \n# ℹ 328 more rows\n# ℹ 1 more variable: year <dbl>\n# sort first by island, then by descending species (if there are ties)\npenguins %>% arrange(island, desc(species))# A tibble: 333 × 8\n  species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n  <chr>   <chr>           <dbl>         <dbl>             <dbl>       <dbl> <chr> \n1 Gentoo  Biscoe           46.1          13.2               211        4500 female\n2 Gentoo  Biscoe           50            16.3               230        5700 male  \n3 Gentoo  Biscoe           48.7          14.1               210        4450 female\n4 Gentoo  Biscoe           50            15.2               218        5700 male  \n5 Gentoo  Biscoe           47.6          14.5               215        5400 male  \n# ℹ 328 more rows\n# ℹ 1 more variable: year <dbl>\n# again, any expression is possible,\n# let's sort first by number of SDs away from mean body mass,\n# then by descending last letter of species name for ties,\n# then by ascending approximate bill volume if there are further ties\npenguins %>% arrange(\n  abs(body_mass_g - mean(body_mass_g)) / sd(body_mass_g),\n  desc(substr(species, nchar(species), nchar(species))),\n  pi * (bill_depth_mm / 2)^2 * bill_length_mm\n)# A tibble: 333 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n  <chr>   <chr>              <dbl>         <dbl>             <dbl>       <dbl> <chr> \n1 Gentoo  Biscoe              45.3          13.8               208        4200 female\n2 Gentoo  Biscoe              45.5          13.9               210        4200 female\n3 Gentoo  Biscoe              45.8          14.6               210        4200 female\n4 Adelie  Torgersen           35.1          19.4               193        4200 male  \n5 Adelie  Torgersen           46            21.5               194        4200 male  \n# ℹ 328 more rows\n# ℹ 1 more variable: year <dbl>"},{"path":"intro-to-dplyr.html","id":"other-row-functions","chapter":"7 Intro to dplyr","heading":"7.5.4 Other row functions","text":"Besides , ’s row operations may sometimes need, distinct() removing duplicates add_row() manually adding new observations. may occasionally show later contexts, now please feel free explore .","code":""},{"path":"intro-to-dplyr.html","id":"missing-values","chapter":"7 Intro to dplyr","heading":"7.6 Missing values","text":"ending chapter, let’s briefly discuss handling missing values, .e. NAs R. Missing values unfortunately common data science usually tricky handle well due many associated pitfalls. briefly discuss missing values theoretical perspective, practical R handling perspective.","code":""},{"path":"intro-to-dplyr.html","id":"why-missing","chapter":"7 Intro to dplyr","heading":"7.6.1 Why missing?","text":"working missing values, ’s important ask : “data missing?” Usually, impossible answer definitively, different observations may missing different reasons.Without getting scope, broadly speaking data can missing either relevant reason relevant reason.31For example, suppose ’m setting weather station several sensors, small budget, buy cheap pressure sensor isn’t properly weather-proofed prone producing NAs rains. NAs missing relevant reason; ’s systematic pattern behind missingness. Since rain pressure closely related, means pattern missingness meaningful, removing NAs add significant bias data.Now, suppose replace different cheap pressure sensor weather-proof just slightly buggy overall, every hour ’s independently 1% chance just randomly read NA. data missing relevant reason, .e. pattern missingness meaningful, can simply remove NAs.working data ALWAYS look pattern NAs ask ’s evidence relevant reason missingness. remove NAs clear pattern, otherwise risk introducing systematic biases analysis results., extremely simplified, sufficient now. general, STAT 240 encounter datasets ’s probably ok drop NA values, assume datasets real world.","code":""},{"path":"intro-to-dplyr.html","id":"nas-in-r","chapter":"7 Intro to dplyr","heading":"7.6.2 NAs in R","text":"Let’s now briefly discuss R techniques handling NAs. First ’s important review difference NA NaN R:NA means absence observation, .e. data point recorded.NaN usually result mathematically invalid operation, like 0/0, 0*Inf, Inf-Inf.\nNote sqrt(-1) log(-1) also return NaN since input type real type. Replace -1 -1+0i trigger complex evaluation.\nNote sqrt(-1) log(-1) also return NaN since input type real type. Replace -1 -1+0i trigger complex evaluation.Even though ’re exact , NA NaN considered missing R can handled together using operations covered . ’s also worth noting NA \"NA\", .e. string made letters \"N\" \"\"general expressions involving NA result NA output (though notable exceptions, particularly character types).","code":"\n# check if two objects are the exact same\nidentical(NA, NA)[1] TRUE\nidentical(NA, NaN)[1] FALSE\nidentical(NA, \"NA\")[1] FALSE"},{"path":"intro-to-dplyr.html","id":"identifying-nas","chapter":"7 Intro to dplyr","heading":"7.6.3 Identifying NAs","text":"NAs (also NaNs) vector can identified using .na() produces TRUE/FALSE vector corresponding value NA :Since ordinary vectorized function, can used inside compatible dplyr function , mutate(), summarize(), filter(), arrange(). demonstrate , let’s create demo data frame mix data types well missing values.common rudimentary way checking missingness aforementioned summary() function section 4.3.1. Note base R summary(), Tidyverse summarize() just covered . Note however reports NAs columns, depending type.better way pipe summarize() use .na() either sum() count mean() proportion missing values. can also add ! front .na() get count/proportion non-missing values. 4 combinations shown :just want apply one function columns, can use summarize_all() shortcut exactly . example, can apply function \\(x) mean(.na(x))*100 get percent missing column:check help page summarize_all() ’ll notice function tagged  basically means can keep using , ’s new recommended alternative syntax.32 function, new recommendation use across() instead, bit outside scope, read discretion.course can also use filter() inspect rows missing values () columns. Examples:One thing, notice ’s actually another form missingness df.demo? data frame appears contain daily observations, \"2024-01-05\" appears missing completely data frame. sneaky situation data missing existing data frame completely surprisingly common can sometimes hard identify (since ’s NAs date column detect).One easy way fix use complete() full_seq() tidyr, yet another core Tidyverse package. combination can used generate complete() data frame generating full_seq()–uence values specified column. columns filled NAs default. Example:","code":"\n# demo missing vector, note both NA/NaN count as missing\nx <- c(3, 8, NA, 2, NaN)\n# which values are NA?\nis.na(x)[1] FALSE FALSE  TRUE FALSE  TRUE\n# get only non-NA values\nx[!is.na(x)][1] 3 8 2\n# demo data frame with missing values\ndf.demo <- tibble(\n  date = ymd(\"24.1.1\") + c(0:3, 5),\n  x = c(NA, rep(c(\"A\", \"B\"), 2)),\n  y = c(NA, 1, 2, NA, 3),\n  z = c(NA, TRUE, FALSE, NA, NA)\n)\ndf.demo# A tibble: 5 × 4\n  date       x         y z    \n  <date>     <chr> <dbl> <lgl>\n1 2024-01-01 <NA>     NA NA   \n2 2024-01-02 A         1 TRUE \n3 2024-01-03 B         2 FALSE\n4 2024-01-04 A        NA NA   \n5 2024-01-06 B         3 NA   \nsummary(df.demo)      date                 x                   y           z          \n Min.   :2024-01-01   Length:5           Min.   :1.0   Mode :logical  \n 1st Qu.:2024-01-02   Class :character   1st Qu.:1.5   FALSE:1        \n Median :2024-01-03   Mode  :character   Median :2.0   TRUE :1        \n Mean   :2024-01-03                      Mean   :2.0   NA's :3        \n 3rd Qu.:2024-01-04                      3rd Qu.:2.5                  \n Max.   :2024-01-06                      Max.   :3.0                  \n                                         NA's   :2                    \n# get the count/proportion of missing/non-missing values\ndf.demo %>% summarize(\n  num_date_na   = sum(is.na(date)),\n  prop_x_na     = mean(is.na(x)),\n  num_y_not_na  = sum(!is.na(y)),\n  prop_z_not_na = mean(!is.na(z)),\n  nrows         = n() # add number of rows for convenience\n)# A tibble: 1 × 5\n  num_date_na prop_x_na num_y_not_na prop_z_not_na nrows\n        <int>     <dbl>        <int>         <dbl> <int>\n1           0       0.2            3           0.4     5\n# get percent missing in each column\ndf.demo %>% summarize_all(\\(x) mean(is.na(x)) * 100)# A tibble: 1 × 4\n   date     x     y     z\n  <dbl> <dbl> <dbl> <dbl>\n1     0    20    40    60\n# get rows with missing x\ndf.demo %>% filter(is.na(x))# A tibble: 1 × 4\n  date       x         y z    \n  <date>     <chr> <dbl> <lgl>\n1 2024-01-01 <NA>     NA NA   \n# get rows with missing x or y\ndf.demo %>% filter(is.na(x) | is.na(y))# A tibble: 2 × 4\n  date       x         y z    \n  <date>     <chr> <dbl> <lgl>\n1 2024-01-01 <NA>     NA NA   \n2 2024-01-04 A        NA NA   \n# get rows where y is missing but x is NOT missing\ndf.demo %>% filter(is.na(y) & !is.na(x))# A tibble: 1 × 4\n  date       x         y z    \n  <date>     <chr> <dbl> <lgl>\n1 2024-01-04 A        NA NA   \n# get rows where ANY variable is missing\n# this uses if_any() which checks if any given cols satisfy a condition\ndf.demo %>% filter(if_any(everything(), is.na))# A tibble: 3 × 4\n  date       x         y z    \n  <date>     <chr> <dbl> <lgl>\n1 2024-01-01 <NA>     NA NA   \n2 2024-01-04 A        NA NA   \n3 2024-01-06 B         3 NA   \n# an alternative method in base R that's maybe slightly simpler\n# complete.cases() gives a TRUE/FALSE vector of which rows are complete (no NA)\ndf.demo[!complete.cases(df.demo),]# A tibble: 3 × 4\n  date       x         y z    \n  <date>     <chr> <dbl> <lgl>\n1 2024-01-01 <NA>     NA NA   \n2 2024-01-04 A        NA NA   \n3 2024-01-06 B         3 NA   \n# this generates a full sequence of dates\n# the argument 1 indicates the sequence increases by 1 each observation\ndf.demo %>% complete(date = full_seq(date, 1))# A tibble: 6 × 4\n  date       x         y z    \n  <date>     <chr> <dbl> <lgl>\n1 2024-01-01 <NA>     NA NA   \n2 2024-01-02 A         1 TRUE \n3 2024-01-03 B         2 FALSE\n4 2024-01-04 A        NA NA   \n5 2024-01-05 <NA>     NA NA   \n6 2024-01-06 B         3 NA   \n# to show ONLY rows that were added using this operation,\n# we can first create a dummy column filled with some value,\n# complete the data frame, then filter to where the dummy column is NA\ndf.demo %>%\n  mutate(preexisting = TRUE) %>%\n  complete(date = full_seq(date, 1)) %>%\n  filter(is.na(preexisting))# A tibble: 1 × 5\n  date       x         y z     preexisting\n  <date>     <chr> <dbl> <lgl> <lgl>      \n1 2024-01-05 <NA>     NA NA    NA         "},{"path":"intro-to-dplyr.html","id":"dropping-nas","chapter":"7 Intro to dplyr","heading":"7.6.4 Dropping NAs","text":"mentioned , limited scope class, usually rows NAs can just dropped simplicity. easiest way use drop_na(col1, col2, ...) col1, col2, … columns care dropping NAs . left empty, drop_na() drop rows column contains NA. Examples:Make sure drop NAs absolutely necessary; avoid -dropping! E.g. suppose want use x y analysis. drop rows z missing since probably won’t impede work.another common pitfall students. general extremely “lazy” dropping, .e. use drop_na() absolutely necessary .","code":"\n# drop only rows where x is missing\ndf.demo %>% drop_na(x)# A tibble: 4 × 4\n  date       x         y z    \n  <date>     <chr> <dbl> <lgl>\n1 2024-01-02 A         1 TRUE \n2 2024-01-03 B         2 FALSE\n3 2024-01-04 A        NA NA   \n4 2024-01-06 B         3 NA   \n# drop rows where x and y are missing\ndf.demo %>% drop_na(x, y)# A tibble: 3 × 4\n  date       x         y z    \n  <date>     <chr> <dbl> <lgl>\n1 2024-01-02 A         1 TRUE \n2 2024-01-03 B         2 FALSE\n3 2024-01-06 B         3 NA   \n# drop rows where ANY values are missing\ndf.demo %>% drop_na()# A tibble: 2 × 4\n  date       x         y z    \n  <date>     <chr> <dbl> <lgl>\n1 2024-01-02 A         1 TRUE \n2 2024-01-03 B         2 FALSE"},{"path":"intro-to-dplyr.html","id":"replacing-nas","chapter":"7 Intro to dplyr","heading":"7.6.5 Replacing NAs","text":"rare circumstances, may want replace NAs values. course can kind conditional replacement using mutate() case_when(), can also use mutate() specialized tidyr function replace_na(). Examples :datasets, NAs may coded using certain obviously invalid values, e.g. -9999. can course also mutated using case_when(), ’s special function na_if() just purpose:","code":"\n# suppose we need to replace NAs in y column with 0\n# using mutate and case_when:\ndf.demo %>% mutate(\n  y = case_when(\n    is.na(y) ~ 0,\n    .default = y\n  )\n)# A tibble: 5 × 4\n  date       x         y z    \n  <date>     <chr> <dbl> <lgl>\n1 2024-01-01 <NA>      0 NA   \n2 2024-01-02 A         1 TRUE \n3 2024-01-03 B         2 FALSE\n4 2024-01-04 A         0 NA   \n5 2024-01-06 B         3 NA   \n# same thing but using replace_na instead of case_when:\ndf.demo %>% mutate(y = replace_na(y, 0))# A tibble: 5 × 4\n  date       x         y z    \n  <date>     <chr> <dbl> <lgl>\n1 2024-01-01 <NA>      0 NA   \n2 2024-01-02 A         1 TRUE \n3 2024-01-03 B         2 FALSE\n4 2024-01-04 A         0 NA   \n5 2024-01-06 B         3 NA   \ndf.demo <- df.demo %>% mutate(w = c(-9999, -9999, 20, 30, -9999))\ndf.demo# A tibble: 5 × 5\n  date       x         y z         w\n  <date>     <chr> <dbl> <lgl> <dbl>\n1 2024-01-01 <NA>     NA NA    -9999\n2 2024-01-02 A         1 TRUE  -9999\n3 2024-01-03 B         2 FALSE    20\n4 2024-01-04 A        NA NA       30\n5 2024-01-06 B         3 NA    -9999\ndf.demo %>% mutate(w = na_if(w, -9999))# A tibble: 5 × 5\n  date       x         y z         w\n  <date>     <chr> <dbl> <lgl> <dbl>\n1 2024-01-01 <NA>     NA NA       NA\n2 2024-01-02 A         1 TRUE     NA\n3 2024-01-03 B         2 FALSE    20\n4 2024-01-04 A        NA NA       30\n5 2024-01-06 B         3 NA       NA"},{"path":"advanced-operations.html","id":"advanced-operations","chapter":"8 Advanced Operations","heading":"8 Advanced Operations","text":"chapter, cover advanced, yet incredibly useful data tidying operations like grouping, joining, binding, pivoting. Along way, also make extensive use dplyr functions learned previous chapter.","code":""},{"path":"advanced-operations.html","id":"grouping","chapter":"8 Advanced Operations","heading":"8.1 Grouping","text":"Often, need apply dplyr’s various operations like mutate(), summarize(), slicing function across entire dataset groups. important technique across data science, whether ’s data cleaning exploration visualization modeling.default, data frames grouped created imported. can create grouping structure group_by() function. basic syntax df %>% group_by(col1, col2, ...) col1, col2, … variables whose values used determine groups. can group just 1 variable, 2 variables, many variables needed. Rows values chosen columns grouped together.grouping, operations normally run across rows now run across group. ’s simple examples using familiar penguins dataset start:","code":"\n# import tidyverse,tweak some options (optional), and load dataset\nlibrary(tidyverse)\noptions(readr.show_col_types = FALSE)\nsource(\"https://bwu62.github.io/stat240-revamp/ggplot_theme_options.R\")\npenguins <- read_csv(\"https://bwu62.github.io/stat240-revamp/data/penguins.csv\")\n# group by species and get mean/median/sd body mass + sample size of each group\npenguins %>%\n  group_by(species) %>%\n  summarize(\n    mean_mass   = mean(body_mass_g),\n    median_mass = median(body_mass_g),\n    sd_mass     = sd(body_mass_g),\n    n           = n()\n  )# A tibble: 3 × 5\n  species   mean_mass median_mass sd_mass     n\n  <chr>         <dbl>       <dbl>   <dbl> <int>\n1 Adelie        3706.        3700    459.   146\n2 Chinstrap     3733.        3700    384.    68\n3 Gentoo        5092.        5050    501.   119\n# we can also group by multiple, e.g. group by species + sex\npenguins %>%\n  group_by(species, sex) %>%\n  summarize(\n    mean_mass   = mean(body_mass_g),\n    median_mass = median(body_mass_g),\n    sd_mass     = sd(body_mass_g),\n    n           = n()\n  )# A tibble: 6 × 6\n# Groups:   species [3]\n  species   sex    mean_mass median_mass sd_mass     n\n  <chr>     <chr>      <dbl>       <dbl>   <dbl> <int>\n1 Adelie    female     3369.        3400    269.    73\n2 Adelie    male       4043.        4000    347.    73\n3 Chinstrap female     3527.        3550    285.    34\n4 Chinstrap male       3939.        3950    362.    34\n5 Gentoo    female     4680.        4700    282.    58\n6 Gentoo    male       5485.        5500    313.    61\n# we can of course also mutate within groups\n# e.g. convert bill length in mm to number of SDs\n#      away from species group mean\n# to better show the result, I'm forcing it to print all rows in order\n# but hiding the output in a collapsible box for style\npenguins %>%\n  select(species, sex, bill_length_mm) %>%\n  group_by(species) %>%\n  mutate(\n    n = n(),\n    bill_length_std = (bill_length_mm - mean(bill_length_mm)) / sd(bill_length_mm)\n  ) %>%\n  arrange(species, bill_length_std) %>%\n  print(n = Inf)# A tibble: 333 × 5\n# Groups:   species [3]\n    species   sex    bill_length_mm     n bill_length_std\n    <chr>     <chr>           <dbl> <int>           <dbl>\n  1 Adelie    female           32.1   146        -2.53   \n  2 Adelie    female           33.1   146        -2.15   \n  3 Adelie    female           33.5   146        -2.00   \n  4 Adelie    female           34     146        -1.81   \n  5 Adelie    female           34.4   146        -1.66   \n  6 Adelie    female           34.5   146        -1.62   \n  7 Adelie    male             34.6   146        -1.59   \n  8 Adelie    female           34.6   146        -1.59   \n  9 Adelie    female           35     146        -1.44   \n 10 Adelie    female           35     146        -1.44   \n 11 Adelie    male             35.1   146        -1.40   \n 12 Adelie    female           35.2   146        -1.36   \n 13 Adelie    female           35.3   146        -1.32   \n 14 Adelie    female           35.5   146        -1.25   \n 15 Adelie    female           35.5   146        -1.25   \n 16 Adelie    female           35.6   146        -1.21   \n 17 Adelie    female           35.7   146        -1.17   \n 18 Adelie    female           35.7   146        -1.17   \n 19 Adelie    female           35.7   146        -1.17   \n 20 Adelie    female           35.9   146        -1.10   \n 21 Adelie    female           35.9   146        -1.10   \n 22 Adelie    female           36     146        -1.06   \n 23 Adelie    female           36     146        -1.06   \n 24 Adelie    female           36     146        -1.06   \n 25 Adelie    female           36     146        -1.06   \n 26 Adelie    female           36.2   146        -0.985  \n 27 Adelie    female           36.2   146        -0.985  \n 28 Adelie    female           36.2   146        -0.985  \n 29 Adelie    male             36.3   146        -0.948  \n 30 Adelie    female           36.4   146        -0.910  \n 31 Adelie    female           36.4   146        -0.910  \n 32 Adelie    female           36.5   146        -0.873  \n 33 Adelie    female           36.5   146        -0.873  \n 34 Adelie    female           36.6   146        -0.835  \n 35 Adelie    female           36.6   146        -0.835  \n 36 Adelie    female           36.7   146        -0.798  \n 37 Adelie    female           36.7   146        -0.798  \n 38 Adelie    female           36.8   146        -0.760  \n 39 Adelie    female           36.9   146        -0.723  \n 40 Adelie    female           37     146        -0.685  \n 41 Adelie    female           37     146        -0.685  \n 42 Adelie    male             37.2   146        -0.610  \n 43 Adelie    male             37.2   146        -0.610  \n 44 Adelie    female           37.3   146        -0.572  \n 45 Adelie    male             37.3   146        -0.572  \n 46 Adelie    female           37.3   146        -0.572  \n 47 Adelie    male             37.5   146        -0.497  \n 48 Adelie    female           37.6   146        -0.460  \n 49 Adelie    male             37.6   146        -0.460  \n 50 Adelie    female           37.6   146        -0.460  \n 51 Adelie    male             37.7   146        -0.422  \n 52 Adelie    female           37.7   146        -0.422  \n 53 Adelie    male             37.7   146        -0.422  \n 54 Adelie    female           37.8   146        -0.385  \n 55 Adelie    male             37.8   146        -0.385  \n 56 Adelie    male             37.8   146        -0.385  \n 57 Adelie    female           37.9   146        -0.347  \n 58 Adelie    female           37.9   146        -0.347  \n 59 Adelie    female           38.1   146        -0.272  \n 60 Adelie    female           38.1   146        -0.272  \n 61 Adelie    female           38.1   146        -0.272  \n 62 Adelie    female           38.1   146        -0.272  \n 63 Adelie    male             38.2   146        -0.234  \n 64 Adelie    male             38.2   146        -0.234  \n 65 Adelie    male             38.3   146        -0.197  \n 66 Adelie    female           38.5   146        -0.122  \n 67 Adelie    male             38.6   146        -0.0841 \n 68 Adelie    female           38.6   146        -0.0841 \n 69 Adelie    female           38.6   146        -0.0841 \n 70 Adelie    female           38.7   146        -0.0466 \n 71 Adelie    male             38.8   146        -0.00900\n 72 Adelie    male             38.8   146        -0.00900\n 73 Adelie    female           38.8   146        -0.00900\n 74 Adelie    female           38.9   146         0.0286 \n 75 Adelie    female           38.9   146         0.0286 \n 76 Adelie    female           39     146         0.0661 \n 77 Adelie    female           39     146         0.0661 \n 78 Adelie    male             39     146         0.0661 \n 79 Adelie    male             39.1   146         0.104  \n 80 Adelie    male             39.2   146         0.141  \n 81 Adelie    male             39.2   146         0.141  \n 82 Adelie    male             39.2   146         0.141  \n 83 Adelie    male             39.3   146         0.179  \n 84 Adelie    female           39.5   146         0.254  \n 85 Adelie    female           39.5   146         0.254  \n 86 Adelie    female           39.5   146         0.254  \n 87 Adelie    male             39.6   146         0.291  \n 88 Adelie    female           39.6   146         0.291  \n 89 Adelie    female           39.6   146         0.291  \n 90 Adelie    male             39.6   146         0.291  \n 91 Adelie    female           39.6   146         0.291  \n 92 Adelie    male             39.7   146         0.329  \n 93 Adelie    male             39.7   146         0.329  \n 94 Adelie    female           39.7   146         0.329  \n 95 Adelie    male             39.7   146         0.329  \n 96 Adelie    male             39.8   146         0.367  \n 97 Adelie    male             40.1   146         0.479  \n 98 Adelie    female           40.2   146         0.517  \n 99 Adelie    male             40.2   146         0.517  \n100 Adelie    female           40.2   146         0.517  \n101 Adelie    female           40.3   146         0.554  \n102 Adelie    male             40.3   146         0.554  \n103 Adelie    female           40.5   146         0.629  \n104 Adelie    male             40.5   146         0.629  \n105 Adelie    male             40.6   146         0.667  \n106 Adelie    male             40.6   146         0.667  \n107 Adelie    male             40.6   146         0.667  \n108 Adelie    male             40.6   146         0.667  \n109 Adelie    male             40.7   146         0.705  \n110 Adelie    male             40.8   146         0.742  \n111 Adelie    male             40.8   146         0.742  \n112 Adelie    male             40.9   146         0.780  \n113 Adelie    female           40.9   146         0.780  \n114 Adelie    male             41     146         0.817  \n115 Adelie    female           41.1   146         0.855  \n116 Adelie    male             41.1   146         0.855  \n117 Adelie    male             41.1   146         0.855  \n118 Adelie    male             41.1   146         0.855  \n119 Adelie    male             41.1   146         0.855  \n120 Adelie    male             41.1   146         0.855  \n121 Adelie    male             41.1   146         0.855  \n122 Adelie    male             41.3   146         0.930  \n123 Adelie    male             41.3   146         0.930  \n124 Adelie    male             41.4   146         0.967  \n125 Adelie    male             41.4   146         0.967  \n126 Adelie    male             41.5   146         1.01   \n127 Adelie    male             41.5   146         1.01   \n128 Adelie    male             41.6   146         1.04   \n129 Adelie    male             41.8   146         1.12   \n130 Adelie    male             42     146         1.19   \n131 Adelie    male             42.1   146         1.23   \n132 Adelie    female           42.2   146         1.27   \n133 Adelie    male             42.2   146         1.27   \n134 Adelie    male             42.3   146         1.31   \n135 Adelie    male             42.5   146         1.38   \n136 Adelie    male             42.7   146         1.46   \n137 Adelie    male             42.8   146         1.49   \n138 Adelie    male             42.9   146         1.53   \n139 Adelie    male             43.1   146         1.61   \n140 Adelie    male             43.2   146         1.64   \n141 Adelie    male             43.2   146         1.64   \n142 Adelie    male             44.1   146         1.98   \n143 Adelie    male             44.1   146         1.98   \n144 Adelie    male             45.6   146         2.54   \n145 Adelie    male             45.8   146         2.62   \n146 Adelie    male             46     146         2.70   \n147 Chinstrap female           40.9    68        -2.38   \n148 Chinstrap female           42.4    68        -1.93   \n149 Chinstrap female           42.5    68        -1.90   \n150 Chinstrap female           42.5    68        -1.90   \n151 Chinstrap female           43.2    68        -1.69   \n152 Chinstrap female           43.5    68        -1.60   \n153 Chinstrap female           45.2    68        -1.09   \n154 Chinstrap female           45.2    68        -1.09   \n155 Chinstrap female           45.4    68        -1.03   \n156 Chinstrap female           45.5    68        -0.998  \n157 Chinstrap female           45.6    68        -0.968  \n158 Chinstrap female           45.7    68        -0.938  \n159 Chinstrap female           45.7    68        -0.938  \n160 Chinstrap female           45.9    68        -0.879  \n161 Chinstrap female           46      68        -0.849  \n162 Chinstrap female           46.1    68        -0.819  \n163 Chinstrap female           46.2    68        -0.789  \n164 Chinstrap female           46.4    68        -0.729  \n165 Chinstrap female           46.4    68        -0.729  \n166 Chinstrap female           46.5    68        -0.699  \n167 Chinstrap female           46.6    68        -0.669  \n168 Chinstrap female           46.7    68        -0.639  \n169 Chinstrap female           46.8    68        -0.609  \n170 Chinstrap female           46.9    68        -0.579  \n171 Chinstrap female           47      68        -0.549  \n172 Chinstrap female           47.5    68        -0.399  \n173 Chinstrap female           47.6    68        -0.369  \n174 Chinstrap female           48.1    68        -0.220  \n175 Chinstrap male             48.5    68        -0.1000 \n176 Chinstrap male             49      68         0.0498 \n177 Chinstrap male             49      68         0.0498 \n178 Chinstrap male             49.2    68         0.110  \n179 Chinstrap male             49.3    68         0.140  \n180 Chinstrap male             49.5    68         0.199  \n181 Chinstrap male             49.6    68         0.229  \n182 Chinstrap male             49.7    68         0.259  \n183 Chinstrap female           49.8    68         0.289  \n184 Chinstrap male             50      68         0.349  \n185 Chinstrap female           50.1    68         0.379  \n186 Chinstrap male             50.2    68         0.409  \n187 Chinstrap female           50.2    68         0.409  \n188 Chinstrap male             50.3    68         0.439  \n189 Chinstrap male             50.5    68         0.499  \n190 Chinstrap female           50.5    68         0.499  \n191 Chinstrap male             50.6    68         0.529  \n192 Chinstrap male             50.7    68         0.559  \n193 Chinstrap male             50.8    68         0.589  \n194 Chinstrap male             50.8    68         0.589  \n195 Chinstrap male             50.9    68         0.619  \n196 Chinstrap female           50.9    68         0.619  \n197 Chinstrap male             51      68         0.649  \n198 Chinstrap male             51.3    68         0.739  \n199 Chinstrap male             51.3    68         0.739  \n200 Chinstrap male             51.3    68         0.739  \n201 Chinstrap male             51.4    68         0.768  \n202 Chinstrap male             51.5    68         0.798  \n203 Chinstrap male             51.7    68         0.858  \n204 Chinstrap male             51.9    68         0.918  \n205 Chinstrap male             52      68         0.948  \n206 Chinstrap male             52      68         0.948  \n207 Chinstrap male             52      68         0.948  \n208 Chinstrap male             52.2    68         1.01   \n209 Chinstrap male             52.7    68         1.16   \n210 Chinstrap male             52.8    68         1.19   \n211 Chinstrap male             53.5    68         1.40   \n212 Chinstrap male             54.2    68         1.61   \n213 Chinstrap male             55.8    68         2.09   \n214 Chinstrap female           58      68         2.74   \n215 Gentoo    female           40.9   119        -2.15   \n216 Gentoo    female           41.7   119        -1.89   \n217 Gentoo    female           42     119        -1.79   \n218 Gentoo    female           42.6   119        -1.60   \n219 Gentoo    female           42.7   119        -1.57   \n220 Gentoo    female           42.8   119        -1.54   \n221 Gentoo    female           42.9   119        -1.50   \n222 Gentoo    female           43.2   119        -1.41   \n223 Gentoo    female           43.3   119        -1.37   \n224 Gentoo    female           43.3   119        -1.37   \n225 Gentoo    female           43.4   119        -1.34   \n226 Gentoo    female           43.5   119        -1.31   \n227 Gentoo    female           43.5   119        -1.31   \n228 Gentoo    female           43.6   119        -1.28   \n229 Gentoo    female           43.8   119        -1.21   \n230 Gentoo    female           44     119        -1.15   \n231 Gentoo    male             44.4   119        -1.02   \n232 Gentoo    female           44.5   119        -0.988  \n233 Gentoo    female           44.9   119        -0.859  \n234 Gentoo    female           44.9   119        -0.859  \n235 Gentoo    male             45     119        -0.827  \n236 Gentoo    female           45.1   119        -0.795  \n237 Gentoo    female           45.1   119        -0.795  \n238 Gentoo    female           45.1   119        -0.795  \n239 Gentoo    male             45.2   119        -0.762  \n240 Gentoo    female           45.2   119        -0.762  \n241 Gentoo    male             45.2   119        -0.762  \n242 Gentoo    female           45.2   119        -0.762  \n243 Gentoo    female           45.3   119        -0.730  \n244 Gentoo    female           45.3   119        -0.730  \n245 Gentoo    female           45.4   119        -0.698  \n246 Gentoo    female           45.5   119        -0.666  \n247 Gentoo    female           45.5   119        -0.666  \n248 Gentoo    male             45.5   119        -0.666  \n249 Gentoo    female           45.5   119        -0.666  \n250 Gentoo    female           45.7   119        -0.601  \n251 Gentoo    female           45.8   119        -0.569  \n252 Gentoo    female           45.8   119        -0.569  \n253 Gentoo    female           46.1   119        -0.473  \n254 Gentoo    male             46.1   119        -0.473  \n255 Gentoo    female           46.2   119        -0.440  \n256 Gentoo    male             46.2   119        -0.440  \n257 Gentoo    female           46.2   119        -0.440  \n258 Gentoo    male             46.3   119        -0.408  \n259 Gentoo    male             46.4   119        -0.376  \n260 Gentoo    female           46.4   119        -0.376  \n261 Gentoo    female           46.5   119        -0.344  \n262 Gentoo    female           46.5   119        -0.344  \n263 Gentoo    female           46.5   119        -0.344  \n264 Gentoo    female           46.5   119        -0.344  \n265 Gentoo    female           46.6   119        -0.312  \n266 Gentoo    male             46.7   119        -0.279  \n267 Gentoo    male             46.8   119        -0.247  \n268 Gentoo    male             46.8   119        -0.247  \n269 Gentoo    female           46.8   119        -0.247  \n270 Gentoo    female           46.9   119        -0.215  \n271 Gentoo    female           47.2   119        -0.118  \n272 Gentoo    female           47.2   119        -0.118  \n273 Gentoo    male             47.3   119        -0.0863 \n274 Gentoo    female           47.4   119        -0.0541 \n275 Gentoo    female           47.5   119        -0.0219 \n276 Gentoo    female           47.5   119        -0.0219 \n277 Gentoo    female           47.5   119        -0.0219 \n278 Gentoo    male             47.6   119         0.0103 \n279 Gentoo    female           47.7   119         0.0425 \n280 Gentoo    male             47.8   119         0.0747 \n281 Gentoo    male             48.1   119         0.171  \n282 Gentoo    female           48.2   119         0.203  \n283 Gentoo    male             48.2   119         0.203  \n284 Gentoo    male             48.4   119         0.268  \n285 Gentoo    male             48.4   119         0.268  \n286 Gentoo    female           48.4   119         0.268  \n287 Gentoo    male             48.5   119         0.300  \n288 Gentoo    female           48.5   119         0.300  \n289 Gentoo    male             48.6   119         0.332  \n290 Gentoo    female           48.7   119         0.364  \n291 Gentoo    male             48.7   119         0.364  \n292 Gentoo    male             48.7   119         0.364  \n293 Gentoo    male             48.8   119         0.397  \n294 Gentoo    male             49     119         0.461  \n295 Gentoo    female           49.1   119         0.493  \n296 Gentoo    female           49.1   119         0.493  \n297 Gentoo    male             49.1   119         0.493  \n298 Gentoo    male             49.2   119         0.525  \n299 Gentoo    male             49.3   119         0.558  \n300 Gentoo    male             49.4   119         0.590  \n301 Gentoo    male             49.5   119         0.622  \n302 Gentoo    male             49.5   119         0.622  \n303 Gentoo    male             49.6   119         0.654  \n304 Gentoo    male             49.6   119         0.654  \n305 Gentoo    male             49.8   119         0.719  \n306 Gentoo    male             49.8   119         0.719  \n307 Gentoo    male             49.9   119         0.751  \n308 Gentoo    male             50     119         0.783  \n309 Gentoo    male             50     119         0.783  \n310 Gentoo    male             50     119         0.783  \n311 Gentoo    male             50     119         0.783  \n312 Gentoo    male             50.1   119         0.815  \n313 Gentoo    male             50.2   119         0.847  \n314 Gentoo    male             50.4   119         0.912  \n315 Gentoo    male             50.4   119         0.912  \n316 Gentoo    male             50.5   119         0.944  \n317 Gentoo    male             50.5   119         0.944  \n318 Gentoo    female           50.5   119         0.944  \n319 Gentoo    male             50.7   119         1.01   \n320 Gentoo    male             50.8   119         1.04   \n321 Gentoo    male             50.8   119         1.04   \n322 Gentoo    male             51.1   119         1.14   \n323 Gentoo    male             51.1   119         1.14   \n324 Gentoo    male             51.3   119         1.20   \n325 Gentoo    male             51.5   119         1.27   \n326 Gentoo    male             52.1   119         1.46   \n327 Gentoo    male             52.2   119         1.49   \n328 Gentoo    male             52.5   119         1.59   \n329 Gentoo    male             53.4   119         1.88   \n330 Gentoo    male             54.3   119         2.17   \n331 Gentoo    male             55.1   119         2.42   \n332 Gentoo    male             55.9   119         2.68   \n333 Gentoo    male             59.6   119         3.87   \n# get the largest 3 penguins by bill depth from each species\npenguins %>%\n  group_by(species) %>%\n  slice_max(bill_depth_mm, n = 3)# A tibble: 9 × 8\n# Groups:   species [3]\n  species   island   bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex  \n  <chr>     <chr>             <dbl>         <dbl>             <dbl>       <dbl> <chr>\n1 Adelie    Torgers…           46            21.5               194        4200 male \n2 Adelie    Torgers…           38.6          21.2               191        3800 male \n3 Adelie    Dream              42.3          21.2               191        4150 male \n4 Chinstrap Dream              54.2          20.8               201        4300 male \n5 Chinstrap Dream              52            20.7               210        4800 male \n6 Chinstrap Dream              51.7          20.3               194        3775 male \n7 Gentoo    Biscoe             44.4          17.3               219        5250 male \n8 Gentoo    Biscoe             50.8          17.3               228        5600 male \n9 Gentoo    Biscoe             52.2          17.1               228        5400 male \n# ℹ 1 more variable: year <dbl>"},{"path":"advanced-operations.html","id":"regrouping","chapter":"8 Advanced Operations","heading":"8.1.1 Regrouping","text":"Sometimes one group_by() may enough get need; may need re-group_by() something else finish job. example, suppose want see percent species came different islands. requires two uses group_by():combination df %>% group_by(...) %>% summarize(n = n()) common, shortcut : df %>% count(...). can demonstrate another example involving regrouping. Suppose want know percent species male/female:Groups also useful prepping data frames plotting. example, ’s chunk produces bar plot showing mean body mass changes species sex:","code":"\n# first group by species + island and summarize to get size of each group,\n# then regroup by species and MUTATE (not summarize) totals for each species,\n# then divide these to get proportion of each species from each island\npenguins %>%\n  group_by(species, island) %>%\n  summarize(n = n()) %>%\n  group_by(species) %>%\n  mutate(\n    species_total = sum(n),\n    pct_of_species_from_island = n / species_total * 100\n  )# A tibble: 5 × 5\n# Groups:   species [3]\n  species   island        n species_total pct_of_species_from_island\n  <chr>     <chr>     <int>         <int>                      <dbl>\n1 Adelie    Biscoe       44           146                       30.1\n2 Adelie    Dream        55           146                       37.7\n3 Adelie    Torgersen    47           146                       32.2\n4 Chinstrap Dream        68            68                      100  \n5 Gentoo    Biscoe      119           119                      100  \n# again, first group by species + sex and get size using the shortcut count(),\n# then regroup by species and MUTATE totals for each species,\n# then divide to get proportions of each species that were male/female\npenguins %>%\n  count(species, sex) %>%\n  group_by(species) %>%\n  mutate(\n    species_total = sum(n),\n    pct_of_species_each_sex = n / species_total * 100\n  )# A tibble: 6 × 5\n# Groups:   species [3]\n  species   sex        n species_total pct_of_species_each_sex\n  <chr>     <chr>  <int>         <int>                   <dbl>\n1 Adelie    female    73           146                    50  \n2 Adelie    male      73           146                    50  \n3 Chinstrap female    34            68                    50  \n4 Chinstrap male      34            68                    50  \n5 Gentoo    female    58           119                    48.7\n6 Gentoo    male      61           119                    51.3\n# get mean body mass by species + sex and plot\n# note print will print the input, then pass it on\n# to the next operation (as if it's not even there)\npenguins %>% \n  group_by(species, sex) %>% \n  summarize(mean_mass = mean(body_mass_g)) %>% print %>% \n  ggplot(aes(x = species, y = mean_mass, fill = sex)) +\n  geom_col(position = \"dodge2\") +\n  labs(x = \"Species\", y = \"Mean body mass (g)\",\n       title = \"Mean body mass of Palmer penguins by species + sex\")# A tibble: 6 × 3\n# Groups:   species [3]\n  species   sex    mean_mass\n  <chr>     <chr>      <dbl>\n1 Adelie    female     3369.\n2 Adelie    male       4043.\n3 Chinstrap female     3527.\n4 Chinstrap male       3939.\n5 Gentoo    female     4680.\n6 Gentoo    male       5485."},{"path":"advanced-operations.html","id":"ungrouping","chapter":"8 Advanced Operations","heading":"8.1.2 Ungrouping","text":"Many operations output grouped data frames. example, look closely output previous chunks ’ll see # Groups:   species [3] . means operations run continue execute grouped way.can remove grouping structure ungroup(). allows revert running operations entire data frame. Example:","code":"\n# get count of each species + sex combination,\n# but this time get its percentage out of ALL observations\npenguins %>%\n  count(species, sex) %>%\n  ungroup() %>%\n  mutate(pct_of_all = n / sum(n))# A tibble: 6 × 4\n  species   sex        n pct_of_all\n  <chr>     <chr>  <int>      <dbl>\n1 Adelie    female    73      0.219\n2 Adelie    male      73      0.219\n3 Chinstrap female    34      0.102\n4 Chinstrap male      34      0.102\n5 Gentoo    female    58      0.174\n6 Gentoo    male      61      0.183"},{"path":"advanced-operations.html","id":"more-practice-fertility-data","chapter":"8 Advanced Operations","heading":"8.1.3 More practice! (fertility data)","text":"Let’s give penguins dataset rest practice dplyr grouping bit different dataset. following chunk imports fertility.csv, cleaned global total fertility rate (TFR) data set World Bank, giving average TFR (number children born per woman lifetime based current conditions) year country 1960 present. remainder notes, simply refer TFR “fertility”. past decades, global fertility sharply declining countries. Many countries now replacement rate 2.1, leading widespread concerns population collapse latter part 21st century.country 60 years data dataset. can see overview countries represented dataset listed region income group temporarily dropping years, removing duplicates, printing full output collapsible box:One additional small processing step continuing convert income_group ordered factor (see section 3.9.5), important later.can begin running summaries explore dataset. start, ’s chunk showing number countries median income group countries region:Next, ’s chunk showing median fertility rate region recent year 2023, well countries highest lowest 2023 rates (rates ) region:can also show latest rate country, well change 2000, just start 21st century:countries biggest drop fertility?countries actually increased?many countries now vs replacement rate? can grouping rate least 2.1—mention can use transformations inside group_by() well?Let’s make plots data well. 2 plots showing median fertility rate time grouping either region income level:","code":"\nfertility <- read_csv(\"https://bwu62.github.io/stat240-revamp/data/fertility.csv\")\nfertility# A tibble: 13,792 × 6\n   code  country     region     income_group  year  rate\n   <chr> <chr>       <chr>      <chr>        <dbl> <dbl>\n 1 AFG   Afghanistan South Asia Low           1960  7.28\n 2 AFG   Afghanistan South Asia Low           1961  7.28\n 3 AFG   Afghanistan South Asia Low           1962  7.29\n 4 AFG   Afghanistan South Asia Low           1963  7.30\n 5 AFG   Afghanistan South Asia Low           1964  7.30\n 6 AFG   Afghanistan South Asia Low           1965  7.30\n 7 AFG   Afghanistan South Asia Low           1966  7.32\n 8 AFG   Afghanistan South Asia Low           1967  7.34\n 9 AFG   Afghanistan South Asia Low           1968  7.36\n10 AFG   Afghanistan South Asia Low           1969  7.39\n# ℹ 13,782 more rows\nfertility %>%\n  select(country:income_group) %>%\n  distinct() %>%\n  print(n = Inf)# A tibble: 215 × 3\n    country                  region                     income_group\n    <chr>                    <chr>                      <chr>       \n  1 Afghanistan              South Asia                 Low         \n  2 Albania                  Europe & Central Asia      Upper middle\n  3 Algeria                  Middle East & North Africa Upper middle\n  4 American Samoa           East Asia & Pacific        High        \n  5 Andorra                  Europe & Central Asia      High        \n  6 Angola                   Sub-Saharan Africa         Lower middle\n  7 Antigua & Barbuda        Latin America & Caribbean  High        \n  8 Argentina                Latin America & Caribbean  Upper middle\n  9 Armenia                  Europe & Central Asia      Upper middle\n 10 Aruba                    Latin America & Caribbean  High        \n 11 Australia                East Asia & Pacific        High        \n 12 Austria                  Europe & Central Asia      High        \n 13 Azerbaijan               Europe & Central Asia      Upper middle\n 14 Bahamas                  Latin America & Caribbean  High        \n 15 Bahrain                  Middle East & North Africa High        \n 16 Bangladesh               South Asia                 Lower middle\n 17 Barbados                 Latin America & Caribbean  High        \n 18 Belarus                  Europe & Central Asia      Upper middle\n 19 Belgium                  Europe & Central Asia      High        \n 20 Belize                   Latin America & Caribbean  Upper middle\n 21 Benin                    Sub-Saharan Africa         Lower middle\n 22 Bermuda                  North America              High        \n 23 Bhutan                   South Asia                 Lower middle\n 24 Bolivia                  Latin America & Caribbean  Lower middle\n 25 Bosnia & Herzegovina     Europe & Central Asia      Upper middle\n 26 Botswana                 Sub-Saharan Africa         Upper middle\n 27 Brazil                   Latin America & Caribbean  Upper middle\n 28 Brunei                   East Asia & Pacific        High        \n 29 Bulgaria                 Europe & Central Asia      High        \n 30 Burkina Faso             Sub-Saharan Africa         Low         \n 31 Burundi                  Sub-Saharan Africa         Low         \n 32 Cambodia                 East Asia & Pacific        Lower middle\n 33 Cameroon                 Sub-Saharan Africa         Lower middle\n 34 Canada                   North America              High        \n 35 Cape Verde               Sub-Saharan Africa         Lower middle\n 36 Cayman Islands           Latin America & Caribbean  High        \n 37 Central African Republic Sub-Saharan Africa         Low         \n 38 Chad                     Sub-Saharan Africa         Low         \n 39 Channel Islands          Europe & Central Asia      High        \n 40 Chile                    Latin America & Caribbean  High        \n 41 China                    East Asia & Pacific        Upper middle\n 42 Colombia                 Latin America & Caribbean  Upper middle\n 43 Comoros                  Sub-Saharan Africa         Lower middle\n 44 Congo, Dem. Rep.         Sub-Saharan Africa         Low         \n 45 Congo, Rep.              Sub-Saharan Africa         Lower middle\n 46 Costa Rica               Latin America & Caribbean  Upper middle\n 47 Croatia                  Europe & Central Asia      High        \n 48 Cuba                     Latin America & Caribbean  Upper middle\n 49 Curaçao                  Latin America & Caribbean  High        \n 50 Cyprus                   Europe & Central Asia      High        \n 51 Czechia                  Europe & Central Asia      High        \n 52 Denmark                  Europe & Central Asia      High        \n 53 Djibouti                 Middle East & North Africa Lower middle\n 54 Dominica                 Latin America & Caribbean  Upper middle\n 55 Dominican Republic       Latin America & Caribbean  Upper middle\n 56 East Timor               East Asia & Pacific        Lower middle\n 57 Ecuador                  Latin America & Caribbean  Upper middle\n 58 Egypt                    Middle East & North Africa Lower middle\n 59 El Salvador              Latin America & Caribbean  Upper middle\n 60 Equatorial Guinea        Sub-Saharan Africa         Upper middle\n 61 Eritrea                  Sub-Saharan Africa         Low         \n 62 Estonia                  Europe & Central Asia      High        \n 63 Eswatini                 Sub-Saharan Africa         Lower middle\n 64 Ethiopia                 Sub-Saharan Africa         Low         \n 65 Faroe Islands            Europe & Central Asia      High        \n 66 Fiji                     East Asia & Pacific        Upper middle\n 67 Finland                  Europe & Central Asia      High        \n 68 France                   Europe & Central Asia      High        \n 69 French Polynesia         East Asia & Pacific        High        \n 70 Gabon                    Sub-Saharan Africa         Upper middle\n 71 Gambia                   Sub-Saharan Africa         Low         \n 72 Georgia                  Europe & Central Asia      Upper middle\n 73 Germany                  Europe & Central Asia      High        \n 74 Ghana                    Sub-Saharan Africa         Lower middle\n 75 Gibraltar                Europe & Central Asia      High        \n 76 Greece                   Europe & Central Asia      High        \n 77 Greenland                Europe & Central Asia      High        \n 78 Grenada                  Latin America & Caribbean  Upper middle\n 79 Guam                     East Asia & Pacific        High        \n 80 Guatemala                Latin America & Caribbean  Upper middle\n 81 Guinea                   Sub-Saharan Africa         Lower middle\n 82 Guinea-Bissau            Sub-Saharan Africa         Low         \n 83 Guyana                   Latin America & Caribbean  High        \n 84 Haiti                    Latin America & Caribbean  Lower middle\n 85 Honduras                 Latin America & Caribbean  Lower middle\n 86 Hong Kong                East Asia & Pacific        High        \n 87 Hungary                  Europe & Central Asia      High        \n 88 Iceland                  Europe & Central Asia      High        \n 89 India                    South Asia                 Lower middle\n 90 Indonesia                East Asia & Pacific        Upper middle\n 91 Iran                     Middle East & North Africa Upper middle\n 92 Iraq                     Middle East & North Africa Upper middle\n 93 Ireland                  Europe & Central Asia      High        \n 94 Isle of Man              Europe & Central Asia      High        \n 95 Israel                   Middle East & North Africa High        \n 96 Italy                    Europe & Central Asia      High        \n 97 Ivory Coast              Sub-Saharan Africa         Lower middle\n 98 Jamaica                  Latin America & Caribbean  Upper middle\n 99 Japan                    East Asia & Pacific        High        \n100 Jordan                   Middle East & North Africa Lower middle\n101 Kazakhstan               Europe & Central Asia      Upper middle\n102 Kenya                    Sub-Saharan Africa         Lower middle\n103 Kiribati                 East Asia & Pacific        Lower middle\n104 Kosovo                   Europe & Central Asia      Upper middle\n105 Kuwait                   Middle East & North Africa High        \n106 Kyrgyzstan               Europe & Central Asia      Lower middle\n107 Laos                     East Asia & Pacific        Lower middle\n108 Latvia                   Europe & Central Asia      High        \n109 Lebanon                  Middle East & North Africa Lower middle\n110 Lesotho                  Sub-Saharan Africa         Lower middle\n111 Liberia                  Sub-Saharan Africa         Low         \n112 Libya                    Middle East & North Africa Upper middle\n113 Liechtenstein            Europe & Central Asia      High        \n114 Lithuania                Europe & Central Asia      High        \n115 Luxembourg               Europe & Central Asia      High        \n116 Macao                    East Asia & Pacific        High        \n117 Madagascar               Sub-Saharan Africa         Low         \n118 Malawi                   Sub-Saharan Africa         Low         \n119 Malaysia                 East Asia & Pacific        Upper middle\n120 Maldives                 South Asia                 Upper middle\n121 Mali                     Sub-Saharan Africa         Low         \n122 Malta                    Middle East & North Africa High        \n123 Marshall Islands         East Asia & Pacific        Upper middle\n124 Mauritania               Sub-Saharan Africa         Lower middle\n125 Mauritius                Sub-Saharan Africa         Upper middle\n126 Mexico                   Latin America & Caribbean  Upper middle\n127 Micronesia               East Asia & Pacific        Lower middle\n128 Moldova                  Europe & Central Asia      Upper middle\n129 Monaco                   Europe & Central Asia      High        \n130 Mongolia                 East Asia & Pacific        Upper middle\n131 Montenegro               Europe & Central Asia      Upper middle\n132 Morocco                  Middle East & North Africa Lower middle\n133 Mozambique               Sub-Saharan Africa         Low         \n134 Myanmar                  East Asia & Pacific        Lower middle\n135 Namibia                  Sub-Saharan Africa         Upper middle\n136 Nauru                    East Asia & Pacific        High        \n137 Nepal                    South Asia                 Lower middle\n138 Netherlands              Europe & Central Asia      High        \n139 New Caledonia            East Asia & Pacific        High        \n140 New Zealand              East Asia & Pacific        High        \n141 Nicaragua                Latin America & Caribbean  Lower middle\n142 Niger                    Sub-Saharan Africa         Low         \n143 Nigeria                  Sub-Saharan Africa         Lower middle\n144 North Korea              East Asia & Pacific        Low         \n145 North Macedonia          Europe & Central Asia      Upper middle\n146 Northern Mariana Islands East Asia & Pacific        High        \n147 Norway                   Europe & Central Asia      High        \n148 Oman                     Middle East & North Africa High        \n149 Pakistan                 South Asia                 Lower middle\n150 Palau                    East Asia & Pacific        High        \n151 Panama                   Latin America & Caribbean  High        \n152 Papua New Guinea         East Asia & Pacific        Lower middle\n153 Paraguay                 Latin America & Caribbean  Upper middle\n154 Peru                     Latin America & Caribbean  Upper middle\n155 Philippines              East Asia & Pacific        Lower middle\n156 Poland                   Europe & Central Asia      High        \n157 Portugal                 Europe & Central Asia      High        \n158 Puerto Rico              Latin America & Caribbean  High        \n159 Qatar                    Middle East & North Africa High        \n160 Romania                  Europe & Central Asia      High        \n161 Russia                   Europe & Central Asia      High        \n162 Rwanda                   Sub-Saharan Africa         Low         \n163 Samoa                    East Asia & Pacific        Lower middle\n164 San Marino               Europe & Central Asia      High        \n165 Saudi Arabia             Middle East & North Africa High        \n166 Senegal                  Sub-Saharan Africa         Lower middle\n167 Serbia                   Europe & Central Asia      Upper middle\n168 Seychelles               Sub-Saharan Africa         High        \n169 Sierra Leone             Sub-Saharan Africa         Low         \n170 Singapore                East Asia & Pacific        High        \n171 Sint Maarten             Latin America & Caribbean  High        \n172 Slovakia                 Europe & Central Asia      High        \n173 Slovenia                 Europe & Central Asia      High        \n174 Solomon Islands          East Asia & Pacific        Lower middle\n175 Somalia                  Sub-Saharan Africa         Low         \n176 South Africa             Sub-Saharan Africa         Upper middle\n177 South Korea              East Asia & Pacific        High        \n178 South Sudan              Sub-Saharan Africa         Low         \n179 Spain                    Europe & Central Asia      High        \n180 Sri Lanka                South Asia                 Lower middle\n181 St. Kitts & Nevis        Latin America & Caribbean  High        \n182 St. Lucia                Latin America & Caribbean  Upper middle\n183 St. Martin               Latin America & Caribbean  High        \n184 St. Vincent & Grenadines Latin America & Caribbean  Upper middle\n185 Sudan                    Sub-Saharan Africa         Low         \n186 Suriname                 Latin America & Caribbean  Upper middle\n187 Sweden                   Europe & Central Asia      High        \n188 Switzerland              Europe & Central Asia      High        \n189 Syria                    Middle East & North Africa Low         \n190 São Tomé & Principe      Sub-Saharan Africa         Lower middle\n191 Tajikistan               Europe & Central Asia      Lower middle\n192 Tanzania                 Sub-Saharan Africa         Lower middle\n193 Thailand                 East Asia & Pacific        Upper middle\n194 Togo                     Sub-Saharan Africa         Low         \n195 Tonga                    East Asia & Pacific        Upper middle\n196 Trinidad & Tobago        Latin America & Caribbean  High        \n197 Tunisia                  Middle East & North Africa Lower middle\n198 Turkey                   Europe & Central Asia      Upper middle\n199 Turkmenistan             Europe & Central Asia      Upper middle\n200 Turks & Caicos Islands   Latin America & Caribbean  High        \n201 Tuvalu                   East Asia & Pacific        Upper middle\n202 Uganda                   Sub-Saharan Africa         Low         \n203 Ukraine                  Europe & Central Asia      Upper middle\n204 United Arab Emirates     Middle East & North Africa High        \n205 United Kingdom           Europe & Central Asia      High        \n206 United States            North America              High        \n207 Uruguay                  Latin America & Caribbean  High        \n208 Uzbekistan               Europe & Central Asia      Lower middle\n209 Vanuatu                  East Asia & Pacific        Lower middle\n210 Vietnam                  East Asia & Pacific        Lower middle\n211 Virgin Islands           Latin America & Caribbean  High        \n212 West Bank & Gaza         Middle East & North Africa Lower middle\n213 Yemen                    Middle East & North Africa Low         \n214 Zambia                   Sub-Saharan Africa         Lower middle\n215 Zimbabwe                 Sub-Saharan Africa         Lower middle\nfertility <- fertility %>% mutate(\n  income_group = factor(income_group, ordered = TRUE, levels = c(\n    \"Low\", \"Lower middle\", \"Upper middle\", \"High\"))\n)\n# strangely, base R median doesn't work on ordered categories,\n# but we can use Median from DescTools instead\nfertility %>%\n  select(country, region, income_group) %>%\n  distinct() %>%\n  group_by(region) %>%\n  summarize(n = n(), median = DescTools::Median(income_group)) %>%\n  arrange(desc(median))# A tibble: 7 × 3\n  region                         n median      \n  <chr>                      <int> <ord>       \n1 Europe & Central Asia         58 High        \n2 North America                  3 High        \n3 East Asia & Pacific           37 Upper middle\n4 Latin America & Caribbean     40 Upper middle\n5 Middle East & North Africa    21 Upper middle\n6 South Asia                     8 Lower middle\n7 Sub-Saharan Africa            48 Lower middle\n# first filter to get the right year, then\n# sort by region, rate (so min, max are the first, last in each group)\n# then summarize to get median, and min/max country/rate\nfertility %>%\n  filter(year == max(year)) %>%\n  arrange(region, rate) %>%\n  group_by(region) %>%\n  summarize(\n    n           = n(),\n    median      = median(rate),\n    min_country = first(country),\n    min         = first(rate),\n    max_country = last(country),\n    max         = last(rate)\n  ) %>%\n  arrange(median)# A tibble: 7 × 7\n  region                         n median min_country   min max_country     max\n  <chr>                      <int>  <dbl> <chr>       <dbl> <chr>         <dbl>\n1 North America                  3   1.40 Canada      1.26  United States  1.62\n2 Europe & Central Asia         58   1.49 Ukraine     0.977 Uzbekistan     3.5 \n3 Latin America & Caribbean     41   1.62 Puerto Rico 0.92  St. Martin     2.72\n4 South Asia                     8   1.98 Bhutan      1.46  Afghanistan    4.84\n5 East Asia & Pacific           37   2.13 Macao       0.586 Samoa          3.83\n6 Middle East & North Africa    21   2.36 Malta       1.06  Yemen          4.59\n7 Sub-Saharan Africa            48   4.00 Mauritius   1.39  Somalia        6.13\n# first filter to get the right years, then\n# sort by country, year (so 2000, 2023 are first and last in each group)\n# then summarize to get 2000 and 2023 rates, mutate to get change,\n# then ungroup, distinct, and arrange to display a neat output\n# again, collapsing output due to lengthy print out\nfertility_change <- fertility %>%\n  filter(year %in% c(2000, max(year))) %>%\n  arrange(country, year) %>%\n  group_by(country) %>%\n  mutate(\n    rate2000 = first(rate),\n    rate2023 = last(rate),\n    change   = rate2023 - rate2000\n  ) %>%\n  select(country, region, rate2000, change, rate2023) %>%\n  ungroup() %>%\n  distinct() %>%\n  arrange(rate2023) %>%\n  print(n = Inf)# A tibble: 215 × 5\n    country                  region                     rate2000  change rate2023\n    <chr>                    <chr>                         <dbl>   <dbl>    <dbl>\n  1 Macao                    East Asia & Pacific           0.933 -0.347     0.586\n  2 South Korea              East Asia & Pacific           1.48  -0.759     0.721\n  3 Hong Kong                East Asia & Pacific           1.03  -0.281     0.751\n  4 Puerto Rico              Latin America & Caribbean     2.05  -1.13      0.92 \n  5 Singapore                East Asia & Pacific           1.6   -0.63      0.97 \n  6 Ukraine                  Europe & Central Asia         1.11  -0.131     0.977\n  7 China                    East Asia & Pacific           1.63  -0.629     0.999\n  8 Malta                    Middle East & North Africa    1.68  -0.62      1.06 \n  9 Andorra                  Europe & Central Asia         1.27  -0.191     1.08 \n 10 Spain                    Europe & Central Asia         1.22  -0.1000    1.12 \n 11 San Marino               Europe & Central Asia         1.40  -0.255     1.15 \n 12 Poland                   Europe & Central Asia         1.37  -0.212     1.16 \n 13 Chile                    Latin America & Caribbean     2.05  -0.879     1.17 \n 14 Lithuania                Europe & Central Asia         1.39  -0.21      1.18 \n 15 Curaçao                  Latin America & Caribbean     2.34  -1.14      1.2  \n 16 Italy                    Europe & Central Asia         1.26  -0.0600    1.2  \n 17 Japan                    East Asia & Pacific           1.36  -0.160     1.2  \n 18 United Arab Emirates     Middle East & North Africa    2.73  -1.53      1.2  \n 19 Belarus                  Europe & Central Asia         1.32  -0.109     1.21 \n 20 Thailand                 East Asia & Pacific           1.73  -0.522     1.21 \n 21 Luxembourg               Europe & Central Asia         1.76  -0.51      1.25 \n 22 Canada                   North America                 1.51  -0.25      1.26 \n 23 Finland                  Europe & Central Asia         1.73  -0.47      1.26 \n 24 Estonia                  Europe & Central Asia         1.36  -0.0500    1.31 \n 25 Austria                  Europe & Central Asia         1.36  -0.0400    1.32 \n 26 Greece                   Europe & Central Asia         1.25   0.0700    1.32 \n 27 Costa Rica               Latin America & Caribbean     2.41  -1.08      1.33 \n 28 Switzerland              Europe & Central Asia         1.5   -0.17      1.33 \n 29 Albania                  Europe & Central Asia         2.22  -0.869     1.35 \n 30 Jamaica                  Latin America & Caribbean     2.35  -0.987     1.36 \n 31 Latvia                   Europe & Central Asia         1.25   0.110     1.36 \n 32 Channel Islands          Europe & Central Asia         1.49  -0.120     1.37 \n 33 Bahamas                  Latin America & Caribbean     2.06  -0.687     1.37 \n 34 St. Lucia                Latin America & Caribbean     2.21  -0.828     1.38 \n 35 Cyprus                   Europe & Central Asia         1.64  -0.249     1.39 \n 36 Germany                  Europe & Central Asia         1.38   0.0100    1.39 \n 37 Mauritius                Sub-Saharan Africa            1.99  -0.6       1.39 \n 38 Norway                   Europe & Central Asia         1.85  -0.45      1.4  \n 39 Bermuda                  North America                 1.74  -0.337     1.40 \n 40 Uruguay                  Latin America & Caribbean     2.20  -0.787     1.41 \n 41 Russia                   Europe & Central Asia         1.20   0.215     1.41 \n 42 Netherlands              Europe & Central Asia         1.72  -0.29      1.43 \n 43 Cuba                     Latin America & Caribbean     1.61  -0.169     1.44 \n 44 Portugal                 Europe & Central Asia         1.55  -0.110     1.44 \n 45 Sint Maarten             Latin America & Caribbean     1.98  -0.535     1.45 \n 46 Czechia                  Europe & Central Asia         1.15   0.3       1.45 \n 47 Liechtenstein            Europe & Central Asia         1.57  -0.120     1.45 \n 48 Sweden                   Europe & Central Asia         1.54  -0.0900    1.45 \n 49 Croatia                  Europe & Central Asia         1.39   0.0700    1.46 \n 50 Bhutan                   South Asia                    3.38  -1.92      1.46 \n 51 Turks & Caicos Islands   Latin America & Caribbean     2.41  -0.943     1.46 \n 52 Belgium                  Europe & Central Asia         1.67  -0.2       1.47 \n 53 Dominica                 Latin America & Caribbean     2.35  -0.862     1.48 \n 54 Bosnia & Herzegovina     Europe & Central Asia         1.28   0.204     1.49 \n 55 Grenada                  Latin America & Caribbean     2.44  -0.954     1.49 \n 56 Slovakia                 Europe & Central Asia         1.3    0.19      1.49 \n 57 Argentina                Latin America & Caribbean     2.59  -1.09      1.5  \n 58 Australia                East Asia & Pacific           1.76  -0.256     1.5  \n 59 Denmark                  Europe & Central Asia         1.77  -0.27      1.5  \n 60 Ireland                  Europe & Central Asia         1.89  -0.39      1.5  \n 61 North Macedonia          Europe & Central Asia         1.86  -0.363     1.5  \n 62 French Polynesia         East Asia & Pacific           2.60  -1.10      1.50 \n 63 St. Kitts & Nevis        Latin America & Caribbean     2.13  -0.621     1.50 \n 64 Hungary                  Europe & Central Asia         1.32   0.19      1.51 \n 65 Slovenia                 Europe & Central Asia         1.26   0.25      1.51 \n 66 Turkey                   Europe & Central Asia         2.49  -0.977     1.51 \n 67 Cape Verde               Sub-Saharan Africa            3.54  -2.02      1.52 \n 68 Kuwait                   Middle East & North Africa    2.89  -1.36      1.52 \n 69 Cayman Islands           Latin America & Caribbean     1.71  -0.176     1.53 \n 70 Trinidad & Tobago        Latin America & Caribbean     1.73  -0.2       1.53 \n 71 Kosovo                   Europe & Central Asia         2.70  -1.15      1.54 \n 72 Isle of Man              Europe & Central Asia         1.87  -0.318     1.55 \n 73 Azerbaijan               Europe & Central Asia         2     -0.45      1.55 \n 74 Malaysia                 East Asia & Pacific           2.94  -1.39      1.55 \n 75 New Zealand              East Asia & Pacific           1.98  -0.42      1.56 \n 76 United Kingdom           Europe & Central Asia         1.64  -0.0800    1.56 \n 77 Maldives                 South Asia                    2.69  -1.12      1.58 \n 78 Antigua & Barbuda        Latin America & Caribbean     2.20  -0.618     1.58 \n 79 Iceland                  Europe & Central Asia         2.08  -0.49      1.59 \n 80 Aruba                    Latin America & Caribbean     1.84  -0.243     1.60 \n 81 Serbia                   Europe & Central Asia         1.48   0.130     1.61 \n 82 United States            North America                 2.06  -0.440     1.62 \n 83 Brazil                   Latin America & Caribbean     2.25  -0.628     1.62 \n 84 Colombia                 Latin America & Caribbean     2.57  -0.929     1.64 \n 85 France                   Europe & Central Asia         1.89  -0.23      1.66 \n 86 Iran                     Middle East & North Africa    2.00  -0.304     1.70 \n 87 Barbados                 Latin America & Caribbean     1.93  -0.225     1.71 \n 88 Romania                  Europe & Central Asia         1.31   0.4       1.71 \n 89 Qatar                    Middle East & North Africa    3.07  -1.34      1.73 \n 90 Moldova                  Europe & Central Asia         1.50   0.235     1.73 \n 91 Montenegro               Europe & Central Asia         1.90  -0.163     1.74 \n 92 Brunei                   East Asia & Pacific           2.40  -0.651     1.75 \n 93 Greenland                Europe & Central Asia         2.33  -0.56      1.77 \n 94 El Salvador              Latin America & Caribbean     3.08  -1.30      1.78 \n 95 St. Vincent & Grenadines Latin America & Caribbean     2.31  -0.531     1.78 \n 96 North Korea              East Asia & Pacific           1.94  -0.161     1.78 \n 97 Bulgaria                 Europe & Central Asia         1.26   0.55      1.81 \n 98 Georgia                  Europe & Central Asia         1.61   0.201     1.81 \n 99 Ecuador                  Latin America & Caribbean     3.10  -1.28      1.82 \n100 Bahrain                  Middle East & North Africa    2.80  -0.974     1.82 \n101 Tunisia                  Middle East & North Africa    2.02  -0.193     1.83 \n102 Faroe Islands            Europe & Central Asia         2.59  -0.728     1.86 \n103 Gibraltar                Europe & Central Asia         2.18  -0.297     1.89 \n104 Armenia                  Europe & Central Asia         1.3    0.6       1.9  \n105 Palau                    East Asia & Pacific           1.82   0.0880    1.91 \n106 Mexico                   Latin America & Caribbean     2.71  -0.804     1.91 \n107 Vietnam                  East Asia & Pacific           2.03  -0.115     1.91 \n108 Philippines              East Asia & Pacific           3.75  -1.84      1.92 \n109 Sri Lanka                South Asia                    2.21  -0.239     1.97 \n110 India                    South Asia                    3.35  -1.38      1.98 \n111 New Caledonia            East Asia & Pacific           2.51  -0.529     1.98 \n112 Peru                     Latin America & Caribbean     2.84  -0.865     1.98 \n113 Virgin Islands           Latin America & Caribbean     1.84   0.141     1.98 \n114 Nepal                    South Asia                    3.98  -2         1.98 \n115 Belize                   Latin America & Caribbean     3.63  -1.63      2.01 \n116 Seychelles               Sub-Saharan Africa            2.08  -0.0600    2.02 \n117 Monaco                   Europe & Central Asia         2.08   0.0280    2.11 \n118 Myanmar                  East Asia & Pacific           2.81  -0.698     2.12 \n119 Panama                   Latin America & Caribbean     2.74  -0.625     2.12 \n120 Indonesia                East Asia & Pacific           2.50  -0.376     2.13 \n121 Bangladesh               South Asia                    3.28  -1.12      2.16 \n122 South Africa             Sub-Saharan Africa            2.41  -0.197     2.22 \n123 Nicaragua                Latin America & Caribbean     3.12  -0.902     2.22 \n124 Morocco                  Middle East & North Africa    2.77  -0.544     2.23 \n125 Lebanon                  Middle East & North Africa    2.6   -0.361     2.24 \n126 Dominican Republic       Latin America & Caribbean     2.87  -0.626     2.24 \n127 Suriname                 Latin America & Caribbean     3.05  -0.797     2.25 \n128 Saudi Arabia             Middle East & North Africa    4.21  -1.93      2.28 \n129 Fiji                     East Asia & Pacific           2.99  -0.711     2.28 \n130 American Samoa           East Asia & Pacific           3.89  -1.60      2.29 \n131 Guatemala                Latin America & Caribbean     4.58  -2.27      2.31 \n132 Northern Mariana Islands East Asia & Pacific           1.64   0.708     2.35 \n133 Libya                    Middle East & North Africa    2.91  -0.551     2.36 \n134 Guyana                   Latin America & Caribbean     3.06  -0.645     2.41 \n135 Laos                     East Asia & Pacific           4.43  -2.01      2.42 \n136 Paraguay                 Latin America & Caribbean     3.51  -1.09      2.42 \n137 Honduras                 Latin America & Caribbean     4.29  -1.79      2.5  \n138 Oman                     Middle East & North Africa    3.85  -1.33      2.52 \n139 Bolivia                  Latin America & Caribbean     3.99  -1.44      2.55 \n140 Cambodia                 East Asia & Pacific           3.79  -1.21      2.58 \n141 Djibouti                 Middle East & North Africa    4.60  -1.99      2.61 \n142 Jordan                   Middle East & North Africa    3.86  -1.23      2.64 \n143 Haiti                    Latin America & Caribbean     4.39  -1.73      2.66 \n144 Turkmenistan             Europe & Central Asia         2.9   -0.21      2.69 \n145 Lesotho                  Sub-Saharan Africa            3.60  -0.911     2.69 \n146 Kyrgyzstan               Europe & Central Asia         2.4    0.300     2.7  \n147 Mongolia                 East Asia & Pacific           2.2    0.5       2.7  \n148 East Timor               East Asia & Pacific           5.93  -3.23      2.71 \n149 Syria                    Middle East & North Africa    4.07  -1.36      2.71 \n150 St. Martin               Latin America & Caribbean     2.88  -0.158     2.72 \n151 Botswana                 Sub-Saharan Africa            3.29  -0.562     2.73 \n152 Micronesia               East Asia & Pacific           4.22  -1.47      2.75 \n153 Egypt                    Middle East & North Africa    3.50  -0.755     2.75 \n154 Eswatini                 Sub-Saharan Africa            4.03  -1.28      2.75 \n155 Algeria                  Middle East & North Africa    2.59   0.176     2.77 \n156 Guam                     East Asia & Pacific           2.92  -0.134     2.78 \n157 Israel                   Middle East & North Africa    2.95  -0.100     2.85 \n158 Marshall Islands         East Asia & Pacific           4.70  -1.78      2.92 \n159 Kazakhstan               Europe & Central Asia         1.90   1.11      3.01 \n160 Tajikistan               Europe & Central Asia         3.49  -0.418     3.07 \n161 Papua New Guinea         East Asia & Pacific           4.53  -1.43      3.10 \n162 Tonga                    East Asia & Pacific           4.16  -1.03      3.13 \n163 Kiribati                 East Asia & Pacific           4.07  -0.924     3.15 \n164 Kenya                    Sub-Saharan Africa            5.14  -1.93      3.21 \n165 Tuvalu                   East Asia & Pacific           3.84  -0.633     3.21 \n166 Namibia                  Sub-Saharan Africa            3.99  -0.779     3.21 \n167 Iraq                     Middle East & North Africa    4.92  -1.67      3.25 \n168 West Bank & Gaza         Middle East & North Africa    5.46  -2.15      3.31 \n169 Nauru                    East Asia & Pacific           3.54  -0.212     3.33 \n170 Ghana                    Sub-Saharan Africa            4.83  -1.43      3.40 \n171 Uzbekistan               Europe & Central Asia         2.70   0.804     3.5  \n172 Solomon Islands          East Asia & Pacific           4.76  -1.2       3.56 \n173 Vanuatu                  East Asia & Pacific           4.54  -0.941     3.60 \n174 Pakistan                 South Asia                    5.35  -1.75      3.60 \n175 São Tomé & Principe      Sub-Saharan Africa            5.16  -1.51      3.64 \n176 Gabon                    Sub-Saharan Africa            4.47  -0.822     3.65 \n177 Malawi                   Sub-Saharan Africa            6.00  -2.35      3.65 \n178 Rwanda                   Sub-Saharan Africa            5.97  -2.27      3.70 \n179 Eritrea                  Sub-Saharan Africa            5.40  -1.68      3.71 \n180 Zimbabwe                 Sub-Saharan Africa            4.01  -0.285     3.72 \n181 Sierra Leone             Sub-Saharan Africa            6.36  -2.56      3.79 \n182 Senegal                  Sub-Saharan Africa            5.50  -1.68      3.82 \n183 Samoa                    East Asia & Pacific           4.52  -0.694     3.83 \n184 Guinea-Bissau            Sub-Saharan Africa            5.79  -1.95      3.84 \n185 South Sudan              Sub-Saharan Africa            6.80  -2.94      3.86 \n186 Comoros                  Sub-Saharan Africa            5.23  -1.35      3.88 \n187 Liberia                  Sub-Saharan Africa            5.88  -1.92      3.95 \n188 Madagascar               Sub-Saharan Africa            5.66  -1.69      3.97 \n189 Ethiopia                 Sub-Saharan Africa            6.65  -2.66      3.99 \n190 Gambia                   Sub-Saharan Africa            5.72  -1.71      4.01 \n191 Equatorial Guinea        Sub-Saharan Africa            5.83  -1.75      4.08 \n192 Zambia                   Sub-Saharan Africa            5.92  -1.82      4.10 \n193 Congo, Rep.              Sub-Saharan Africa            4.72  -0.567     4.16 \n194 Burkina Faso             Sub-Saharan Africa            6.52  -2.33      4.19 \n195 Togo                     Sub-Saharan Africa            5.16  -0.973     4.19 \n196 Guinea                   Sub-Saharan Africa            5.92  -1.70      4.22 \n197 Ivory Coast              Sub-Saharan Africa            5.77  -1.49      4.28 \n198 Uganda                   Sub-Saharan Africa            6.79  -2.50      4.28 \n199 Cameroon                 Sub-Saharan Africa            5.51  -1.19      4.32 \n200 Sudan                    Sub-Saharan Africa            5.60  -1.27      4.32 \n201 Nigeria                  Sub-Saharan Africa            6.12  -1.64      4.48 \n202 Benin                    Sub-Saharan Africa            5.94  -1.38      4.56 \n203 Yemen                    Middle East & North Africa    6.32  -1.73      4.59 \n204 Tanzania                 Sub-Saharan Africa            5.67  -1.06      4.61 \n205 Mauritania               Sub-Saharan Africa            5.46  -0.759     4.70 \n206 Mozambique               Sub-Saharan Africa            5.83  -1.07      4.76 \n207 Afghanistan              South Asia                    7.57  -2.73      4.84 \n208 Burundi                  Sub-Saharan Africa            6.87  -1.99      4.88 \n209 Angola                   Sub-Saharan Africa            6.64  -1.52      5.12 \n210 Mali                     Sub-Saharan Africa            6.89  -1.27      5.61 \n211 Central African Republic Sub-Saharan Africa            5.85   0.161     6.01 \n212 Congo, Dem. Rep.         Sub-Saharan Africa            6.70  -0.651     6.05 \n213 Niger                    Sub-Saharan Africa            7.83  -1.77      6.06 \n214 Chad                     Sub-Saharan Africa            7.25  -1.13      6.12 \n215 Somalia                  Sub-Saharan Africa            7.64  -1.51      6.13 \nfertility_change %>% slice_min(change, n = 10)# A tibble: 10 × 5\n   country      region                    rate2000 change rate2023\n   <chr>        <chr>                        <dbl>  <dbl>    <dbl>\n 1 East Timor   East Asia & Pacific           5.93  -3.23     2.71\n 2 South Sudan  Sub-Saharan Africa            6.80  -2.94     3.86\n 3 Afghanistan  South Asia                    7.57  -2.73     4.84\n 4 Ethiopia     Sub-Saharan Africa            6.65  -2.66     3.99\n 5 Sierra Leone Sub-Saharan Africa            6.36  -2.56     3.79\n 6 Uganda       Sub-Saharan Africa            6.79  -2.50     4.28\n 7 Malawi       Sub-Saharan Africa            6.00  -2.35     3.65\n 8 Burkina Faso Sub-Saharan Africa            6.52  -2.33     4.19\n 9 Guatemala    Latin America & Caribbean     4.58  -2.27     2.31\n10 Rwanda       Sub-Saharan Africa            5.97  -2.27     3.70\nfertility_change %>% slice_max(change, n = 10)# A tibble: 10 × 5\n   country                  region                rate2000 change rate2023\n   <chr>                    <chr>                    <dbl>  <dbl>    <dbl>\n 1 Kazakhstan               Europe & Central Asia     1.90  1.11      3.01\n 2 Uzbekistan               Europe & Central Asia     2.70  0.804     3.5 \n 3 Northern Mariana Islands East Asia & Pacific       1.64  0.708     2.35\n 4 Armenia                  Europe & Central Asia     1.3   0.6       1.9 \n 5 Bulgaria                 Europe & Central Asia     1.26  0.55      1.81\n 6 Mongolia                 East Asia & Pacific       2.2   0.5       2.7 \n 7 Romania                  Europe & Central Asia     1.31  0.4       1.71\n 8 Kyrgyzstan               Europe & Central Asia     2.4   0.300     2.7 \n 9 Czechia                  Europe & Central Asia     1.15  0.3       1.45\n10 Slovenia                 Europe & Central Asia     1.26  0.25      1.51\n# group by a new column called at_replacement indicating rate>=2.1\nfertility %>%\n  filter(year == max(year)) %>%\n  group_by(at_replacement = rate >= 2.1) %>%\n  summarize(n = n()) %>%\n  mutate(pct = 100 * n / sum(n))# A tibble: 2 × 3\n  at_replacement     n   pct\n  <lgl>          <int> <dbl>\n1 FALSE            117  54.2\n2 TRUE              99  45.8\n# we can repeat this grouping by region, this time using count()\n# and just showing the percentage of countries at replacement\nfertility %>%\n  filter(year == max(year)) %>%\n  count(region, at_replacement = rate >= 2.1) %>%\n  mutate(pct_at_rep = 100 * n / sum(n)) %>%\n  filter(at_replacement) %>%\n  select(-n, -at_replacement) %>%\n  arrange(-pct_at_rep)# A tibble: 6 × 2\n  region                     pct_at_rep\n  <chr>                           <dbl>\n1 Sub-Saharan Africa              20.8 \n2 East Asia & Pacific              9.26\n3 Middle East & North Africa       6.48\n4 Latin America & Caribbean        5.09\n5 Europe & Central Asia            2.78\n6 South Asia                       1.39\n# first get median rate in each region + year, then pipe into line plot\n# fct_reorder2() is used to reorder legend to be same order as end of lines\n# see https://forcats.tidyverse.org/reference/fct_reorder.html for more details\nfertility %>%\n  group_by(region, year) %>%\n  summarize(median_rate = median(rate)) %>%\n  ggplot(aes(x = year, y = median_rate,\n             linetype = fct_reorder2(region, year, median_rate),\n             color = fct_reorder2(region, year, median_rate))) +\n  geom_hline(yintercept = 2.1, linetype = \"dotted\") + geom_line(linewidth = 1) +\n  labs(x = \"Year\", y = \"Median fertility rate\", linetype = \"Region\", color = \"Region\",\n       title = \"Median fertility by region from 1960-2022\",\n       subtitle = \"(black dotted line at replacement rate of 2.1)\") +\n  scale_x_continuous(expand = 0, breaks = seq(1960, 2022, 10),\n                     minor_breaks = seq(1960, 2022, 2)) +\n  scale_y_continuous(expand = 0, limits = c(1.2, 7.2),\n                     breaks = 2:7, minor_breaks = seq(1.2, 7.2, .2))\n# first get median rate in each income group + year, then pipe into line plot\nfertility %>%\n  group_by(income_group, year) %>%\n  summarize(median_rate = median(rate)) %>%\n  ggplot(aes(x = year, y = median_rate,\n             linetype = income_group, color = income_group)) +\n  geom_hline(yintercept = 2.1, linetype = \"dotted\") + geom_line(linewidth = 1) +\n  labs(x = \"Year\", y = \"Median fertility rate\",\n       linetype = \"Income group\", color = \"Income group\",\n       title = \"Median fertility by income group from 1960-2022\",\n       subtitle = \"(black dotted line at replacement rate of 2.1)\") +\n  scale_x_continuous(expand = 0, breaks = seq(1960, 2022, 10),\n                     minor_breaks = seq(1960, 2022, 2)) +\n  scale_y_continuous(expand = 0, limits = c(1.4, 7.2),\n                     breaks = 2:7, minor_breaks = seq(1.6, 7.2, .2))"},{"path":"advanced-operations.html","id":"bonus-choropleth","chapter":"8 Advanced Operations","heading":"8.1.3.1 Bonus: choropleth","text":"’s bonus plotly choropleth just fun (need learn ). can change year pan/zoom see specific country. Red color indicates countries replacement rate 2.1, white indicates replacement rate, blue indicates replacement rate.","code":"\nlibrary(plotly)\na <- min(fertility$rate)\nb <- max(fertility$rate)\nplot_ly(fertility, type = \"choropleth\", locations = ~code, z = ~rate,\n        text = ~country, frame = ~year, zmin = a, zmax = b, colorscale =\n          list(c(0, \"red\"), c((2.1-a)/(b-a), \"white\"), c(1, \"blue\"))) %>%\n  layout(margin = list(l = 10, r = 10, b = 10, t = 35),\n         geo = list(resolution = 50, projection = list(type = \"robinson\"))) %>%\n  animation_opts(frame = 100)"},{"path":"advanced-operations.html","id":"merging","chapter":"8 Advanced Operations","heading":"8.2 Merging","text":"Let’s move another topic: merging data frames. Often, information need may spread across several data frames different samples different sources, case may need merge data frames together.generally 2 common ways data may need merged: binding rows, joining columns.Binding rows done two data frames share columns, different rows, want combine rows together.Joining columns done two data frames share rows, different columns, want combine columns together.two , ’s important remember .","code":""},{"path":"advanced-operations.html","id":"binding","chapter":"8 Advanced Operations","heading":"8.2.1 Binding","text":"Binding simpler two, let’s start . Returning briefly penguins data set, note ’s year column indicates sample collected. means 3 different studies conducted. Suppose years originally data frame (suspect likely true).can see 3 data frames penguins2007, penguins2008, penguins2009 share exact columns (number columns, column names, column types) different rows. data frame can thought containing fraction overall samples subjects. case, must row-bind 3 data frames together.function bind_rows(df1, df2, ...) lets us . Just pass data frame needs binding :may hard see, since first 10 rows printed , look row numbers, can see now 103+113+117=333 rows binding.best results, ensure input data frames exact columns, names types!","code":"\npenguins2007 <- penguins %>% filter(year == 2007)\npenguins2008 <- penguins %>% filter(year == 2008)\npenguins2009 <- penguins %>% filter(year == 2009)\nprint(penguins2007, n = 5)# A tibble: 103 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n  <chr>   <chr>              <dbl>         <dbl>             <dbl>       <dbl> <chr> \n1 Adelie  Torgersen           39.1          18.7               181        3750 male  \n2 Adelie  Torgersen           39.5          17.4               186        3800 female\n3 Adelie  Torgersen           40.3          18                 195        3250 female\n4 Adelie  Torgersen           36.7          19.3               193        3450 female\n5 Adelie  Torgersen           39.3          20.6               190        3650 male  \n# ℹ 98 more rows\n# ℹ 1 more variable: year <dbl>\nprint(penguins2008, n = 5)# A tibble: 113 × 8\n  species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n  <chr>   <chr>           <dbl>         <dbl>             <dbl>       <dbl> <chr> \n1 Adelie  Biscoe           39.6          17.7               186        3500 female\n2 Adelie  Biscoe           40.1          18.9               188        4300 male  \n3 Adelie  Biscoe           35            17.9               190        3450 female\n4 Adelie  Biscoe           42            19.5               200        4050 male  \n5 Adelie  Biscoe           34.5          18.1               187        2900 female\n# ℹ 108 more rows\n# ℹ 1 more variable: year <dbl>\nprint(penguins2009, n = 5)# A tibble: 117 × 8\n  species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n  <chr>   <chr>           <dbl>         <dbl>             <dbl>       <dbl> <chr> \n1 Adelie  Biscoe           35            17.9               192        3725 female\n2 Adelie  Biscoe           41            20                 203        4725 male  \n3 Adelie  Biscoe           37.7          16                 183        3075 female\n4 Adelie  Biscoe           37.8          20                 190        4250 male  \n5 Adelie  Biscoe           37.9          18.6               193        2925 female\n# ℹ 112 more rows\n# ℹ 1 more variable: year <dbl>\npenguins_bound <- bind_rows(penguins2007, penguins2008, penguins2009)\npenguins_bound# A tibble: 333 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex  \n   <chr>   <chr>              <dbl>         <dbl>             <dbl>       <dbl> <chr>\n 1 Adelie  Torgersen           39.1          18.7               181        3750 male \n 2 Adelie  Torgersen           39.5          17.4               186        3800 fema…\n 3 Adelie  Torgersen           40.3          18                 195        3250 fema…\n 4 Adelie  Torgersen           36.7          19.3               193        3450 fema…\n 5 Adelie  Torgersen           39.3          20.6               190        3650 male \n 6 Adelie  Torgersen           38.9          17.8               181        3625 fema…\n 7 Adelie  Torgersen           39.2          19.6               195        4675 male \n 8 Adelie  Torgersen           41.1          17.6               182        3200 fema…\n 9 Adelie  Torgersen           38.6          21.2               191        3800 male \n10 Adelie  Torgersen           34.6          21.1               198        4400 male \n# ℹ 323 more rows\n# ℹ 1 more variable: year <dbl>"},{"path":"advanced-operations.html","id":"joining","chapter":"8 Advanced Operations","heading":"8.2.2 Joining","text":"Joining slightly complicated. rows together, columns need spread several data frames. Often, occurs obtain data different sources want combine look patterns, can also happen sometimes multiple data frames gathered source.Joining performed first matching rows using set key variables adding additional columns. key variables uniquely identify rows. ’s simple demo start. Suppose following data frames:Suppose want join x y using column “key”, .e. “” column uniquely matches rows across data frames. 4 different joins can use: inner_join(), full_join(), left_join(), right_join() , depending rows want result:inner_join(x, y) matches rows using key variable(s), returns rows x y,full_join(x, y) matches rows using key variable(s), returns rows either x y,left_join(x, y) matches rows using key variable(s), returns rows x,right_join(x, y) matches rows using key variable(s), returns rows y.demos join. Note join exact columns; difference rows included output. Also note NAs automatically used fill missing values arise merge.4 called “mutating joins” since mutate (.e. change) data frame adding additional columns. 2 less commonly used joins called “filterting joins” called semi_join() anti_join() actually add columns; used filter rows match don’t match another data frame. Examples:examples 1 key column, works 2 key columns; rows matched values key columns.joining, R automatically look columns name use key columns. Therefore, run smoothly, ’s highly recommended ensure following joining:Key columns name type.non-key columns different names.picking join, carefully consider rows actually need later.Generally, key values unique row. need duplicate key values, watch joins closely ensure join works way want.Let’s see real example joins action.","code":"\nx <- tibble(\n  A = c(\"a\", \"b\", \"c\"),\n  B = c(1, 2, 3)\n)\ny <- tibble(\n  A = c(\"a\", \"b\", \"d\"),\n  C = ymd(\"2024.1.1\") + 0:2\n)\nx# A tibble: 3 × 2\n  A         B\n  <chr> <dbl>\n1 a         1\n2 b         2\n3 c         3\ny# A tibble: 3 × 2\n  A     C         \n  <chr> <date>    \n1 a     2024-01-01\n2 b     2024-01-02\n3 d     2024-01-03\n# only return rows in both x and y\ninner_join(x, y)# A tibble: 2 × 3\n  A         B C         \n  <chr> <dbl> <date>    \n1 a         1 2024-01-01\n2 b         2 2024-01-02\n# return rows in either x or y\nfull_join(x, y)# A tibble: 4 × 3\n  A         B C         \n  <chr> <dbl> <date>    \n1 a         1 2024-01-01\n2 b         2 2024-01-02\n3 c         3 NA        \n4 d        NA 2024-01-03\n# return only rows in x\nleft_join(x, y)# A tibble: 3 × 3\n  A         B C         \n  <chr> <dbl> <date>    \n1 a         1 2024-01-01\n2 b         2 2024-01-02\n3 c         3 NA        \n# return only rows in y\nright_join(x, y)# A tibble: 3 × 3\n  A         B C         \n  <chr> <dbl> <date>    \n1 a         1 2024-01-01\n2 b         2 2024-01-02\n3 d        NA 2024-01-03\n# semi_join(x, y) will FILTER rows in x that have matches in y\n# note there are NO new columns added to x\nsemi_join(x, y)# A tibble: 2 × 2\n  A         B\n  <chr> <dbl>\n1 a         1\n2 b         2\n# conversely, anti_join() will FILTER rows in x with NO match in y\n# again, note NO new columns were added to x\nanti_join(x, y)# A tibble: 1 × 2\n  A         B\n  <chr> <dbl>\n1 c         3"},{"path":"advanced-operations.html","id":"example-cleaning-fertility","chapter":"8 Advanced Operations","heading":"8.2.3 Example: cleaning fertility","text":"World Bank fertility data set used earlier obviously didn’t start perfectly clean tidy. download CSV file data page comes zip following two (renamed) files: fertility_meta.csv fertility_raw.csv. One contains metadata country, contains actual annual fertility rates.First, several columns like \"SpecialNotes\", \"Indicator Name\", \"Indicator Code\" don’t need; can remove select(). Country names duplicated data frames, can pick one use. Let’s pick \"TableName\" since names slightly nicer (check ). rest columns probably renamed rename().can also change \"Low income\", \"Lower middle income\", … values just \"Low\", \"Lower middle\", … slightly simplify column. Shorter labels also easier work filter() plots.Note well existence several NAs region income. , verify , subregion groupings countries (e.g. AFE AFW Eastern/Southern Western/Central African countries), can simply drop rows.Now comes key step! ’re going use \"code\" columns (3-letter ISO 3166-1 country codes) key columns join two data frames together. Let’s use inner_join() keep countries appear , since want look region income group together fertility rates.Note data frames already satisfy recommended conditions, .e. key columns name type uniquely identify country, columns different names, issues.already looks pretty good! data frames joined nicely along key \"code\" column, now region, income, fertility data 1 single data frame instead spread across 2 data frames.However, may noticed still problem: annual fertility rates spread across 64 columns, year column. compatible tidyverse functions. fix ?","code":"\nfertility_meta <- read_csv(\n  \"https://bwu62.github.io/stat240-revamp/data/fertility_meta.csv\")\nfertility_raw <- read_csv(\n  \"https://bwu62.github.io/stat240-revamp/data/fertility_raw.csv\")\nfertility_meta# A tibble: 265 × 5\n   `Country Code` Region                     IncomeGroup       SpecialNotes TableName\n   <chr>          <chr>                      <chr>             <chr>        <chr>    \n 1 ABW            Latin America & Caribbean  High income        <NA>        Aruba    \n 2 AFE            <NA>                       <NA>              \"26 countri… Africa E…\n 3 AFG            South Asia                 Low income        \"The report… Afghanis…\n 4 AFW            <NA>                       <NA>              \"22 countri… Africa W…\n 5 AGO            Sub-Saharan Africa         Lower middle inc… \"The World … Angola   \n 6 ALB            Europe & Central Asia      Upper middle inc…  <NA>        Albania  \n 7 AND            Europe & Central Asia      High income        <NA>        Andorra  \n 8 ARB            <NA>                       <NA>              \"Arab World… Arab Wor…\n 9 ARE            Middle East & North Africa High income        <NA>        United A…\n10 ARG            Latin America & Caribbean  Upper middle inc… \"The World … Argentina\n# ℹ 255 more rows\nfertility_raw# A tibble: 266 × 68\n   `Country Name`      `Country Code` `Indicator Name` `Indicator Code` `1960` `1961`\n   <chr>               <chr>          <chr>            <chr>             <dbl>  <dbl>\n 1 Aruba               ABW            Fertility rate,… SP.DYN.TFRT.IN     4.57   4.42\n 2 Africa Eastern and… AFE            Fertility rate,… SP.DYN.TFRT.IN     6.65   6.67\n 3 Afghanistan         AFG            Fertility rate,… SP.DYN.TFRT.IN     7.28   7.28\n 4 Africa Western and… AFW            Fertility rate,… SP.DYN.TFRT.IN     6.47   6.48\n 5 Angola              AGO            Fertility rate,… SP.DYN.TFRT.IN     6.71   6.79\n 6 Albania             ALB            Fertility rate,… SP.DYN.TFRT.IN     6.38   6.27\n 7 Andorra             AND            Fertility rate,… SP.DYN.TFRT.IN     2.54   2.54\n 8 Arab World          ARB            Fertility rate,… SP.DYN.TFRT.IN     6.92   6.97\n 9 United Arab Emirat… ARE            Fertility rate,… SP.DYN.TFRT.IN     6.50   6.49\n10 Argentina           ARG            Fertility rate,… SP.DYN.TFRT.IN     3.14   3.12\n# ℹ 256 more rows\n# ℹ 62 more variables: `1962` <dbl>, `1963` <dbl>, `1964` <dbl>, `1965` <dbl>,\n#   `1966` <dbl>, `1967` <dbl>, `1968` <dbl>, `1969` <dbl>, `1970` <dbl>,\n#   `1971` <dbl>, `1972` <dbl>, `1973` <dbl>, `1974` <dbl>, `1975` <dbl>,\n#   `1976` <dbl>, `1977` <dbl>, `1978` <dbl>, `1979` <dbl>, `1980` <dbl>,\n#   `1981` <dbl>, `1982` <dbl>, `1983` <dbl>, `1984` <dbl>, `1985` <dbl>,\n#   `1986` <dbl>, `1987` <dbl>, `1988` <dbl>, `1989` <dbl>, `1990` <dbl>, …\n# apply the processing steps above\nfertility_meta2 <- fertility_meta %>%\n  rename(code = \"Country Code\", country = \"TableName\", region = \"Region\", income_group = \"IncomeGroup\") %>%\n  select(code, country, region, income_group) %>%\n  mutate(income_group = sub(\" income\", \"\", income_group)) %>%\n  filter(!is.na(income_group))\nfertility_raw2 <- fertility_raw %>%\n  select(2, \"1960\":last_col()) %>%\n  rename(code = \"Country Code\")\nprint(fertility_meta2, n = 5)# A tibble: 216 × 4\n  code  country     region                    income_group\n  <chr> <chr>       <chr>                     <chr>       \n1 ABW   Aruba       Latin America & Caribbean High        \n2 AFG   Afghanistan South Asia                Low         \n3 AGO   Angola      Sub-Saharan Africa        Lower middle\n4 ALB   Albania     Europe & Central Asia     Upper middle\n5 AND   Andorra     Europe & Central Asia     High        \n# ℹ 211 more rows\nprint(fertility_raw2, n = 5)# A tibble: 266 × 65\n  code  `1960` `1961` `1962` `1963` `1964` `1965` `1966` `1967` `1968` `1969` `1970`\n  <chr>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>\n1 ABW     4.57   4.42   4.26   4.11   3.94   3.80   3.62   3.45   3.28   3.11   2.97\n2 AFE     6.65   6.67   6.69   6.71   6.72   6.74   6.77   6.78   6.78   6.78   6.79\n3 AFG     7.28   7.28   7.29   7.30   7.30   7.30   7.32   7.34   7.36   7.39   7.4 \n4 AFW     6.47   6.48   6.49   6.50   6.52   6.53   6.56   6.58   6.61   6.63   6.66\n5 AGO     6.71   6.79   6.87   6.95   7.04   7.12   7.19   7.27   7.33   7.39   7.43\n# ℹ 261 more rows\n# ℹ 53 more variables: `1971` <dbl>, `1972` <dbl>, `1973` <dbl>, `1974` <dbl>,\n#   `1975` <dbl>, `1976` <dbl>, `1977` <dbl>, `1978` <dbl>, `1979` <dbl>,\n#   `1980` <dbl>, `1981` <dbl>, `1982` <dbl>, `1983` <dbl>, `1984` <dbl>,\n#   `1985` <dbl>, `1986` <dbl>, `1987` <dbl>, `1988` <dbl>, `1989` <dbl>,\n#   `1990` <dbl>, `1991` <dbl>, `1992` <dbl>, `1993` <dbl>, `1994` <dbl>,\n#   `1995` <dbl>, `1996` <dbl>, `1997` <dbl>, `1998` <dbl>, `1999` <dbl>, …\nfertility_joined <- inner_join(fertility_meta2, fertility_raw2)\nfertility_joined# A tibble: 216 × 68\n   code  country region income_group `1960` `1961` `1962` `1963` `1964` `1965` `1966`\n   <chr> <chr>   <chr>  <chr>         <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>\n 1 ABW   Aruba   Latin… High           4.57   4.42   4.26   4.11   3.94   3.80   3.62\n 2 AFG   Afghan… South… Low            7.28   7.28   7.29   7.30   7.30   7.30   7.32\n 3 AGO   Angola  Sub-S… Lower middle   6.71   6.79   6.87   6.95   7.04   7.12   7.19\n 4 ALB   Albania Europ… Upper middle   6.38   6.27   6.11   5.93   5.71   5.47   5.32\n 5 AND   Andorra Europ… High           2.54   2.54   2.55   2.60   2.69   2.72   2.76\n 6 ARE   United… Middl… High           6.50   6.49   6.49   6.50   6.49   6.50   6.50\n 7 ARG   Argent… Latin… Upper middle   3.14   3.12   3.13   3.11   3.08   3.00   2.97\n 8 ARM   Armenia Europ… Upper middle   4.79   4.67   4.52   4.34   4.15   3.99   3.83\n 9 ASM   Americ… East … High           6.58   6.58   6.60   6.69   6.78   6.73   6.62\n10 ATG   Antigu… Latin… High           4.60   4.56   4.55   4.54   4.48   4.42   4.32\n# ℹ 206 more rows\n# ℹ 57 more variables: `1967` <dbl>, `1968` <dbl>, `1969` <dbl>, `1970` <dbl>,\n#   `1971` <dbl>, `1972` <dbl>, `1973` <dbl>, `1974` <dbl>, `1975` <dbl>,\n#   `1976` <dbl>, `1977` <dbl>, `1978` <dbl>, `1979` <dbl>, `1980` <dbl>,\n#   `1981` <dbl>, `1982` <dbl>, `1983` <dbl>, `1984` <dbl>, `1985` <dbl>,\n#   `1986` <dbl>, `1987` <dbl>, `1988` <dbl>, `1989` <dbl>, `1990` <dbl>,\n#   `1991` <dbl>, `1992` <dbl>, `1993` <dbl>, `1994` <dbl>, `1995` <dbl>, …"},{"path":"advanced-operations.html","id":"pivoting","chapter":"8 Advanced Operations","heading":"8.3 Pivoting","text":"pivoting becomes relevant. Pivoting (also sometimes called reshaping) refers process transforming data different representations . Let’s start small example better illustrate point.Suppose two people Alice Bob playing game. agree play 3 rounds. round, person tries score points maximum 10. person wins rounds wins overall. Suppose final scores:However, data can also represented alternative format:Despite difference shape dimensions, data contained data frames identical. often called long wide representations data, named one tends longer (.e. rows) wider (.e. columns). Generally, long formats easier use R/tidyverse, whereas wide formats easier humans read, course exceptions everything.Usually, data considered “tidy” needs longer format. Tidy data data satisfies following:variable column; column variable.observation row; row observation.value cell; cell single value.33Data format generally easier work using tidyverse, broadly speaking R, since format highly versatile functions designed work best format.Hopefully, easy see scores_long satisfies properties. variable indeed column (vice versa), row observation (vv.), etc. also evident scores_wide tidy, since row actually contains 2 scores, column person, variable (distinction subtle meaningful).Tidyr’s pivot_longer() pivot_wider() functions let easily convert two representations. used correctly, two inverses , .e. can take data frame pivot longer, pivot wider (vice versa) get back original data frame. many ways using functions (see help page) ’s brief guide common important usage:df %>% pivot_longer(cols, names_to = \"..\", values_to = \"..\") turns “wider” representation “longer” representation.\ncols set columns actual observations “spread ”. can set using numbers, names, ranges, selector functions (just like select()). scores_wide data frame, two columns Alice Bob since points round stored.\nnames_to sets name new column containing names cols. scores_wide data frame, filled repetitions \"Alice\" \"Bob\" corresponding round’s points.\nvalues_to sets name new column containing values inside cols. scores_wide data frame, filled actual points scored round.\n\n# example:\nscores_wide %>%\n  pivot_longer(Alice:Bob, names_to = \"player\", values_to = \"points\")\n# tibble: 6 × 3\n  round player points\n  <int> <chr>   <dbl>\n1     1 Alice       9\n2     1 Bob         4\n3     2 Alice       7\n4     2 Bob         1\n5     3 Alice       2\n6     3 Bob         7cols set columns actual observations “spread ”. can set using numbers, names, ranges, selector functions (just like select()). scores_wide data frame, two columns Alice Bob since points round stored.names_to sets name new column containing names cols. scores_wide data frame, filled repetitions \"Alice\" \"Bob\" corresponding round’s points.values_to sets name new column containing values inside cols. scores_wide data frame, filled actual points scored round.df %>% pivot_wider(names_from = \"..\", values_from = \"..\") turns “longer” represetation “wider” representation.\nnames_from sets column names become column wider data frame. scores_long data frame, player column.\nvalues_from sets column actual observations fill newly created wider set columns. scores_long data frame, points column.\n\n# example:\nscores_long %>%\n  pivot_wider(names_from = \"player\", values_from = \"points\")\n# tibble: 3 × 3\n  round Alice   Bob\n  <int> <dbl> <dbl>\n1     1     9     4\n2     2     7     1\n3     3     2     7names_from sets column names become column wider data frame. scores_long data frame, player column.values_from sets column actual observations fill newly created wider set columns. scores_long data frame, points column.Note examples, passing scores_wide pivot_longer() gives us scores_long exactly, passing scores_long pivot_wider() gives us scores_wide exactly.","code":"\n# create demo scores data frame\n# sample() is used here to randomly generate points\nscores_long <- tibble(\n  round  = rep(1:3, each = 2),\n  player = rep(c(\"Alice\", \"Bob\"), 3),\n  points = c(9, 4, 7, 1, 2, 7)\n)\nscores_long# A tibble: 6 × 3\n  round player points\n  <int> <chr>   <dbl>\n1     1 Alice       9\n2     1 Bob         4\n3     2 Alice       7\n4     2 Bob         1\n5     3 Alice       2\n6     3 Bob         7\nscores_wide <- tibble(\n  round = 1:3,\n  Alice = c(9, 7, 2),\n  Bob   = c(4, 1, 7)\n)\nscores_wide# A tibble: 3 × 3\n  round Alice   Bob\n  <int> <dbl> <dbl>\n1     1     9     4\n2     2     7     1\n3     3     2     7\n# example:\nscores_wide %>%\n  pivot_longer(Alice:Bob, names_to = \"player\", values_to = \"points\")# A tibble: 6 × 3\n  round player points\n  <int> <chr>   <dbl>\n1     1 Alice       9\n2     1 Bob         4\n3     2 Alice       7\n4     2 Bob         1\n5     3 Alice       2\n6     3 Bob         7\n# example:\nscores_long %>%\n  pivot_wider(names_from = \"player\", values_from = \"points\")# A tibble: 3 × 3\n  round Alice   Bob\n  <int> <dbl> <dbl>\n1     1     9     4\n2     2     7     1\n3     3     2     7"},{"path":"advanced-operations.html","id":"example-finish-cleaning-fertility","chapter":"8 Advanced Operations","heading":"8.3.1 Example: finish cleaning fertility","text":"Let’s turn back fertility_joined example. joining, left following data frame:Now clear need apply pivot_longer():fertility data set now 100% fully cleaned ready exploration, visualization, modeling!move , let’s demonstrate one usage pivot_wider(). function also great making human-friendly summary tables, especially looking 3 variables interrelate. example, suppose want look , region, percentage countries income group. just compute percentages print, get something like :great plotting ’s wanted , e.g:However, can also see numeric values laid table using pivot_wider() put income_group column:NAs due lack countries region/income combination. case, can fill missing values setting values_fill argument:","code":"\nfertility_joined# A tibble: 216 × 68\n   code  country region income_group `1960` `1961` `1962` `1963` `1964` `1965` `1966`\n   <chr> <chr>   <chr>  <chr>         <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>\n 1 ABW   Aruba   Latin… High           4.57   4.42   4.26   4.11   3.94   3.80   3.62\n 2 AFG   Afghan… South… Low            7.28   7.28   7.29   7.30   7.30   7.30   7.32\n 3 AGO   Angola  Sub-S… Lower middle   6.71   6.79   6.87   6.95   7.04   7.12   7.19\n 4 ALB   Albania Europ… Upper middle   6.38   6.27   6.11   5.93   5.71   5.47   5.32\n 5 AND   Andorra Europ… High           2.54   2.54   2.55   2.60   2.69   2.72   2.76\n 6 ARE   United… Middl… High           6.50   6.49   6.49   6.50   6.49   6.50   6.50\n 7 ARG   Argent… Latin… Upper middle   3.14   3.12   3.13   3.11   3.08   3.00   2.97\n 8 ARM   Armenia Europ… Upper middle   4.79   4.67   4.52   4.34   4.15   3.99   3.83\n 9 ASM   Americ… East … High           6.58   6.58   6.60   6.69   6.78   6.73   6.62\n10 ATG   Antigu… Latin… High           4.60   4.56   4.55   4.54   4.48   4.42   4.32\n# ℹ 206 more rows\n# ℹ 57 more variables: `1967` <dbl>, `1968` <dbl>, `1969` <dbl>, `1970` <dbl>,\n#   `1971` <dbl>, `1972` <dbl>, `1973` <dbl>, `1974` <dbl>, `1975` <dbl>,\n#   `1976` <dbl>, `1977` <dbl>, `1978` <dbl>, `1979` <dbl>, `1980` <dbl>,\n#   `1981` <dbl>, `1982` <dbl>, `1983` <dbl>, `1984` <dbl>, `1985` <dbl>,\n#   `1986` <dbl>, `1987` <dbl>, `1988` <dbl>, `1989` <dbl>, `1990` <dbl>,\n#   `1991` <dbl>, `1992` <dbl>, `1993` <dbl>, `1994` <dbl>, `1995` <dbl>, …\n# let's pivot_longer(), then drop any rows with missing values\n# after our careful processing, it should be safe to just do drop_na()\n# we can also again convert income_group to ordered categories if needed,\n# and finally arrange by country and year for extra neatness\nfertility <- fertility_joined %>%\n  pivot_longer(\"1960\":last_col(), names_to = \"year\", values_to = \"rate\") %>%\n  drop_na() %>%\n  mutate(income_group = factor(income_group, ordered = TRUE, levels = c(\n    \"Low\", \"Lower middle\", \"Upper middle\", \"High\"))) %>%\n  arrange(country, year)\n# print our final, clean & tidy data frame!\nfertility# A tibble: 13,792 × 6\n   code  country     region     income_group year   rate\n   <chr> <chr>       <chr>      <ord>        <chr> <dbl>\n 1 AFG   Afghanistan South Asia Low          1960   7.28\n 2 AFG   Afghanistan South Asia Low          1961   7.28\n 3 AFG   Afghanistan South Asia Low          1962   7.29\n 4 AFG   Afghanistan South Asia Low          1963   7.30\n 5 AFG   Afghanistan South Asia Low          1964   7.30\n 6 AFG   Afghanistan South Asia Low          1965   7.30\n 7 AFG   Afghanistan South Asia Low          1966   7.32\n 8 AFG   Afghanistan South Asia Low          1967   7.34\n 9 AFG   Afghanistan South Asia Low          1968   7.36\n10 AFG   Afghanistan South Asia Low          1969   7.39\n# ℹ 13,782 more rows\n# count how many countries in each region + income group,\n# regroup by region to compute percentages,\n# remove the n count column, and print\nregion_income_pct <- fertility %>%\n  filter(year == max(year)) %>%\n  count(region, income_group) %>%\n  group_by(region) %>%\n  mutate(pct = round(100 * n / sum(n), 1)) %>%\n  select(-n) %>%\n  print()# A tibble: 22 × 3\n# Groups:   region [7]\n   region                    income_group   pct\n   <chr>                     <ord>        <dbl>\n 1 East Asia & Pacific       Low            2.7\n 2 East Asia & Pacific       Lower middle  32.4\n 3 East Asia & Pacific       Upper middle  24.3\n 4 East Asia & Pacific       High          40.5\n 5 Europe & Central Asia     Lower middle   5.2\n 6 Europe & Central Asia     Upper middle  25.9\n 7 Europe & Central Asia     High          69  \n 8 Latin America & Caribbean Lower middle   9.8\n 9 Latin America & Caribbean Upper middle  46.3\n10 Latin America & Caribbean High          43.9\n# ℹ 12 more rows\n# plot income group percentages by region\n# str_wrap used to line-break long region names\nregion_income_pct %>% \n  ggplot(aes(x = str_wrap(region, 12), y = pct, fill = income_group)) +\n  geom_col() + labs(x = NULL, y = \"Percent\", fill = \"Income level\",\n                    title = \"% countries in each income level by region\")\n# pivot region/income percentages to wider table format\nregion_income_pct %>%\n  pivot_wider(names_from = income_group, values_from = pct)# A tibble: 7 × 5\n# Groups:   region [7]\n  region                       Low `Lower middle` `Upper middle`  High\n  <chr>                      <dbl>          <dbl>          <dbl> <dbl>\n1 East Asia & Pacific          2.7           32.4           24.3  40.5\n2 Europe & Central Asia       NA              5.2           25.9  69  \n3 Latin America & Caribbean   NA              9.8           46.3  43.9\n4 Middle East & North Africa   9.5           33.3           19    38.1\n5 North America               NA             NA             NA   100  \n6 South Asia                  12.5           75             12.5  NA  \n7 Sub-Saharan Africa          45.8           39.6           12.5   2.1\n# fill in missing values with 0 in region/income table\nregion_income_pct %>%\n  pivot_wider(names_from = income_group, values_from = pct, values_fill = 0)# A tibble: 7 × 5\n# Groups:   region [7]\n  region                       Low `Lower middle` `Upper middle`  High\n  <chr>                      <dbl>          <dbl>          <dbl> <dbl>\n1 East Asia & Pacific          2.7           32.4           24.3  40.5\n2 Europe & Central Asia        0              5.2           25.9  69  \n3 Latin America & Caribbean    0              9.8           46.3  43.9\n4 Middle East & North Africa   9.5           33.3           19    38.1\n5 North America                0              0              0   100  \n6 South Asia                  12.5           75             12.5   0  \n7 Sub-Saharan Africa          45.8           39.6           12.5   2.1"},{"path":"data-modeling.html","id":"data-modeling","chapter":"Data Modeling","heading":"Data Modeling","text":"next section, discuss start modeling cleaned data. loosely broken several sections.start discussing big picture view modeling works learn key conceptual topics like populations vs samples, probability vs statistics, parameters vs estimates, etc., cover foundational probability principles necessary understanding later modeling steps, discrete continuous random variables, distributions, PDF/PMFs vs CDFs, expectation variance, .Next, learn arguably two important versatile random variables: normal & binomial, form basis two general categories models discuss later (means vs proportions infereence).Finally, conclude introducing basic framework hypothesis tests & confidence intervals, explore apply following section.","code":""},{"path":"overview.html","id":"overview","chapter":"9 Overview","heading":"9 Overview","text":"want start modeling section following overview role modeling plays “life cycle” data science project.Roughly speaking, data science can divided 3 phases:Phase , identify research question, design experiment, gather sample, collect raw data. steps correspond first row “life cycle”.Phase II, start raw data clean (probably 50-90% time actually spent), explore , figure best way model . second row.Phase III, fine tune model, double check work, interpret results, may involve reporting estimates, computing tests/intervals, making predictions, etc. third row.Usually, iterative process; start question, gather data, analyze , either modify initial inquiry ask follow question, cycle continues.Experiment design & sampling advanced topic, thus covered detail STAT 240, however summarize key ideas Phase next subsection since relevant later topics.’ve spent lot time learning basics data cleaning, exploration, visualization, ’re reasonably well covered Phase II now, though course ’re always encouraged explore .remainder class, focus Phase III: identifying appropriate models, fitting well, interpreting results meaningfully, producing useful inference hypothesis tests & confidence intervals, communicating results effectively broader audience.First, need briefly summarize key concepts relating experiment design greatly enrich later exploration models.","code":""},{"path":"overview.html","id":"population-vs-sample","chapter":"9 Overview","heading":"9.1 Population vs sample","text":"Statistics primarily science studying samples understand populations. Generally, ’s impractical observe every member population, luckily usually necessary well-drawn sample sufficient answer questions.population can large group want learn , e.g. US mothers (~85 million), arctic terns (~3 million), gen-5 Toyota Priuses (~15000), etc..sample smaller set drawn (intended represent) population.MANY ways draw sample, pros, cons, potential biases. detailed discussion reserved advanced courses.","code":""},{"path":"overview.html","id":"model-vs-data","chapter":"9 Overview","heading":"9.2 Model vs data","text":"Using mathematical logic, can derive different theoretical probability models certain parameters aim represent real world phenomena, compare real data, .e. fitting, evaluate performance make inferences /predictions.model idealized mathematical representation process, e.g. normal distribution may used model distribution human heights.Models often parameters values can adjusted (“tuned”) fit model matches real data.distinction model vs data may seem obvious, often analogous features often easily confused one another. example, model can theoretical statistics values mean, median, variance, skew, etc. related different sample statistics mean, median, variance, skew.Recall statistic can computed numeric summary dataset (hypothethical model). values discussed chapter 5 examples statistics.sample suffciently large high quality, model well chosen, values match, .e. model’s predicted statistics agree sample statistics, problems along process can result discrepancies.","code":""},{"path":"intro-to-probability.html","id":"intro-to-probability","chapter":"10 Intro to Probability","heading":"10 Intro to Probability","text":"section, ’ll introduce foundational probability theory necessary later models. way semi-rigorous, emphasis teaching materials intuitive fashion.","code":""},{"path":"intro-to-probability.html","id":"random-variables-rvs","chapter":"10 Intro to Probability","heading":"10.1 Random variables (RVs)","text":"Suppose experiment produces outcome time ’s observed. modeled random variable, often denoted capital letter, e.g. \\(X\\) \\(Y\\). set possible outcomes called sample space often denoted \\(\\Omega\\).Sets possible outcomes called events. event probability associated . probability event often denoted \\(\\p(\\text{event})\\).distribution specification outcomes associated probabilities random variable.Let \\(X\\) result rolling standard 6-sided die fair, .e. outcomes 1, 2, 3, 4, 5, 6 equal probability. examples events corresponding probabilities:Probability getting 1: \\(\\p(X=1)=\\frac16\\)Probability getting 4: \\(\\p(X>4)=\\frac13\\)Probability getting even number: \\(\\p(X=2,4,\\text{}6)=\\frac12\\)Probability getting 7: \\(\\p(X=7)=0\\)","code":""},{"path":"intro-to-probability.html","id":"axioms-of-probability","chapter":"10 Intro to Probability","heading":"10.2 Axioms of probability","text":"math, axioms basic rules formally define object assumed true without proof. form basis everything else rests. axioms probability:probability event always non-negative.\nMathematically, \\(\\p(E)\\ge0\\) event \\(E\\) random variable.\nMathematically, \\(\\p(E)\\ge0\\) event \\(E\\) random variable.probability entire sample space always 1.\nMathematically, \\(\\p(\\Omega)=1\\) random variable. Note 1 equivalent 100%.\nMathematically, \\(\\p(\\Omega)=1\\) random variable. Note 1 equivalent 100%.probability union mutually exclusive events equal sum probabilities event.\nMathematically, \\(\\cap B\\) empty, \\(\\p(\\cup B)=\\p()+\\p(B)\\)\nMathematically, \\(\\cap B\\) empty, \\(\\p(\\cup B)=\\p()+\\p(B)\\)Let \\(\\) \\(B\\) two events random variable.union \\(\\) \\(B\\), denoted \\(\\cup B\\), event observing \\(\\) \\(B\\).intersection \\(\\) \\(B\\), denoted \\(\\cap B\\), event observing \\(\\) \\(B\\).\\(\\), \\(B\\) called mutually exclusive don’t intersect, .e. outcomes common.Let’s see example. Let \\(X\\) result rolling fair, 6-sided die outcomes \\(1,2,\\ldots,6\\).Let \\(\\) event observing \\(X\\) 4, let \\(B\\) event observing \\(X\\) even number. , \\(\\cap B=\\{6\\}\\) \\(\\cup B=\\{2,4,5,6\\}\\). Note \\(1,3\\) neither \\(\\) \\(B\\).Since \\(\\cap B=\\{6\\}\\) empty, \\(\\) \\(B\\) mutually exclusive. Suppose define third event \\(C\\) observing \\(X\\) either \\(1\\) \\(3\\). , \\(C\\) mutually exclusive \\(\\) \\(B\\), since \\(\\cap C\\) \\(B\\cap C\\) empty.","code":""},{"path":"intro-to-probability.html","id":"corollaries","chapter":"10 Intro to Probability","heading":"10.2.1 Corollaries","text":"axioms, important corollaries (.e. derived statements) also true:Probabilities always 0 1.\nMathematically, event \\(E\\), \\(0\\le \\p(E)\\le1\\)\nMathematically, event \\(E\\), \\(0\\le \\p(E)\\le1\\)get probability “opposite” event, subtract 1.\nMathematically, event \\(E\\), \\(\\p(\\text{}E)=1-\\p(E)\\).\nMathematically, event \\(E\\), \\(\\p(\\text{}E)=1-\\p(E)\\).general, \\(,B\\), probability \\(\\) \\(B\\) probability \\(\\) plus \\(B\\) minus intersection \\(\\) \\(B\\). generalized form 3rd axiom.\nMathematically, \\(\\p(\\cup B)=\\p()+\\p(B)-\\p(\\cap B)\\).\nMathematically, \\(\\p(\\cup B)=\\p()+\\p(B)-\\p(\\cap B)\\).difficult derive axioms, omit proofs brevity.34Here’s example use axioms. Suppose Nice Town, average day, ’s 70% chance ’s sunny, 40% chance light breeze. Suppose ’s 20% chance neither sunny breezy. ’s probability ’s sunny breezy?Let \\(S\\) represent sunny, \\(B\\) represent breezy. , information given, know \\(\\p(S)=0.7\\), \\(\\p(B)=0.4\\), \\(\\p(\\text{neither \\(S\\) \\(B\\)})=0.2\\).corollary 2, \\(\\p(\\text{neither \\(S\\) \\(B\\)})=1-\\p(S\\cup B)\\), \\(\\p(S\\cup B)=0.8\\).corollary 3, \\(\\p(S\\cup B)=\\p(S)+\\p(B)-\\p(S\\cap B)\\). Rearranging terms, get \\(\\p(S\\cap B)=\\p(S)+\\p(B)-\\p(S\\cup B)=0.7+0.4-0.8=0.3\\). Thus, ’s 30% chance sunny breezy.","code":""},{"path":"intro-to-probability.html","id":"discrete-vs-continuous-rvs","chapter":"10 Intro to Probability","heading":"10.3 Discrete vs Continuous RVs","text":"Generally, random variables either discrete continuous.discrete RV one whose outcomes can listed one--one. list allowed infinite.35A continuous RV opposite, outcomes continuous range listable.practice, usually use discrete RVs model integer valued outcomes, e.g. counts something; continuous RVs model real number valued outcomes, e.g. lengths/weights/durations. require slightly different mathematical notations/treatments.","code":""},{"path":"intro-to-probability.html","id":"discrete-pmfs","chapter":"10 Intro to Probability","heading":"10.3.1 Discrete (PMFs)","text":"discrete RVs, distribution outcomes probabilities specified probability mass function (PMF). Note PMF must satisfy rules probability. particular, probabilities must \\([0,1]\\), \\(\\p(\\Omega)=\\sum_\\text{k}\\p(k)=1\\) \\(k\\) represents possible outcome.Let \\(X\\) discrete RV. probability mass function (PMF) \\(X\\) function \\(P\\) , possible outcome \\(k\\) sample space, specifies probability \\(X\\) observed \\(k\\), denoted \\(\\p(X\\! =\\!k)\\), sometimes \\(\\p(k)\\) short.valid PMF, \\(P\\) must satisfy probability axioms, namely must always non-negative sum 1 across possible outcomes sample space.PMFs can specified using either table, function, plot.Let \\(X\\) number dollars win new casino game ’s 4 possible outcomes: either win nothing 40% chance, win $1 30% chance, win $2 20% chance, win $3 10% chance.First, ’s easy see axioms satisfied, since probabilities \\([0,1]\\) \\(0.1+0.2+0.3+0.4=1\\). Thus, valid PMF. can specify PMF following equivalent ways:\\[\\p(X\\! =\\!k)=\\begin{cases}\\frac1{10}\\big(4-k\\big) & k=0,1,2,3 \\\\ 0 & \\text{otherwise}\\end{cases}\\]another example, let \\(X\\) sum rolling 2 ordinary, fair 6-sided dice (independently)36. PMF \\(X\\)?First, note possible outcomes \\(k\\) sample space integers \\(k=2,3,\\ldots,12\\). Next, since dice fair, can find probability outcome \\(k\\) counting number combinations add \\(k\\). example, \\(k=5\\) outcomes 14, 23, 32, 41. outcome probability 1/36 summing get \\(\\p(X=5)=4\\cdot\\frac1{36}=\\frac19\\), can show probability \\(k=2,3,\\ldots,12\\) \\(\\p(X=k)=(6-|k-7|)/36\\). Thus can write PMF :\\[\\p(X\\! =\\!k)=\\begin{cases}\\frac1{36}\\big(6-|k-7|\\big)&k=2,3,\\ldots,12\\\\0&\\text{otherwise}\\end{cases}\\]can easily check PMF satisfies probability axioms. ’s plot PMF:","code":" k | p(k)\n---+-----\n 0 | 0.4 \n 1 | 0.3 \n 2 | 0.2 \n 3 | 0.1 \n# remember to import tidyverse (and optionally, update theme options)\ntibble(k = 0:3, p = (4:1)/10) %>%\n  ggplot(aes(x = k, y = p)) + geom_col() +\n  labs(title = \"Distribution of X (winnings from made-up casino game)\")\ntibble(k = 2:12, p = (6-abs(k-7))/36) %>%\n  ggplot(aes(x = k, y = p)) + geom_col() +\n  labs(title = \"Distribution of X (sum of rolling two fair 6-sided dice)\") +\n  scale_x_continuous(breaks = 2:12) +\n  scale_y_continuous(breaks = seq(0,.2,.02))"},{"path":"intro-to-probability.html","id":"table","chapter":"10 Intro to Probability","heading":"Table:","text":"","code":" k | p(k)\n---+-----\n 0 | 0.4 \n 1 | 0.3 \n 2 | 0.2 \n 3 | 0.1 "},{"path":"intro-to-probability.html","id":"function","chapter":"10 Intro to Probability","heading":"Function:","text":"\\[\\p(X\\! =\\!k)=\\begin{cases}\\frac1{10}\\big(4-k\\big) & k=0,1,2,3 \\\\ 0 & \\text{otherwise}\\end{cases}\\]","code":""},{"path":"intro-to-probability.html","id":"plot","chapter":"10 Intro to Probability","heading":"Plot:","text":"","code":"\n# remember to import tidyverse (and optionally, update theme options)\ntibble(k = 0:3, p = (4:1)/10) %>%\n  ggplot(aes(x = k, y = p)) + geom_col() +\n  labs(title = \"Distribution of X (winnings from made-up casino game)\")"},{"path":"intro-to-probability.html","id":"continuous-pdfs","chapter":"10 Intro to Probability","heading":"10.3.2 Continuous (PDFs)","text":"continuous RVs, distributions specified probability density function (PDF). similar PMFs key distinction: PDFs output probability outcome, rather denotes “density” can thought rate change probability.Let \\(X\\) continuous RV. probability density function (PDF) \\(X\\) function \\(P\\) , outcome \\(x\\) sample space, specifies density probability around \\(x\\).valid PDF, \\(P\\) must also satisfy probability axioms, .e. \\(P\\) must always non-negative integrate 1 across sample space.important enough warrant repeating: continuous PDF output probability! continuous PDFs, probabilities ALWAYS correspond areas PDF.Also note ’s customary use \\(k\\) represent possible outcomes discrete PMFs, \\(x\\) represent possible outcomes continuous PDFs.PDFs continuous analog PMFs, whenever might use PMFs summation \\(\\sum\\) expression, switch definite integral \\(\\int\\) PDF. STAT 240, require evaluate integrals may occasionally show familiarize notation. Computations simple PMFs may asked however.PDFs can feel strange first, ’s easy example start. According 2011-12 National Health Nutrition Examination Survey (NHANES) CDC, US adult human height approximately normal distribution mean 65.8 inches standard deviation 3.98 inches. plot distribution:can easily see density non-negative, careful math area can shown 1. Thus satisfies probability axioms.Remember density \\(x\\) probability \\(x\\). , probabilities events correspond areas curve. example, one can show normal distribution:Approx. 68% outcomes ±1 standard deviation,Approx. 95% outcomes ±2 standard deviations,Approx. 99.7% outcomes ±3 standard deviations.rule called empirical rule. distribution heights, means 68% people 62 70 inches, 95% 58 74 inches, etc.another example, consider uniform distribution \\((0,1)\\). distribution generalizes rolling fair die drawing number interval, every value inside interval equally likely selected. plot distribution:One can also show percentiles always uniformly distributed, actually useful distribution (STAT 340).","code":"\n# use geom_function to plot dnorm (the normal PDF function)\n# also plot the mean ± up to 3 standard deviations\nggplot(tibble(SDs = 65.8 + (-3:3)*3.98)) +\n  geom_function(fun = \\(x) dnorm(x, 65.8, 3.98), xlim = c(52, 80), n = 1e3) +\n  geom_segment(aes(x = SDs, xend = SDs, y = 0, yend = dnorm(SDs, 65.8, 3.98)),\n               color = \"red\", linetype = \"dashed\", linewidth = 0.7) +\n  scale_x_continuous(breaks = seq(52, 80, 2), expand = 0) +\n  scale_y_continuous(breaks = seq(0, 0.10, 0.01), expand = c(0, 0.001)) +\n  labs(title = \"US adult human height distribution\",\n       subtitle = '(approx normal w/ mean 65.8\", SD 3.98\"; SDs shown in dashed)',\n       x = \"height (inches)\", y = \"probability density\")\n# use geom_function to plot dunif (the uniform PDF function)\nggplot() + geom_function(fun = dunif, xlim = c(-.5, 1.5), n = 1e4) +\n  labs(title = \"Uniform distribution between 0 and 1\",\n       x = \"x\", y = \"probability density\")"},{"path":"intro-to-probability.html","id":"expectation-variance","chapter":"10 Intro to Probability","heading":"10.4 Expectation & Variance","text":"","code":""},{"path":"intro-to-probability.html","id":"expected-value","chapter":"10 Intro to Probability","heading":"10.4.1 Expected value","text":"Expected value refers average value random variable (function thereof). discrete random variable \\(X\\), expected value \\(X\\)—also called \\(\\e(X)\\), \\(\\mu\\), simply mean \\(X\\)—defined:\\[\\mu=\\e(X)=\\sum_k\\,k\\cdot \\p(X\\! =\\!k)\\]summation performed possible outcomes \\(k\\) sample space. plain words, mean sum product outcome probability (.e. weighted sum using probabilities weights). formula continuous variable similar integral instead summation.expected value function random variable \\(f(X)\\) can also defined, represents average value \\(f(X)\\) :\\[\\e\\big(f(X)\\big)=\\sum_k\\,f(k)\\cdot \\p(X\\! =\\!k)\\], simple example, consider \\(X\\) rolling single fair, 6-sided die. Find \\(\\e(X)\\).die fair, \\(\\p(X\\! =\\!k)=1/6\\) \\(k=1,2,\\ldots,6\\). , get\\[\\e(X)=\\sum_{k=1}^6\\,k\\cdot(1/6)=\\frac16(1+2+\\cdots+6)=\\frac{21}6=3.5\\]means average value die roll 3.5. Note average need possible observation! Even though ’s impossible roll 3.5, average roll many rolls fact 3.5.Now consider \\(f(x)=x^2\\). \\(\\e\\big(f(X)\\big)\\), .e. average value \\(X^2\\)?\\[\\e(X^2)=\\sum_{k=1}^6\\,k^2\\cdot(1/6)=\\frac16(1+4+\\cdots+36)=\\frac{91}6\\approx15.17\\]Thus, average square 6-sided die 15.17.Let’s reconsider casino game example , \\(X\\) average winnings per game. Find expected value \\(X\\).Recall outcomes \\(0,1,2,3\\) corresponding probabilities \\(0.4,0.3,0.2,0.1\\). Using formula, get\\[\\e(X)=0\\!\\cdot\\!(0.4)+1\\!\\cdot\\!(0.3)+2\\!\\cdot\\!(0.2)+3\\!\\cdot\\!(0.1)=0.3+0.4+0.3=1\\]Thus, play average wins $1. means casino wants lose money, need charge least $1 per play.","code":""},{"path":"intro-to-probability.html","id":"variance","chapter":"10 Intro to Probability","heading":"10.4.2 Variance","text":"variance random variable—also called \\(\\var(X)\\) \\(\\sigma^2\\)—can thought average squared distance mean defined :\\[\\sigma^2=\\var(X)=\\e\\big((X-\\mu)^2\\big)=\\sum_k\\,(k\\!-\\!\\mu)^2\\cdot \\p(X\\! =\\!k)\\]\\(\\mu\\) represents \\(\\e(X)\\). variance used measure spread. higher variance, “spread ” RV .standard deviation \\(X\\), often denoted \\(\\sigma\\), defined square root variance. often convenient use calculations instead variance since units data. can thought average distance observation mean.Consider previous single, fair 6-sided die example. Find variance standard deviation \\(X\\).found earlier \\(\\mu=\\e(X)=3.5\\). Applying variance formula gives:\\[\\var(X)=\\sum_{k=1}^6(k\\!-\\!3.5)^2\\cdot(1/6)=\\frac16\\big((1\\!-\\!3.5)^2+\\cdots+(6\\!-\\!3.5)^2\\big)=\\frac{35}{12}\\approx2.92\\], found variance \\(\\sigma^2\\approx2.92\\), thus standard deviation \\(\\sigma\\approx1.71\\).Let’s consider casino game example final time. Find variance standard deviation \\(X\\).found earlier \\(\\mu=\\e(X)=1\\). Applying variance formula gives:\\[\\var(X)=(0\\!-\\!1)^2\\!\\cdot\\!(0.4)+(1\\!-\\!1)^2\\!\\cdot\\!(0.3)+(2\\!-\\!1)^2\\!\\cdot\\!(0.2)+(3\\!-\\!1)^2\\!\\cdot\\!(0.1)=1\\]Thus, variance also \\(\\sigma^2=1\\), standard deviation \\(\\sigma=1\\).","code":""},{"path":"intro-to-probability.html","id":"binomial-distribution","chapter":"10 Intro to Probability","heading":"10.5 Binomial distribution","text":"binomial distribution perhaps important example discrete random variable. generalizes idea sum sequence independent, repeated, binary trials, e.g. number heads series coin flips, number patients respond experimental treatment.","code":""},{"path":"intro-to-probability.html","id":"definition","chapter":"10 Intro to Probability","heading":"10.5.1 Definition","text":"Suppose \\(n\\) independent trials, \\(p\\) probability producing \\(1\\) (called “success”), \\(1-p\\) probability producing \\(0\\) (called “failure”). , sum trials \\(X\\) follows binomial distribution parameters \\(n,p\\) words \\(X\\sim\\bin(n,p)\\).Suppose flip fair coin 10 times let \\(X=\\) total number heads. can see \\(X\\sim\\bin(10,0.5)\\). shows plot distribution:following assumptions acronym BINS must hold binomial distribution apply:Binary outcomes: individual trial must produce 1 0Independence: individual trial must independent others (.e. neither affects affected trials)N fixed: number trials \\(n\\) must fixed, predetermined quantity, variable/conditionSame probability: trial must probability \\(p\\) observing 1If criteria met, resultant variable follow binomial distribution.","code":"\n# use dbinom (the binomial PMF function) to plot distribution\ntibble(k = 0:10, p = dbinom(k, 10, 0.5)) %>% \nggplot(aes(x = k, y = p)) + geom_col() +\n  scale_x_continuous(breaks = seq(0,10,1), expand = 0) +\n  scale_y_continuous(breaks = seq(0, 0.25, 0.05), limits = c(0, 0.25),\n                     minor_breaks = seq(0, 0.25, 0.01), expand = 0) +\n  labs(title = \"Binomial(10, 0.5) distribution (number of heads in 10 flips of a fair coin)\",\n       x = \"k\", y = \"probability\")"},{"path":"intro-to-probability.html","id":"binom-pmf","chapter":"10 Intro to Probability","heading":"10.5.2 PMF","text":"First, formula PMF \\(X\\sim\\bin(n,p)\\)? can shown following:\\[\\p(X\\!=\\!k)={n\\choose k}p^k(1\\!-\\!p)^{n-k}\\quad\\text{outcomes $k=0,1,\\ldots,n$}\\]\\[\\small\\text{}~~{n\\choose k}=\\dfrac{n!}{k!(n\\!-\\!k)!}~~~\\text{}~~n!=n\\!\\times\\!(n\\!-\\!1)\\!\\times\\cdots\\!\\times\\!1\\]\\(n!\\) called “\\(n\\) factorial” counts number permutations (.e. ways ordering) \\(n\\) distinct objects\nNote: \\(0!=1\\) definition\nNote: \\(0!=1\\) definition\\(n\\choose k\\) called “\\(n\\) choose \\(k\\)” counts number ways choosing \\(k\\) objects \\(n\\) distinct objects (order choice doesn’t matter)example, 5 objects, 5×4×3×2×1=120 ways order . want choose 2 5, 5 choose 2 10 different combinations possible.help understand PMF shown . \\(n\\choose k\\) counts number ways \\(k\\) successes happened \\(n\\) total trials, whereas \\(p^k\\) \\((1-p)^{n-k}\\) respectively probabilities exactly \\(k\\) successes \\(n-k\\) failures.Let \\(X\\sim\\bin(10,0.5)\\) . \\(\\p(X=4)\\), .e. probability observing exactly 4 total heads?\\[P(X=4)={10\\choose4}0.5^4(1\\!-\\!0.5)^{10-4}=\\frac{10\\cdot9\\cdot8\\cdot7}{4\\cdot3\\cdot2}\\cdot\\frac1{2^{10}}=\\frac{210}{1024}\\approx20.5\\%\\]can also use R help us compute :consistent plot PMF previously made. also ask ’s probability getting 3 fewer heads total, .e. \\(\\p(X\\le3)\\).\\[\n\\begin{aligned}\nP(X\\le3)&=P(X\\!=\\!0)+P(X\\!=\\!1)+P(X\\!=\\!2)+P(X\\!=\\!3)\\\\\n&={\\small{10\\choose0}0.5^0(1\\!-\\!0.5)^{10-0}+\\cdots+{10\\choose3}0.5^3(1\\!-\\!0.5)^{10-3}}\\\\\n&=\\frac{176}{1024}\\\\\n&\\approx17.2%\n\\end{aligned}\n\\]","code":"\n# using R, we can find 5!\nfactorial(5)[1] 120\n# as well as 5 choose 2\nchoose(5, 2)[1] 10\n# apply the formula\nchoose(10, 4) * 0.5^4 * (1-0.5)^6[1] 0.2050781\n# compute P(X≤3) with vectorization\nsum(\n  choose(10, 0:3) * 0.5^(0:3) * (1-0.5)^(10-0:3)\n)[1] 0.171875"},{"path":"intro-to-probability.html","id":"e-var","chapter":"10 Intro to Probability","heading":"10.5.3 E & Var","text":"Recall general expectation variance definitions :\\[\n\\begin{aligned}\n\\e(X)&=\\sum_k\\,k\\cdot \\p(X\\! =\\!k)\\\\\n\\var(X)&=\\sum_k\\,(k\\!-\\!\\mu)^2\\cdot \\p(X\\! =\\!k)\n\\end{aligned}\n\\]can plug binomial PMF \\(\\p(X\\! =\\!k)\\) derive following identities binomial distribution:\\[\n\\text{binomial,}\\quad\n\\begin{aligned}\n\\e(X)&=np\\\\\n\\var(X)&=np(1\\!-\\!p)\\\\\n\\sd(X)&=\\sqrt{\\var(X)}=\\sqrt{np(1\\!-\\!p)}\n\\end{aligned}\n\\]Consider \\(X\\sim\\bin(10,0.5)\\) . Compute expectation & variance using thetheir definitions check agree identities given .","code":"\n# compute expectation using definition\nsum(0:10 * dbinom(0:10, 10, 0.5))[1] 5\n# compare with identity E=np\n10 * 0.5[1] 5\n# compute variance using definition\nsum((0:10 - 5)^2 * dbinom(0:10, 10, 0.5))[1] 2.5\n# compare with identity Var=np(1-p)\n10 * 0.5 * (1-0.5)[1] 2.5"},{"path":"intro-to-probability.html","id":"binom-functions","chapter":"10 Intro to Probability","heading":"10.5.4 *binom() functions:","text":"4 main R functions working binomial distributions, dbinom(), pbinom(), qbinom(), rbinom(). go .","code":""},{"path":"intro-to-probability.html","id":"dbinom-pmf","chapter":"10 Intro to Probability","heading":"10.5.5 dbinom() (PMF)","text":"dbinom() simply implements PMF equation shown section 10.5.2. order compute \\(\\p(X=k)\\) \\(X\\sim\\bin(n,p)\\), simply dbinom(k, n, p).’s plot PMF \\(n=10\\), \\(p=0.5\\).","code":"\n# check the computation earlier for P(X=4) where X~Bin(10,0.5)\ndbinom(4, 10, 0.5)[1] 0.2050781\n# show the probability of every possible outcome\n# (this is how the PMF was plotted earlier)\ndbinom(0:10, 10, 0.5) %>% round(3) [1] 0.001 0.010 0.044 0.117 0.205 0.246 0.205 0.117 0.044 0.010 0.001\n# use dbinom (the binomial PMF function) to plot distribution\ntibble(k = 0:10, p = dbinom(k, 10, 0.5)) %>% \nggplot(aes(x = k, y = p)) + geom_col() +\n  scale_x_continuous(breaks = seq(0, 10, 1), expand = 0) +\n  scale_y_continuous(breaks = seq(0, 0.25, 0.05), limits = c(0, 0.25),\n                     minor_breaks = seq(0, 0.25, 0.01), expand = 0) +\n  labs(title = \"Binomial(10, 0.5) PMF\", x = \"k\", y = \"probability\")"},{"path":"intro-to-probability.html","id":"pbinom-cdf","chapter":"10 Intro to Probability","heading":"10.5.6 pbinom() (CDF)","text":"pbinom() ’s called binomial’s cumulative distribution function CDF, finds probability getting less equal given input. find \\(\\p(X\\le k)\\) \\(X\\sim\\bin(n,p)\\), can pbinom(k, n, p).Note 1-pbinom(k, n, p) can also give \\(\\p(X>k)\\).’s plot binomial CDF \\(n=10\\), \\(p=0.5\\). Note stepwise nature function, since discrete distribution cumulative probabilities increase discrete “steps” possible values \\(k\\).Also note CDF always goes 0 left 1 right, can never decrease move left right; can increase stay .37","code":"\n# check the computation earlier for P(X≤3) where X~Bin(10,0.5)\npbinom(3, 10, 0.5)[1] 0.171875\n# another way\nsum(dbinom(0:3, 10, 0.5))[1] 0.171875\n# example: find P(X>7) where X~Bin(10,0.5)\n1 - pbinom(7, 10, 0.5)[1] 0.0546875\nggplot() + geom_function(fun = \\(x) pbinom(x, 10, 0.5), xlim = c(0,10), n = 1e4) +\n  scale_x_continuous(breaks = seq(0, 10, 1), expand = 0) +\n  scale_y_continuous(breaks = seq(0, 1, 0.1), expand = c(0, 0.005)) +\n  labs(title = \"Binomial(10, 0.5) CDF\", x = \"k\", y = \"probability\")"},{"path":"intro-to-probability.html","id":"qbinom-inverse-cdf","chapter":"10 Intro to Probability","heading":"10.5.7 qbinom() (inverse CDF)","text":"qbinom() inverse function CDF, .e. opposite. given probability \\(p\\), finds smallest \\(k\\) \\(\\p(X\\le k)\\ge p\\). ’s also called quantile function since used compute observation \\(k\\) given percentile \\(p\\).’s plot binomial inverse CDF \\(n=10\\), \\(p=0.5\\), note ’s reflection CDF across \\(y=x\\) diagonal line. discrete distributions, like binomial inverse CDF also stepwise function.","code":"\n# what observation is at the 17.1%-ile of X~Bin(10,0.5)?\nqbinom(0.171, 10, 0.5)[1] 3\n# what if we \"cross\" the value of P(X≤3) and ask for the 17.2%-ile?\nqbinom(0.172, 10, 0.5)[1] 4\nggplot() + geom_function(fun = \\(x) qbinom(x, 10, 0.5), xlim = c(0,1), n = 1e4) +\n  scale_x_continuous(breaks = seq(0, 1, 0.1), expand = c(0, 0.005)) +\n  scale_y_continuous(breaks = seq(0, 10, 1), expand = 0) +\n  labs(title = \"Binomial(10, 0.5) inverse CDF\", x = \"probability\", y = \"k\")"},{"path":"intro-to-probability.html","id":"rbinom-random-generator","chapter":"10 Intro to Probability","heading":"10.5.8 rbinom() (random generator)","text":"rbinom() random generator function, allows simulate drawing random samples binomial distribution. can useful simulation studies, empirical computations, etc.","code":"\n# draw a sample of size 100 from Bin(10,0.5)\nsamp <- rbinom(100, 10, 0.5)\nsamp  [1] 4 4 5 7 4 7 7 6 6 3 4 4 6 5 6 5 6 9 5 6 7 4 6 3 4 5 2 5 7 4 5 5 5 4 6 6 6 3 6 5\n [41] 6 6 6 5 5 6 2 5 6 6 5 7 5 4 3 3 4 5 6 5 7 4 5 4 6 4 5 6 3 7 4 7 4 4 5 7 7 5 6 8\n [81] 5 6 5 4 6 4 6 3 4 3 4 3 6 7 6 6 5 5 6 5\n# what are the mean and SD in our sample?\n# these should be close to the theoretical E & SD\nmean(samp)[1] 5.14\nsd(samp)[1] 1.325888\n# a histogram of our sample\nggplot(tibble(samp), aes(x = samp)) +\n  geom_histogram(binwidth = 1, center = 0, color = \"white\") +\n  scale_x_continuous(breaks = seq(0, 10, 1), expand = 0) +\n  scale_y_continuous(breaks = seq(0, 30, 5), minor_breaks = 0:30, expand = 0) +\n  labs(title = \"Histogram of 100 observations sampled from Bin(10,0.5)\", x = \"k\")"},{"path":"intro-to-probability.html","id":"normal-distribution","chapter":"10 Intro to Probability","heading":"10.6 Normal distribution","text":"normal distribution definitely important continuous distributions, perhaps important statistics. ’s named “normal” due ubiquitous throughout nature, showing everywhere look, hence ’s “normal” distribution encounter.specific circumstances give rise normal distribution something called Central Limit Theorem revisit later, extremely oversimplified summary, normal distributions often appear observation can viewed sum average many smaller, independent processes.","code":""},{"path":"intro-to-probability.html","id":"definition-pdf","chapter":"10 Intro to Probability","heading":"10.6.1 Definition & PDF","text":"normal distribution parametrized (.e. specified) two parameters: mean \\(\\mu\\) standard deviation \\(\\sigma\\), named mean SD population. \\(X\\) follows distribution, also write \\(X\\sim\\n(\\mu,\\sigma)\\).Given parameters, PDF normal given following equation:\\[f(x) = \\frac1{\\sigma\\sqrt{2\\pi}}\\,e^{-\\frac12\\left(\\frac{x-\\mu}\\sigma\\right)^2}\\]expression produces familiarly shaped “bell curve” distribution know love. example, IQ scores standardized mean 100 SD 15 general population. shown corresponding normal curve distribution:Recall earlier since continuous random variable, PDF plot probability, rather probability density! Probabilities events instead always interpreted corresponding areas curve!","code":"\nSDs <- seq(55, 145, 15)\nggplot() + geom_segment(aes(x = SDs, y = 0, yend = dnorm(SDs, 100, 15)),\n                        color = rep(c(\"red\", \"blue\", \"red\"), times = c(3, 1, 3))) + \n  geom_function(fun = \\(x) dnorm(x, 100, 15), n = 1e3, xlim = c(50, 150)) +\n  scale_x_continuous(breaks = seq(55, 145, 15), expand = 0, minor_breaks = NULL) +\n  scale_y_continuous(expand = c(0.004, 0)) +\n  labs(title = \"N(100, 15) distribution (standardized IQ scores)\",\n       subtitle = \"(mean shown in blue, SDs shown in red)\",\n       x = \"score\", y = \"probability density\")"},{"path":"intro-to-probability.html","id":"empirical-rule","chapter":"10 Intro to Probability","heading":"10.6.2 Empirical rule","text":"normal distributions, can shown :68% values ±1 SD mean,95% values ±2 SDs mean,99.7% values ±3 SDs mean.called empirical rule normal useful roughly approximating ’d observe based parameters.Using \\(X\\sim\\n(100,15)\\) model IQ scores, approximately percent people expect score following ranges?85 11570 13085 130Less 70Greater 145Using empirical rule, know 68% fall 85 115 (±1 SD), 95% fall 70 130 (±2 SDs).Using symmetry, know 85-100 68%/2 34%, 100-130 95%/2 47.5%. Thus, 85-130 covers 81.5% distribution.know 70-130 covers 95% empirical rule, means <70 >130 must 5%. symmetry, <70 must half 2.5%.using empirical rule, know 55 145 covers 99.7%, means <55 >145 makes 0.3%. symmetry , know >145 must around 0.15%.","code":""},{"path":"intro-to-probability.html","id":"e-var-1","chapter":"10 Intro to Probability","heading":"10.6.3 E & Var","text":"’d expect definition, expectation variance equal \\(\\mu\\) \\(\\sigma^2\\).\\[\n\\text{normal,}\\quad\n\\begin{aligned}\n\\e(X)&=\\mu\\\\\n\\var(X)&=\\sigma^2\\\\\n\\sd(X)&=\\sqrt{\\var(X)}=\\sigma\n\\end{aligned}\n\\]","code":""},{"path":"intro-to-probability.html","id":"norm-functions","chapter":"10 Intro to Probability","heading":"10.6.4 *norm() functions:","text":"also 4 main R functions working normal distributions, dnorm(), pnorm(), qnorm(), rnorm(). continuous analogs *binom() functions.","code":""},{"path":"intro-to-probability.html","id":"dnorm-pdf","chapter":"10 Intro to Probability","heading":"10.6.5 dnorm() (PDF)","text":"dnorm() normal PDF (probability density function), plotted just . Remember PDF represent probability certain outcome, rather probability density, akin rate change probability around point. us, main use case dnorm() show plot PDF.","code":"\n# what's the density at the mean of N(100,15)?\ndnorm(100, 100, 15)[1] 0.02659615\n# replot the N(100,15) PDF\nggplot() + geom_function(fun = \\(x) dnorm(x, 100, 15), xlim = c(50, 150)) +\n    scale_x_continuous(breaks = seq(55, 145, 15), minor_breaks = NULL) +\n  labs(title = \"N(100, 15) PDF\", x = \"x\", y = \"probability density\")"},{"path":"intro-to-probability.html","id":"pnorm-cdf","chapter":"10 Intro to Probability","heading":"10.6.6 pnorm() (CDF)","text":"pnorm() normal CDF (cumulative distribution function), finds probability less equal given input. function extremely useful computing probabilities ranges normal curve, e.g. compute \\(\\p(X\\le k)\\) \\(X\\sim\\n(\\mu,\\sigma)\\), simply pnorm(k, µ, σ).Note find probability \\(\\p(<X\\le b)\\), can pnorm(, µ, σ) - pnorm(b, µ, σ). Together remebering area entire curve 1 just like RV, can check probabilities range previous example:Due way probabilities work continuous variables (’re always areas ranges curve), can show probability observing single number38 probability 0. specifically, number \\(\\), \\(\\p(X=)=0\\).may seem strange, try imagine area single number \\(\\) “bounds” curve, makes line, area. mean \\(\\) impossible observe outcome39, just can’t meaningfully assign probability due way probabilities defined continuous RVs.consequence continuous variables, \\(<\\) \\(\\le\\) interchangeable, well \\(>\\) \\(\\ge\\). words, don’t need worry counting counting “boundaries” ranges math probabilities different intervals.plot normal CDF \\(\\mu=100\\), \\(\\sigma=15\\). note move left right, CDF goes 0 1 never decreases.","code":"\nc(  # find the probability an observation from X~N(100,15) falls in each range:\n  pnorm(115, 100, 15) - pnorm(85, 100, 15),   # between 85-115\n  pnorm(130, 100, 15) - pnorm(70, 100, 15),   # between 70-130\n  pnorm(130, 100, 15) - pnorm(85, 100, 15),   # between 85-130\n  pnorm(70, 100, 15),                         # less than 70\n  1 - pnorm(145, 100, 15)                     # greater than 145\n) %>% round(4)[1] 0.6827 0.9545 0.8186 0.0228 0.0013\nggplot() + geom_function(fun = \\(x) pnorm(x, 100, 15), xlim = c(55, 145), n = 1e3) + \n  scale_x_continuous(breaks = seq(55, 145, 15), minor_breaks = NULL, expand = 0) + \n  scale_y_continuous(breaks = seq(0, 1, 0.2), expand = c(0, 0.002)) + \n  labs(title = \"N(100, 15) CDF\", x = \"x\", y = \"probability\")"},{"path":"intro-to-probability.html","id":"qnorm-inverse-cdf","chapter":"10 Intro to Probability","heading":"10.6.7 qnorm() (inverse CDF)","text":"qnorm() inverse CDF quantile function normal. Since normal distribution continuous (unlike binomial), inverse CDF works little simpler. given probability \\(p\\), finds \\(k\\) \\(\\p(X\\le k)=p\\).plot normal inverse CDF \\(\\mu=100\\), \\(\\sigma=15\\), note ’s reflection CDF across \\(y=x\\) diagonal line.","code":"\n# what observation is at the 90%-ile of X~N(100,15)?\nqnorm(0.9, 100, 15)[1] 119.2233\n# we can also check that P(X≤119.2233) = 90% (up to rounding)\npnorm(119.2233, 100, 15)[1] 0.9000003\nggplot() + geom_function(fun = \\(x) qnorm(x, 100, 15), xlim = c(0, 1), n = 1e3) + \n  scale_x_continuous(breaks = seq(0, 1, 0.2), expand = c(0, 0.002)) + \n  scale_y_continuous(breaks = seq(55, 145, 15), minor_breaks = NULL, expand = 0) + \n  labs(title = \"N(100, 15) inverse CDF\", x = \"probability\", y = \"x\")"},{"path":"intro-to-probability.html","id":"rnorm-random-generator","chapter":"10 Intro to Probability","heading":"10.6.8 rnorm() (random generator)","text":"rnorm() normal random generator function, simulates drawing random samples normal distribution.","code":"\n# draw a sample of size 80 from N(100,15)\nsamp <- rnorm(80, 100, 15)\nsamp [1] 105.97159  90.81960 105.11680  83.05955 121.49536 129.70600  94.49168  84.33798\n [9] 108.54579  97.97418 136.02427  99.41140 110.34609 100.42003  88.85090 102.83188\n[17]  72.92562 121.98332 102.29880 132.58918 107.13264  89.35080 109.16090  85.98854\n[25]  81.19550 104.37169  93.35062 100.01658 101.11512  91.15719  91.46997  97.97232\n[33] 117.67130  77.14650 108.90919 104.99426 115.94650  95.43724 105.55028 104.00648\n[41]  91.86220 118.11802 117.40604 110.50320 123.80250 108.37730  80.85112  91.40102\n[49]  81.63081  92.89899  90.69450 100.63174  86.33618 102.37043  90.18123 126.50931\n[57] 110.75061 113.65261 105.76278 125.23264  90.46395  93.07533 121.48423  90.23955\n[65]  96.88929  94.10788  95.20011  95.81330 107.41282  97.34004  92.41064 120.14558\n[73]  96.78131  97.30665  98.49714 110.68999  98.89653  99.43549  89.77509  95.13595\n# again, we can check our sample mean/sd are close to theory\nmean(samp)[1] 101.5152\nsd(samp)[1] 13.28363\n# a histogram of our sample\nggplot(tibble(samp), aes(x = samp)) +\n  geom_histogram(binwidth = 5, boundary = 0, color = \"white\") +\n  scale_x_continuous(expand = 0) + scale_y_continuous(expand = 0) +\n  labs(title = \"Histogram of 80 observations sampled from N(100,15)\", x = \"x\")"},{"path":"intro-to-probability.html","id":"aside-normality-qq-plots","chapter":"10 Intro to Probability","heading":"10.6.9 Aside: Normality & QQ-plots","text":"Sometimes may wish evalute normality given sample, .e. see normal distribution good “fit” data. common easy way make ’s called quantile-quantile plot QQ plot. Basically, plot quantiles sample vs theoretical normal quantiles40. one plots prefer make using base R, since ’s significantly easier.interpretation also simple: linear QQ plot, normal sample. smaller samples, course expect variation larger samples. Often can even tell way sample non-normal, see post -depth discussion read QQ plots.","code":"\n# pch=20 is the dumb base R way of using dots as point markers,\n# see https://r-charts.com/base-r/pch-symbols for more info\nqqnorm(samp,pch=20)"},{"path":"intro-to-probability.html","id":"standard-normal-n01","chapter":"10 Intro to Probability","heading":"10.7 Standard Normal \\(\\n(0,1)\\)","text":"important member normal family standard normal, usually denoted \\(Z\\) mean \\(\\mu=0\\) SD \\(\\sigma=1\\). words, \\(Z\\sim\\n(0,1)\\).standard normal important serves reference distribution, normal distributions can compared. turns every normal distribution similar , .e. shape, axes scaling. example, plot , 3 distributions appear different shapes:However, take exact plot simply zoom curve, can see fact exact shape, axes scaling.consequence problem involving arbitrary normal distribution \\(X\\sim\\n(\\mu,\\sigma)\\) can turned equivalent problem standard normal \\(Z\\sim\\n(0,1)\\) simply applying transformation converts \\(X\\) \\(Z\\). called standardization, computing Z-score.","code":"\nnormals_plot <- ggplot() + geom_function(fun = dnorm, n = 1e3, color = \"red3\") + \n  geom_function(fun = \\(x) dnorm(x, -3, 0.5), n = 1e3, color = \"seagreen\") + \n  geom_function(fun = \\(x) dnorm(x, 5, 2), n = 1e3, color = \"navy\") + \n  scale_x_continuous(breaks = -5:10, limits = c(-5, 10), expand = 0) + \n  scale_y_continuous(expand = 0) + \n  labs(title = \"Plot of 3 normals: N(0,1), N(-3,0.5), N(5,2)\",\n       y = \"probability density\")\nnormals_plot\np1 <- normals_plot + xlim(-4.5, -1.5) + ylim(0, dnorm(-3, -3, 0.5)) + \n  labs(title = \"Same plot, zoomed to N(-3,0.5)\", y = NULL, x = NULL)\np2 <- normals_plot + xlim(-3, 3) + ylim(0, dnorm(0, 0, 1)) + \n  labs(title = \"Same plot, zoomed to N(0,1)\", y = NULL, x = NULL)\np3 <- normals_plot + xlim(-1, 11) + ylim(0, dnorm(5, 5, 2)) + \n  labs(title = \"Same plot, zoomed to N(5,2)\", y = NULL, x = NULL)\ngridExtra::grid.arrange(p1, p2, p3, nrow = 1)"},{"path":"intro-to-probability.html","id":"z-score","chapter":"10 Intro to Probability","heading":"10.7.1 Z-score","text":"Suppose draw observation \\(x\\) normal population mean \\(\\mu\\) SD \\(\\sigma\\). Z-score \\(x\\) defined :\\[z=\\frac{x-\\mu}\\sigma\\], solving problem using \\(z\\) standard normal equivalent solving original problem.Continuing IQ example, suppose ’re asked find percent population IQ score less 115. words, want find \\(\\p(X<115)\\) \\(X\\sim\\n(100,15)\\). can following:Using Z-score approach, \\[z=\\frac{x-\\mu}\\sigma=\\frac{115-100}{15}=1\\]Thus, problem equivalent asking ’s \\(\\p(Z<1)\\) \\(Z\\sim\\n(0,1)\\) standard normal:Note two areas exactly correspond, solving one solves .Since R, won’t need rely Z-scores much, however ’s important concept normal curves, also shows later discussing inference, e.g. test statistics hypothesis testing.","code":"\n# use pnorm() to get area to the left of 115\npnorm(115, 100, 15)[1] 0.8413447\n# showing this area on a quick plot\nggplot() + geom_function(fun = \\(x) dnorm(x, 100, 15), xlim = c(55, 145)) + \n  stat_function(fun = \\(x) dnorm(x, 100, 15), xlim = c(55, 115),\n                geom = \"area\", fill = \"red4\") + \n  labs(title = \"Plot showing P(X<115) where X~N(100,15)\")\n# pnorm() defaults to mean 0 SD 1, i.e. standard normal, if not specified\npnorm(1)[1] 0.8413447\n# showing the same equivalent area on a plot\nggplot() + geom_function(fun = dnorm, xlim = c(-3, 3)) + \n  stat_function(fun = dnorm, xlim = c(-3, 1), geom = \"area\", fill = \"red4\") + \n  labs(title = \"Plot showing P(Z<1) where Z~N(0,1)\")"},{"path":"intro-to-probability.html","id":"normal-approx.-of-binomial","chapter":"10 Intro to Probability","heading":"10.8 Normal approx. of binomial","text":"One topic chapter end; certain circumstances, binomial distribution well-approximated normal. computers, extremely useful since areas normal curve much easier find normal table summing many bars binomial ditribution, especially large \\(n\\). days, ’s still useful teaching exercise well example central limit theorem.Usually, condition approximation considered good enough \\(np\\) \\(n(1\\!-\\!p)\\) must \\(>5\\). Note necessarily implies \\(n>10\\), otherwise ’s impossible true.condition satisfied, approximate \\(X\\sim\\bin(n,p)\\) picking normal \\(\\mu=np\\) \\(\\sigma=\\sqrt{np(1\\!-\\!p)}\\).example, suppose \\(n=15\\), \\(p=0.4\\). One can check \\(np=6\\), \\(n(1\\!-\\!p)=9\\) approximation hold well . pick normal \\(\\mu=6\\) \\(\\sigma\\approx1.9\\) match binomial:approximation gets better better higher \\(n\\) (assuming \\(p\\) isn’t extremely close 0 1). another example, suppose \\(n=150\\), \\(p=0.75\\). obvious condition satisfied . pick normal \\(\\mu=105\\) \\(\\sigma=5.6\\) match binomial observe approximation even better fit previous example:","code":"\nn = 15 ; p = 0.4\ntibble(x = 0:15, p = dbinom(x,n,p)) %>%\n  ggplot(aes(x,p)) + geom_col(width=1, fill=NA, color=\"black\", linewidth=1) +\n  geom_function(fun = \\(x)dnorm(x,n*p,sqrt(n*p*(1-p))), color=\"red\", linewidth=1.2) +\n  labs(title = \"Approximation of Bin(15,0.4) using N(6,1.9) which has same µ,σ\")\nn = 150 ; p = 0.7\ntibble(x = 80:130, p = dbinom(x,n,p)) %>%\n  ggplot(aes(x,p)) + geom_col(width=1, fill=NA, color=\"black\", linewidth=.8) +\n  geom_function(fun = \\(x)dnorm(x,n*p,sqrt(n*p*(1-p))), color=\"red\", linewidth=1) +\n  labs(title = \"Approximation of Bin(150,0.7) using N(105,5.6) which has same µ,σ\")"},{"path":"intro-to-probability.html","id":"continuity-correction","chapter":"10 Intro to Probability","heading":"10.8.1 Continuity correction","text":", computers, normal approximation extremely useful computing areas binomial distribution. Suppose consider \\(X\\sim\\bin(15,0.4)\\) suppose ’re interested computing \\(\\p(X\\le5)\\) using normal approximation \\(X\\;\\approx\\;Y\\!\\sim\\!\\n(6,1.9)\\). words, ’re interested finding light blue area:Note order approximate light blue area well, actually find \\(\\p(Y<5.5)\\) .e. light red area:called continuity correction, adjust evaluation region normal \\(\\pm0.5\\) based ’re counting bars towards left right side (’re starting somewhere counting bars towards left, \\(+0.5\\) bound; otherwise ’re starting somewhere counting bars towards right, \\(-0.5\\) bound, e.g. \\(\\p(X\\ge5)\\), ’d find \\(\\p(Y>4.5)\\)).can easily check correction significantly improves approximation:final example, keeping \\(X,Y\\), suppose wish find \\(\\p(5\\le X\\le9)\\), .e. blue area:Applying continuity correction either side, see need \\(\\p(4.5<Y<9.5)\\), .e. red area:","code":"\nn = 15 ; p = 0.4 ; mu = n*p ; sigma = sqrt(n*p*(1-p))\ntibble(x = 0:15, p = dbinom(x,n,p), c = rep(c(F,T),c(6,10))) %>%\n  ggplot(aes(x,p)) + geom_col(aes(fill=c), width=1, color=\"black\", linewidth=1, alpha=.4) +\n  scale_fill_manual(values = c(\"blue\",\"white\")) +\n  geom_function(fun = \\(x)dnorm(x,mu,sigma), color=\"red\", linewidth=1.2) +\n  labs(title = \"P(X≤5) where X~Bin(15,0.4), i.e. probability of 5 or fewer successes\") + theme(legend.position=\"none\")\ntibble(x = 0:15, p = dbinom(x,n,p)) %>%\n  ggplot(aes(x,p)) + geom_col(width=1, fill=NA, color=\"black\", linewidth=1) +\n  geom_function(fun = \\(x)dnorm(x,mu,sigma), color=\"red\", linewidth=1.2) +\n  stat_function(fun = \\(x)dnorm(x,mu,sigma), geom=\"area\", xlim=c(0,5.5), fill=\"red\", alpha=.4) +\n  labs(title = \"Approximate P(X≤5) where X~Bin(15,0.4) with P(Y<5.5) where Y~N(6,1.9)\")\n# define parameters and mu,sigma\nn = 15 ; p = 0.4 ; mu = n*p ; sigma = sqrt(n*p*(1-p))\n# exact area P(X≤5)\npbinom(5,n,p)[1] 0.4032156\n# normal approx P(Y<5) without continuity correction (~26% error)\npnorm(5,mu,sigma)[1] 0.2990807\n# normal approx P(Y<5.5) with continuity correction (~2% error)\npnorm(5.5,mu,sigma)[1] 0.3960737\ntibble(x = 0:15, p = dbinom(x,n,p), c = rep(c(T,F,T),c(5,5,6))) %>%\n  ggplot(aes(x,p)) + geom_col(aes(fill=c), width=1, color=\"black\", linewidth=1, alpha=.4) +\n  scale_fill_manual(values = c(\"blue\",\"white\")) +\n  geom_function(fun = \\(x)dnorm(x,mu,sigma), color=\"red\", linewidth=1.2) +\n  labs(title = \"P(5≤X≤9) where X~Bin(15,0.4)\") + theme(legend.position=\"none\")\nn = 15 ; p = 0.4\ntibble(x = 0:15, p = dbinom(x,n,p)) %>%\n  ggplot(aes(x,p)) + geom_col(width=1, fill=NA, color=\"black\", linewidth=1) +\n  geom_function(fun = \\(x)dnorm(x,mu,sigma), color=\"red\", linewidth=1.2) +\n  stat_function(fun = \\(x)dnorm(x,mu,sigma), geom=\"area\", xlim=c(4.5,9.5), fill=\"red\", alpha=.4) +\n  labs(title = \"Approx. P(5≤X≤9) where X~Bin(15,0.4) with P(4.5<Y<9.5) where Y~N(6,1.9)\")\n# exact area P(5≤X≤9) = P(X≤9)-P(X≤4)\npbinom(9,n,p) - pbinom(4,n,p)[1] 0.748889\n# normal approx P(5<Y<9) without continuity correction (~14% error)\npnorm(9,mu,sigma) - pnorm(5,mu,sigma)[1] 0.6439961\n# normal approx P(4.5<Y<9.5) with continuity correction (~0.5% error)\npnorm(9.5,mu,sigma) - pnorm(4.5,mu,sigma)[1] 0.752859"},{"path":"sampling-inference.html","id":"sampling-inference","chapter":"11 Sampling & Inference","heading":"11 Sampling & Inference","text":"previous chapter, introduced basics probability theory discussed basic random variable models. form basis inference techniques rest notes.chapter ’ll discuss concept inference, use observations sample try infer underlying facts broader population. process, make extensive use probability concepts random variable models previous chapter.","code":""},{"path":"sampling-inference.html","id":"population-vs-sample-1","chapter":"11 Sampling & Inference","heading":"11.1 Population vs sample","text":"Recall idea population vs sample. Usually imagine population fixed distribution unknown constant parameters. draw sample population observe sample statistics. Based sample well possibly reasonable assumptions can make, identify potential model can applied population.sample intended model, two common methods inference can pursue; conceptually related, subtle differences:can run hypothesis test, test models data determine explains results better.can find confidence intervals underlying parameters interest model, producing range quite certain true value lies.methods mutually exclusive can certainly , problems call one method. discuss shortly, first let’s briefly discuss sampling distributions.","code":""},{"path":"sampling-inference.html","id":"sampling-distributions","chapter":"11 Sampling & Inference","heading":"11.2 Sampling distributions","text":"inference even necessary first place? main reason : samples random! may sound trivially obvious, implications worth examining closely.","code":""},{"path":"sampling-inference.html","id":"example-coin-flips","chapter":"11 Sampling & Inference","heading":"11.2.1 Example: Coin flips","text":"Imagine perfectly fair coin, .e. Heads (H) Tails (T) exact probability (can’t land edge). Also assume good throwing technique flip coin good many times air, past throws don’t affect subsequent throws, .e. throws mutually independent. course idealized model, usually turns quite practical.","code":""},{"path":"sampling-inference.html","id":"heads-v.-n","chapter":"11 Sampling & Inference","heading":"11.2.2 # Heads v. \\(n\\)","text":"Suppose flip coin 10 times define \\(X\\) number total heads. Note can model \\(X\\sim\\bin(10,0.5)\\). probably getting exactly 5? number? fact ’ve already seen exact distribution function last chapter:can see even though 5 likely outcome, ’s actually 75% chance ’d observe something else entirely.Now suppose keep going get 100 flips, consider total number heads \\(X\\sim\\bin(100,0.5)\\). ’s distribution:One thing immediately apparent: even though raw SD increased 1.58 5, range likely outcomes narrowed compared \\(\\Omega\\), .e. compared \\(n\\), ’re unlikely deviate much total number flips higher. continue 1000 flips, becomes even evident:","code":"\n# remember to import tidyverse (and optionally, update theme options)\n# also importing latex2exp to write math in plot annotations\nlibrary(latex2exp)\ntibble(k = 0:10, p = dbinom(k, 10, 0.5)) %>% \nggplot(aes(x = k, y = p)) + geom_col() +\n  scale_x_continuous(breaks = seq(0, 10, 1), expand = 0) +\n  scale_y_continuous(breaks = seq(0, 0.25, 0.05), limits = c(0, 0.25),\n                     minor_breaks = seq(0, 0.25, 0.01), expand = 0) +\n  labs(title = TeX(\"Binomial(10, 0.5) PMF    [ $\\\\mu=(10)(0.5)=5$,  $\\\\sigma\n                   =\\\\sqrt{(10)(0.5)(1-0.5)}\\\\approx 1.58$ ]\"),\n       x = \"k\", y = \"probability\")\ntibble(k = 0:100, p = dbinom(k, 100, 0.5)) %>% \nggplot(aes(x = k, y = p)) + geom_col(color = \"white\") +\n  scale_x_continuous(breaks = seq(0, 100, 10), expand = 0) +\n  scale_y_continuous(breaks = seq(0, 0.08, 0.01), limits = c(0, 0.08), expand = 0) +\n  labs(title = TeX(\"Binomial(100, 0.5) PMF    [ $\\\\mu=(100)(0.5)=50$,  $\n                   \\\\sigma=\\\\sqrt{(100)(0.5)(1-0.5)}=5$ ]\"),\n       x = \"k\", y = \"probability\")\ntibble(k = 0:1000, p = dbinom(k, 1000, 0.5)) %>% \n    ggplot(aes(x = k, y = p)) + geom_area(color=\"gray35\", fill=\"gray35\") +\n    scale_x_continuous(breaks = seq(0, 1000, 100), expand = 0) +\n    scale_y_continuous(breaks = seq(0, 0.025, 0.005), expand = 0,\n                       minor_breaks = seq(0, 0.025, 0.001)) +\n  labs(title = TeX(\"Binomial(1000, 0.5) PMF    [ $\\\\mu=(1000)(0.5)=500$,  $\n                   \\\\sigma=\\\\sqrt{(1000)(0.5)(1-0.5)}\\\\approx 15.8$ ]\"),\n       x = \"k\", y = \"probability\") + theme(plot.title = element_text(size=12),\n                                           plot.margin = margin(6,10,6,6))"},{"path":"sampling-inference.html","id":"heads-v.-n-1","chapter":"11 Sampling & Inference","heading":"11.2.3 % Heads v. \\(n\\)","text":"’ve just seen \\(n\\) increases, range values \\(X\\) (number heads) likely observe narrows respect total range 0 \\(n\\).Let’s see fact different way. Let’s actually simulate sequence 1000 flips according model, make plot showing proportion heads go along flip.Let’s increase sample size \\(n=10^{5}\\) show \\(n\\) logarithmic scale, improves readability (generally whenever column always positive covers several orders magnitude, can significantly help improve readability).Let’s make final addition plot add 2 additional runs entire experiment repeated, well 2 pairs dashed gray curves showing ±1 ±2 SD \\(n\\).Now see remarkable phenomenon: runs converge true value \\(p=0.5\\), rates convergence fall reasonably within “envelope” ’s proportional SD-vs-n curves.","code":"\n# use rbinom to explicitly generate 1000 individual flips\n# enframe wraps the vector in a data frame for ggplot\n# cumsum computes the cumulative sum, i.e. \"how many heads so far\"\nsamp <- rbinom(1000, 1, 0.5)\nsamp %>% enframe(\"n\", \"x\") %>% ggplot(aes(x = n, y = cumsum(x)/n)) +\n  geom_hline(yintercept = 0.5, color = \"blue\", linetype = \"dashed\") +\n  geom_line() + labs(title = \"Running proportion of heads vs n\", y = \"proportion\",\n                     subtitle = TeX(\"(blue dashed line shows theoretical $p=0.5$)\"))\n# extend experiment out to 1e5 flips\n# log.indices used to slice out indices evenly spaced on a log scale,\n# which significantly improves performance at no cost to visual fidelity\n# scales::comma suppresses scientific notation\nsamp <- c(samp, rbinom(1e5-length(samp), 1, 0.5))\nlog.indices <- unique(round(10^seq(0,5,length.out=1e3)))\nsamp %>% enframe(\"n\", \"x\") %>% mutate(p = cumsum(x)/n) %>% slice(log.indices) %>%\n  ggplot(aes(x = n, y = p)) +\n  geom_hline(yintercept = 0.5, color = \"blue\", linetype = \"dashed\") +\n  geom_line() + scale_x_log10(breaks = 10^(0:5), labels = scales::comma) +\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +\n  labs(title = \"Running proportion of heads vs n\", y = \"proportion\",\n       subtitle = TeX(\"(blue dashed line shows theoretical $p=0.5$)\"))\np <- tibble(n = 1:1e5, run1 = cumsum(samp)/n,\n            run2 = cumsum(rbinom(1e5, 1, 0.5))/n,\n            run3 = cumsum(rbinom(1e5, 1, 0.5))/n) %>%\n  slice(log.indices) %>%\n  pivot_longer(contains(\"run\"), names_to = \"Run\", values_to = \"x\") %>%\n  ggplot(aes(x = n, y = x, color = Run))\nfor(i in -2:2) p <- p + geom_function(fun = \\(x, i) 0.5*(1+i/sqrt(x)),\n                                      args = list(i = i), color = \"black\",\n                                      linetype = \"dashed\", alpha = 0.5)\np <- p + geom_line(linewidth = 0.7) +\n  scale_x_log10(breaks = 10^(0:5), labels = scales::comma, expand = 0) +\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 10), limits = c(0, 1), expand = .01) +\n  labs(title = \"Running proportion of heads vs n  (3 runs)\", y = \"proportion\",\n       subtitle = TeX(\"(dashed lines show theoretical $p=0.5$ and ±1, ±2 SD curves)\"))\np"},{"path":"sampling-inference.html","id":"law-of-large-numbers-lln","chapter":"11 Sampling & Inference","heading":"11.2.4 Law of large numbers (LLN)","text":"previous plots show simulated demonstration ’s called law large numbers (LLN), essentially states certain statistics guaranteed converge true values increase sample size.Specifically, LLN guarantees sample independently drawn observations, \\(n\\\\infty\\), statistic computed average expression converge true theoretical average value long exists (.e. well-defined, finite value). oversimplification course, serves purposes. Note however ’s statement rate convergence, .e. fast converges, merely ’s inevitable.Examples statistics converge LLN:\\[\n\\begin{align}\n\\text{(sample mean)}&&\\bar{x}=\\frac1n\\sum_{=1}^nx_i~~&\\longrightarrow~\\mu=\\e(X)&&\\text{(expected value)}\\\\\n\\text{(sample variance)}&&s^2=\\frac{\\sum_{=1}^n(x_i-\\bar{x})^2}{n-1}~~&\\longrightarrow~\\sigma^2=\\e\\big((X-\\mu)^2\\big)&&\\text{(variance)}\n\\end{align}\n\\]\\(x_i\\) sample independent identically distributed (iid) observations drawn population represented random variable \\(X\\).many statistics also converge (e.g. correlation, skew, etc…) two important us right now. Note sample variance also converges true variance, since can thought average squared distance mean factor \\(\\frac{n}{n-1}\\) correction factor term (long \\(\\sigma^2\\) exists).Also note proportion heads saw last example can thought average sample 1s 0s 1 denotes H 0 denotes T, also converges true probability heads LLN.LLN forms basis inference topics embark .","code":""},{"path":"sampling-inference.html","id":"confidence-intervals-ci","chapter":"11 Sampling & Inference","heading":"11.3 Confidence intervals (CI)","text":"look last plot coin flips example different way, since run stays within fairly predictable margin true probability, given sample size can use sample proportion find range reasonable values true probability. call confidence interval.confidence interval usually following form:\\[\n\\text{$C\\%$ confidence interval}~=~(\\text{estimate})~\\pm~(\\text{crit. value})\\cdot(\\text{std. error})\n\\]\\(C\\) level confidence desired. commonly, 95%-level confidence reported convention, can number 0-100% (inclusive).Estimate best guess true value based sample. example, ’re trying estimate probability heads, ’d use proportion heads sample.Critical value multiplier depends \\(C\\), level confidence. want higher confidence, must cover values. standard 95% CI, value usually approximately 2.Standard error call estimate true SD using sample. last plot, estimate far inner gray lines true probability \\(p=0.5\\).’ll cover detail compute quantities different experiments next chapters, now let’s quick visual example help build intuition.Using last plot, give approximate 95% confidence interval true probability heads \\(p\\) based just first \\(n=10\\) flips “run1” experiment.Looking vertical slice along \\(n=10\\), can see “run1” sample proportion 0.6, implies 6 first 10 flips heads. Thus, sample estimate 0.6.can also see point, 1-SD-line approximately 0.65, 0.15 away true value, implying standard error close 0.15.Using approximate critical value 2, 95% confidence interval \\(p\\) \\(0.6\\pm2\\cdot0.15=(0.3,0.9)\\).Put different way, based sample thus far (6/10 heads), 95% confident true probability heads \\(p\\) 30-90%.plot 95% CI updates run1 experiment progresses one flip time. can see given point time, usually cover true probability heads \\(p=0.5\\), expect .","code":"\n# compute and plot 95% CI as experiment progresses\n# this will be covered in detail later, but SE is computed by sqrt(p*(1-p)/n)\n# where here p refers to the running sample proportion of % of heads out of n\n# pmin & pmax are used to ensure the CI bounds for the true p don't exceed [0,1]\ntibble(n = 1:1e5, p = cumsum(samp)/n, se = sqrt(p*(1-p)/n)) %>%\n  slice(log.indices) %>%\n  ggplot(aes(x = n, y = p)) +\n  geom_ribbon(aes(ymin = pmax(0, p-2*se), ymax = pmin(1, p+2*se)), alpha = 0.2) +\n  geom_hline(yintercept = 0.5, color = \"black\", alpha = 0.5) + geom_line(aes(color = \"run1\")) +\n  scale_x_log10(breaks = 10^(0:5), labels = scales::comma, expand = 0) +\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 10), limits = c(0, 1), expand = .01) +\n  labs(title = \"95% confidence interval for p vs n for run1\", y = \"proportion\")"},{"path":"sampling-inference.html","id":"interpretation-of-ci","chapter":"11 Sampling & Inference","heading":"11.3.1 Interpretation of CI","text":"Let’s briefly examine interpretation CIs closely. mean exactly say “95% confidence”?Briefly, 95% CI means repeat exact experiment , time performing exact 95% CI calculation, average expect 95% reps result CI fact contain true \\(p=0.5\\). plot shows simulation fact.Remember real-world example, don’t know true probability heads , data single experiment, don’t know interval captures true value , can 95% confident .","code":"\nN = 20 ; n = 10  # number of repetitions / number of flips in each rep\ntibble(flips = rbinom(N*n, 1, 0.5),\n       run = rep(1:N,each=n) %>% str_pad(2,pad=\"0\") %>% paste0(\"run\",.)) %>% \n  group_by(run) %>%\n  summarize(heads = sum(flips), flips = str_flatten(ifelse(flips,\"H\",\"T\"))) %>% \n  mutate(p = heads/n, se = sqrt(p*(1-p)/n), ci1 = pmax(0,p-2*se), ci2 = pmin(1,p+2*se),\n         capture = ci1<=0.5 & 0.5<=ci2) %>% \n  ggplot(aes(y = fct_rev(paste(run,flips)), x = p, xmin = ci1, xmax = ci2, color = capture)) +\n  geom_vline(xintercept = 0.5, color = \"black\", linetype = \"dashed\", alpha = 0.5) +\n  geom_pointrange(linewidth = 0.8) + scale_color_manual(values = c(\"red\", \"black\")) +\n  theme(axis.text.y = element_text(family = \"mono\")) + labs(\n    y = NULL, title = \"Repeated 95% CIs for p vs true p=0.5\")"},{"path":"sampling-inference.html","id":"hypothesis-testing","chapter":"11 Sampling & Inference","heading":"11.4 Hypothesis testing","text":"instead estimating parameter, primary goal test certain parameter value agrees data, probably want perform hypothesis test.hypothesis test usually starts specifying pair competing hypotheses following form:\\[\n\\begin{align}\nH_0&:\\text{parameter}~\\theta=\\theta_0\\\\\nH_a&:\\text{parameter}~\\theta\\left.\\begin{cases}&<\\theta_0\\\\\\text{one }\\!\\!&\\ne\\theta_0\\\\&>\\theta_0\\end{cases}\\right\\rbrace\n\\end{align}\n\\]\\(H_0\\) called null hypothesis represents starting assumption, .e. begin assuming \\(H_0\\) true goal rejecting differs much data.\\(H_a\\) called alternative hypothesis exactly sounds like: alternative explanation settle reject null.\\(\\theta\\) simply represents parameter underlying distribution, e.g. \\(p\\) binomial, \\(\\mu\\) normal.\\(\\theta_0\\) value interest parameter \\(\\theta\\) testing data.actual decision whether reject null made computing p-value, simply probability observing dataset null actually true, .e. likeliness data \\(H_0\\). probability small enough, means null doesn’t fit data, leading rejection. threshold use p-value (.e. begin reject null) called significance level \\(\\alpha\\) commonly set 0.05 convention.summary, steps involved typical hypothesis test:Specify \\(H_0\\) \\(H_a\\), set desired significance level \\(\\alpha\\).Assuming \\(H_0\\) true, find likeliness data, .e. p-value.p-value \\(<\\alpha\\), reject \\(H_0\\) favor \\(H_a\\). Otherwise, stay \\(H_0\\).Interpret results clearly context original problem.probably feels new nebulous, let’s ground concrete example.suspect given coin weighted come heads often . decide flip 10 times observe total 7 heads. common \\(\\alpha=0.05\\), can conclude coin biased?Let \\(p\\) represent true underlying probability heads coin. null \\(H_0:p=0.5\\) (.e. coin fair), alternative \\(H_a:p>0.5\\) (.e. coin biasing heads).Next, assume null true compute p-value. Note null assumption, can model number heads \\(X\\sim\\bin(10,p\\!=\\!0.5)\\). reasons explained soon, “likeliness data” p-value actually defined \\(\\p(X\\ge7)\\). can find using pbinom().Thus, p-value 0.17, pass threshold \\(\\alpha=0.05\\), reject null. words, data strong enough conclude coin biased heads.result previous example may seem surprising, simply reflection fact 7/10 heads unusual even perfectly fair coin, data exceed threshold disbelief \\(\\alpha\\). instead 7 observed 9/10 heads, can check ’ve rejected \\(H_0\\) able conclude bias coin.interpretations previous example carefully worded. hypothesis testing, mental model always data disagree strongly enough null reject . particular, following common pitfalls hypothesis testing:choice always reject reject null, never “accept” null.reject null, mean null fact false (made incorrect rejection).Similarly reject null, mean null true (inability reject also incorrect).Failing reject null thought evidence rising high enough standard prefer alternative.p-value true probability obeys usual laws probability.","code":"\n# recall pbinom gives ≤ the input, so for ≥7, we need:\n1 - pbinom(6, 10, 0.5)[1] 0.171875\n# another way using dbinom instead:\nsum(dbinom(7:10, 10, 0.5))[1] 0.171875"},{"path":"sampling-inference.html","id":"h_a-directions","chapter":"11 Sampling & Inference","heading":"11.4.1 \\(H_a\\) directions","text":"compute p-value earlier \\(\\p(X\\ge7)\\)? One way thinking “likeliness” dataset must found comparing possible outcomes dataset ranking . p-value 0.17 earlier basically means ranks top 17% possible datasets using total number heads statistic.’s follow-question , want find “top percentage” value number heads? reason alternative \\(H_a:p>0.5\\). order disprove \\(H_0\\) support \\(H_a\\), dataset must get heads expect vs fair. instead alternative hypothesis \\(H_a:p<0.5\\), ’ve instead computed \\(\\p(X\\le7)\\), much larger value (.e. much closer 1 0).instead suspecting coin biasing heads, suspected unfair unsure direction (.e. bias heads tails)? case, ’d set \\(H_a:p\\ne0.5\\). p-value, need compute \\(\\p(X\\ge7)+\\p(X\\le3)\\). first term represents “top percentage” comparing possible outcomes even heads, second term represents situation reversed instead 7 tails 3 heads “likeliness” scenario “bottom percentage” comparing possible outcomes even fewer heads.might seem strange, ’ll make sense think test results different way.","code":""},{"path":"sampling-inference.html","id":"rejection-regions","chapter":"11 Sampling & Inference","heading":"11.4.2 Rejection regions","text":"Let’s change different example fresh perspective (aid upcoming diagrams). Suppose know certain population distributed normally \\(X\\sim\\n(\\mu,1)\\), words know standard deviation \\(\\sigma=1\\) don’t know mean \\(\\mu\\). like draw sample use test \\(\\alpha=0.05\\) null \\(H_0:\\mu=0\\) vs one possible alternatives \\(H_a:\\mu<,\\ne,\\text{}>0\\). Note \\(H_0\\), \\(X\\) standard normal.time, instead simulating specifying actual sample, let’s just broadly consider possible samples result conclusions alternative.\\(H_a:\\mu>0\\), order reject \\(H_0\\) favor \\(H_a\\), sample need sample mean \\(\\bar{x}\\) high lands top \\(\\alpha\\), top 5% possible datasets. , ’d p-value \\(<\\alpha\\) ’d reject \\(H_0\\). forms right-side one-tailed rejection region:\n\n\nggplot() + geom_function(fun=dnorm, xlim=c(-4,4)) +\n  stat_function(fun=dnorm, geom=\"area\", xlim=c(qnorm(.95),4), fill=\"red\") +\n  scale_x_continuous(\n    breaks=c(-4:4,qnorm(.95)), labels=\\(x)prettyNum(x,digits=3), expand=0) +\n  scale_y_continuous(expand=0) + labs(x=\"sample mean\", y=\"density\",\n    title=TeX(\"$H_a\\\\,:\\\\,\\\\mu>0$ one-sided rejection region ($\\\\alpha=0.05$)\"))\n\\(H_a:\\mu>0\\), order reject \\(H_0\\) favor \\(H_a\\), sample need sample mean \\(\\bar{x}\\) high lands top \\(\\alpha\\), top 5% possible datasets. , ’d p-value \\(<\\alpha\\) ’d reject \\(H_0\\). forms right-side one-tailed rejection region:\\(H_a:\\mu<0\\), conversely order reject \\(H_0\\) favor \\(H_a\\), sample need sample mean \\(\\bar{x}\\) low lands bottom \\(\\alpha\\), bottom 5% possible datasets. , ’d p-value \\(<\\alpha\\) ’d reject \\(H_0\\). forms left-side one-tailed rejection region:\n\n\nggplot() + geom_function(fun=dnorm, xlim=c(-4,4)) +\n  stat_function(fun=dnorm, geom=\"area\", xlim=c(-4,qnorm(.05)), fill=\"red\") +\n  scale_x_continuous(\n    breaks=c(-4:4,qnorm(.05)), labels=\\(x)prettyNum(x,digits=3), expand=0) +\n  scale_y_continuous(expand=0) + labs(x=\"sample mean\", y=\"density\",\n    title=TeX(\"$H_a\\\\,:\\\\,\\\\mu<0$ one-sided rejection region ($\\\\alpha=0.05$)\"))\n\\(H_a:\\mu<0\\), conversely order reject \\(H_0\\) favor \\(H_a\\), sample need sample mean \\(\\bar{x}\\) low lands bottom \\(\\alpha\\), bottom 5% possible datasets. , ’d p-value \\(<\\alpha\\) ’d reject \\(H_0\\). forms left-side one-tailed rejection region:Finally, \\(H_a:\\mu\\ne0\\), sample mean \\(\\bar{x}\\) either high low result rejecting \\(H_0\\) favor \\(H_a\\). , ’d need rejection region cover together \\(\\alpha\\) 5% extreme possible datasets either side. words, want \\(\\alpha/2\\) 2.5% top bottom corners. forms two-sided rejection region:\n\n\nggplot() + geom_function(fun=dnorm, xlim=c(-4,4)) +\n  stat_function(fun=dnorm, geom=\"area\", xlim=c(-4,qnorm(.025)), fill=\"red\") +\n  stat_function(fun=dnorm, geom=\"area\", xlim=c(qnorm(.975),4), fill=\"red\") +\n  scale_x_continuous(\n    breaks=c(-4:-3,-1:1,3:4,qnorm(c(.025,.975))), labels=\\(x)prettyNum(x,digits=3), expand=0) +\n  scale_y_continuous(expand=0) + labs(x=\"sample mean\", y=\"density\",\n    title=TeX(\"$H_a\\\\,:\\\\,\\\\mu{\\\\ne}0$ two-sided rejection region ($\\\\alpha=0.05$)\"))\nFinally, \\(H_a:\\mu\\ne0\\), sample mean \\(\\bar{x}\\) either high low result rejecting \\(H_0\\) favor \\(H_a\\). , ’d need rejection region cover together \\(\\alpha\\) 5% extreme possible datasets either side. words, want \\(\\alpha/2\\) 2.5% top bottom corners. forms two-sided rejection region:Thus, order properly compute p-value given sample, need evaluate different “tail” areas depending \\(H_a\\). Suppose parameter \\(\\theta\\), hypothesized value null \\(\\theta_0\\), sample observed statistic \\(\\theta_s\\). :one-sided hypotheses, simply take corresponding “tail” area side,\n\\(H_a:\\theta>\\theta_0\\), take upper tail area, \\(\\p(\\theta\\ge\\theta_s)\\),\n\\(H_a:\\theta<\\theta_0\\), take lower tail area, \\(\\p(\\theta\\le\\theta_s)\\),\none-sided hypotheses, simply take corresponding “tail” area side,\\(H_a:\\theta>\\theta_0\\), take upper tail area, \\(\\p(\\theta\\ge\\theta_s)\\),\\(H_a:\\theta>\\theta_0\\), take upper tail area, \\(\\p(\\theta\\ge\\theta_s)\\),\\(H_a:\\theta<\\theta_0\\), take lower tail area, \\(\\p(\\theta\\le\\theta_s)\\),\\(H_a:\\theta<\\theta_0\\), take lower tail area, \\(\\p(\\theta\\le\\theta_s)\\),two-sided hypothesis, take outer “tails” either side distribution beyond statistic,\n\\(H_a:\\theta\\ne\\theta_0\\), take \\(\\p(\\theta\\le\\theta_{s\\text{-lower}})+\\p(\\theta\\ge\\theta_{s\\text{-upper}})\\) \\(\\theta_{s\\text{-lower}},\\theta_{s\\text{-upper}}\\) two opposite-tailed lower upper statistics sample.\ndistribution null (’ve plotting time) symmetric, can just find one side multiply 2 get sides.\nFurthermore, distribution symmetric also centered around 0 (example ), two-sided p-value expression often written \\(\\p(|\\theta|\\!\\ge\\!|\\theta_s|)\\).\ntwo-sided hypothesis, take outer “tails” either side distribution beyond statistic,\\(H_a:\\theta\\ne\\theta_0\\), take \\(\\p(\\theta\\le\\theta_{s\\text{-lower}})+\\p(\\theta\\ge\\theta_{s\\text{-upper}})\\) \\(\\theta_{s\\text{-lower}},\\theta_{s\\text{-upper}}\\) two opposite-tailed lower upper statistics sample.\\(H_a:\\theta\\ne\\theta_0\\), take \\(\\p(\\theta\\le\\theta_{s\\text{-lower}})+\\p(\\theta\\ge\\theta_{s\\text{-upper}})\\) \\(\\theta_{s\\text{-lower}},\\theta_{s\\text{-upper}}\\) two opposite-tailed lower upper statistics sample.distribution null (’ve plotting time) symmetric, can just find one side multiply 2 get sides.distribution null (’ve plotting time) symmetric, can just find one side multiply 2 get sides.Furthermore, distribution symmetric also centered around 0 (example ), two-sided p-value expression often written \\(\\p(|\\theta|\\!\\ge\\!|\\theta_s|)\\).Furthermore, distribution symmetric also centered around 0 (example ), two-sided p-value expression often written \\(\\p(|\\theta|\\!\\ge\\!|\\theta_s|)\\).’s quick summary choosing direction \\(H_a\\) impacts p-value computation:suspect \\(\\theta>\\theta_0\\), thus wish reject \\(H_0\\) observed sample statistic \\(\\theta_s\\) high compared \\(\\theta_0\\), choose \\(H_a:\\theta>\\theta_0\\) take right side observed sample statistic.suspect \\(\\theta>\\theta_0\\), thus wish reject \\(H_0\\) observed sample statistic \\(\\theta_s\\) high compared \\(\\theta_0\\), choose \\(H_a:\\theta>\\theta_0\\) take right side observed sample statistic.suspect \\(\\theta<\\theta_0\\), thus wish reject \\(H_0\\) observed sample statistic \\(\\theta_s\\) low compared \\(\\theta_0\\), choose \\(H_a:\\theta<\\theta_0\\) take left side observed sample statistic.suspect \\(\\theta<\\theta_0\\), thus wish reject \\(H_0\\) observed sample statistic \\(\\theta_s\\) low compared \\(\\theta_0\\), choose \\(H_a:\\theta<\\theta_0\\) take left side observed sample statistic.suspected side want keep options open, thus wish reject \\(H_0\\) observed sample statistic \\(\\theta_s\\) either low high compared \\(\\theta_0\\), choose \\(H_a:\\theta\\ne\\theta_0\\) take two outer tails observed sample statistic.suspected side want keep options open, thus wish reject \\(H_0\\) observed sample statistic \\(\\theta_s\\) either low high compared \\(\\theta_0\\), choose \\(H_a:\\theta\\ne\\theta_0\\) take two outer tails observed sample statistic.Commonly, also say test significant p-val \\(<\\alpha\\) reject null, insignificant reject null","code":"\nggplot() + geom_function(fun=dnorm, xlim=c(-4,4)) +\n  stat_function(fun=dnorm, geom=\"area\", xlim=c(qnorm(.95),4), fill=\"red\") +\n  scale_x_continuous(\n    breaks=c(-4:4,qnorm(.95)), labels=\\(x)prettyNum(x,digits=3), expand=0) +\n  scale_y_continuous(expand=0) + labs(x=\"sample mean\", y=\"density\",\n    title=TeX(\"$H_a\\\\,:\\\\,\\\\mu>0$ one-sided rejection region ($\\\\alpha=0.05$)\"))\nggplot() + geom_function(fun=dnorm, xlim=c(-4,4)) +\n  stat_function(fun=dnorm, geom=\"area\", xlim=c(-4,qnorm(.05)), fill=\"red\") +\n  scale_x_continuous(\n    breaks=c(-4:4,qnorm(.05)), labels=\\(x)prettyNum(x,digits=3), expand=0) +\n  scale_y_continuous(expand=0) + labs(x=\"sample mean\", y=\"density\",\n    title=TeX(\"$H_a\\\\,:\\\\,\\\\mu<0$ one-sided rejection region ($\\\\alpha=0.05$)\"))\nggplot() + geom_function(fun=dnorm, xlim=c(-4,4)) +\n  stat_function(fun=dnorm, geom=\"area\", xlim=c(-4,qnorm(.025)), fill=\"red\") +\n  stat_function(fun=dnorm, geom=\"area\", xlim=c(qnorm(.975),4), fill=\"red\") +\n  scale_x_continuous(\n    breaks=c(-4:-3,-1:1,3:4,qnorm(c(.025,.975))), labels=\\(x)prettyNum(x,digits=3), expand=0) +\n  scale_y_continuous(expand=0) + labs(x=\"sample mean\", y=\"density\",\n    title=TeX(\"$H_a\\\\,:\\\\,\\\\mu{\\\\ne}0$ two-sided rejection region ($\\\\alpha=0.05$)\"))"},{"path":"inference-types.html","id":"inference-types","chapter":"Inference Types","heading":"Inference Types","text":"next section, synthesize everything ’ve learned thus far data probability see real world examples inference specific type data/question covered 240:Proportions inference: samples modeled binomial distribution, use sample proportion infer underlying probability parameter population.Proportions inference: samples modeled binomial distribution, use sample proportion infer underlying probability parameter population.Means inference: samples modeled normal distribution, use sample mean infer underlying mean parameter population. also introduce t-distribution.Means inference: samples modeled normal distribution, use sample mean infer underlying mean parameter population. also introduce t-distribution.Regression inference: introduce linear regression model relationship two numeric variables.Regression inference: introduce linear regression model relationship two numeric variables.","code":""},{"path":"proportions.html","id":"proportions","chapter":"12 Proportions","heading":"12 Proportions","text":"Let’s look specifically proportions-type inference setups, apply binomial model sample proportions infer underlying probability population. divide two scenarios:One-proportion scenario, single population single probability parameter interest, andTwo-proportions scenario, two populations, probability parameter, seek compare.proportions-type inference approach using binomial model appropriate sample consists either 1 2 samples , sample, data looks like counts across two categories, one consider “success” “failure” (e.g. heads/tails, true/false, etc.).setups, probability parameter binomial model always corresponds probability success category convention. Make sure define category whose probability primary interest “success”.","code":""},{"path":"proportions.html","id":"one-proportion","chapter":"12 Proportions","heading":"12.1 One proportion","text":"","code":""},{"path":"proportions.html","id":"model-notation","chapter":"12 Proportions","heading":"12.1.1 Model notation","text":"one-proportion scenario, suppose draw fixed sample size \\(n\\) observations, model independent constant underlying true probability \\(p\\) success, \\(1-p\\) failure (’s possible outcome).’s customary use upper-case \\(X\\) represent model true distribution \\(x\\), choose \\(X\\sim\\bin(n,p)\\).Let lower-case \\(x\\) observed number successes sample, note must \\(0\\le x\\le n\\). Let \\(\\hat p=x/n\\) denote proportion successes sample.assumptions, \\(\\hat p\\) natural point estimate \\(p\\), .e. sample proportion successes estimates true underlying probability success \\(p\\), since LLN guarantees \\(\\hat p\\p\\) \\(n\\\\infty\\).Note several things:\\(n\\) must fixed, predetermined sample size,trials must able reasonably modeled independent,must 2 possible outcomes (success & failure) trial,\\(p\\) always TRUE probability “success” category, however ’s defined,\\(\\hat p\\) observed sample proportion, can estimate true \\(p\\) LLN,\\(X\\) represents theoretical RV model choose apply sample’s observed \\(x\\),\\(x\\) represents actual observed number successes sample \\(n\\) trials.Let’s see context example. \\(n=200\\) rolls purportedly fair 5-sided die bought online (yes, really sat office rolled 200 times).Suppose question interest whether specific die design fact fair. different ways testing this41, simple way using proportions-type setup ask whether two triangular-shaped faces (4 5) observed 2/5 40% time.setup, can define “success” getting 4 5, theoretical probability \\(p=0.4\\). RV model \\(X\\), .e. number successes (4s & 5s), \\(X\\sim\\bin(200,0.4)\\).sample, observed \\(x=41+38=79\\) times actually occurred, gives us sample proportion \\(\\hat p=x/n=79/200=0.395\\).","code":"\n# combine 50-roll chunks of data, split, then parse to numeric vector\nrolls <- paste0(\"21252134521555355115322514314333142113433335345333\",\n                \"43242443535423223523352541521331244241531554241354\",\n                \"34244524112155254341541335443433245314125431131335\",\n                \"43114331251421521112234535251142334354341123345541\") %>%\n  strsplit(\"\") %>% unlist %>% as.numeric\nrolls  [1] 2 1 2 5 2 1 3 4 5 2 1 5 5 5 3 5 5 1 1 5 3 2 2 5 1 4 3 1 4 3 3 3 1 4 2 1 1\n [38] 3 4 3 3 3 3 5 3 4 5 3 3 3 4 3 2 4 2 4 4 3 5 3 5 4 2 3 2 2 3 5 2 3 3 5 2 5\n [75] 4 1 5 2 1 3 3 1 2 4 4 2 4 1 5 3 1 5 5 4 2 4 1 3 5 4 3 4 2 4 4 5 2 4 1 1 2\n[112] 1 5 5 2 5 4 3 4 1 5 4 1 3 3 5 4 4 3 4 3 3 2 4 5 3 1 4 1 2 5 4 3 1 1 3 1 3\n[149] 3 5 4 3 1 1 4 3 3 1 2 5 1 4 2 1 5 2 1 1 1 2 2 3 4 5 3 5 2 5 1 1 4 2 3 3 4\n[186] 3 5 4 3 4 1 1 2 3 3 4 5 5 4 1\n# quick table + bar plot of results using base R\ntable(rolls)rolls\n 1  2  3  4  5 \n39 32 50 41 38 \nbarplot(table(rolls))"},{"path":"proportions.html","id":"confidence-interval","chapter":"12 Proportions","heading":"12.1.2 Confidence interval","text":"one-proportion scenario, confidence interval following form:\\[\n\\text{$C\\%$ $(1\\!-\\!\\alpha)$ interval}~=~\\hat p~\\pm~z_{\\alpha/2}\\cdot\\sqrt{\\frac{\\hat p(1-\\hat p)}{n}},~\\text{ }\n\\]\\(\\alpha\\) implicitly defined 100% – C%, e.g. 95% confidence interval, \\(\\alpha=1-0.95=0.05\\),\\(\\alpha\\) implicitly defined 100% – C%, e.g. 95% confidence interval, \\(\\alpha=1-0.95=0.05\\),\\(\\hat p=x/n\\), sample proportion successes, point estimate true probability \\(p\\),\\(\\hat p=x/n\\), sample proportion successes, point estimate true probability \\(p\\),\\(z_{\\alpha/2}\\) \\(\\alpha\\)-level normal critical value \\(\\p(|Z|>|z_{\\alpha/2}|)=\\p(Z>z_{\\alpha/2})+\\p(Z<-z_{\\alpha/2})=\\alpha\\), words observation standard normal two “outer-tails” defined mirror image sum \\(\\alpha\\) together.\n\n\nlibrary(latex2exp)\nggplot() + geom_function(fun=dnorm, xlim=c(-4,4)) +\n  stat_function(fun=dnorm, geom=\"area\", xlim=c(-4,qnorm(.025)), fill=\"red\") +\n  stat_function(fun=dnorm, geom=\"area\", xlim=c(qnorm(.975),4), fill=\"red\") +\n  scale_x_continuous(breaks=qnorm(c(.025,.5,.975)), minor_breaks=NULL, expand=0,\n                     labels=TeX(c(\"$-z_{\\\\alpha/2}$\",\"0\",\"$z_{\\\\alpha/2}$\"))) +\n  scale_y_continuous(breaks=NULL, limits=c(0,.4), expand=0) + theme(axis.text.x=element_text(size=13)) +\n  labs(x=NULL, y=NULL, title=TeX(\"$z_{\\\\alpha/2}$ critical value Z (red areas sum $\\\\alpha$)\"))\n\n\nvalue called \\(z_{\\alpha/2}\\) since convention subscript denotes area right-corner, \\(\\alpha/2\\) symmetry. compute \\(\\alpha/2\\) C% interval, need ask qnorm() \\((1-\\alpha/2)\\)–percentile, e.g. 95% confidence interval, seek 97.5%-tile:\n\n# given α, e.g.\nalpha <- 0.05\n# compute 1-α/2 percentile z_α/2 critical value\n1-alpha/2\n[1] 0.975\n\nqnorm(0.975)  # often approximated 1.96 simply 2\n[1] 1.959964\\(z_{\\alpha/2}\\) \\(\\alpha\\)-level normal critical value \\(\\p(|Z|>|z_{\\alpha/2}|)=\\p(Z>z_{\\alpha/2})+\\p(Z<-z_{\\alpha/2})=\\alpha\\), words observation standard normal two “outer-tails” defined mirror image sum \\(\\alpha\\) together.value called \\(z_{\\alpha/2}\\) since convention subscript denotes area right-corner, \\(\\alpha/2\\) symmetry. compute \\(\\alpha/2\\) C% interval, need ask qnorm() \\((1-\\alpha/2)\\)–percentile, e.g. 95% confidence interval, seek 97.5%-tile:finally \\(\\se(\\hat p)=\\sqrt{\\hat p(1-\\hat p)/n}\\) estimated standard error \\(\\hat p\\), can thought dividing binomial SD \\(n\\) substituting \\(p\\\\hat p\\) everywhere.finally \\(\\se(\\hat p)=\\sqrt{\\hat p(1-\\hat p)/n}\\) estimated standard error \\(\\hat p\\), can thought dividing binomial SD \\(n\\) substituting \\(p\\\\hat p\\) everywhere.Continuing example , 95% confidence interval \\(p\\) based 79 successes 200 trials can computed :Thus, 95% confidence interval \\(p\\), .e. true probability 4 5, (0.33,0.46), words, 95% confident true \\(p\\) 0.33 0.46.different level confidence desired, simply change argument qnorm():Note lower confidence desired smaller interval, vice versa higher confidence desired larger interval.","code":"\nlibrary(latex2exp)\nggplot() + geom_function(fun=dnorm, xlim=c(-4,4)) +\n  stat_function(fun=dnorm, geom=\"area\", xlim=c(-4,qnorm(.025)), fill=\"red\") +\n  stat_function(fun=dnorm, geom=\"area\", xlim=c(qnorm(.975),4), fill=\"red\") +\n  scale_x_continuous(breaks=qnorm(c(.025,.5,.975)), minor_breaks=NULL, expand=0,\n                     labels=TeX(c(\"$-z_{\\\\alpha/2}$\",\"0\",\"$z_{\\\\alpha/2}$\"))) +\n  scale_y_continuous(breaks=NULL, limits=c(0,.4), expand=0) + theme(axis.text.x=element_text(size=13)) +\n  labs(x=NULL, y=NULL, title=TeX(\"$z_{\\\\alpha/2}$ critical value for Z (red areas sum to $\\\\alpha$)\"))\n# for a given α, e.g.\nalpha <- 0.05\n# compute the 1-α/2 percentile as the z_α/2 critical value\n1-alpha/2[1] 0.975\nqnorm(0.975)  # often approximated as 1.96 or simply 2[1] 1.959964\n# define x, n\nx <- sum(rolls>=4) ; n <- length(rolls)\nx[1] 79\nn[1] 200\n# define p-hat\nphat <- x/n   # alternative shortcut: phat <- mean(rolls>=4)\nphat[1] 0.395\n# 95% confidence interval, using c(-1,1) as a shortcut for ±\nphat + c(-1,1) * qnorm(0.975) * sqrt(phat*(1-phat)/n)[1] 0.32725 0.46275\n# e.g. for 90% interval, implied alpha=0.10 so we want qnorm of (1-0.1/2)=0.95\nphat + c(-1,1) * qnorm(0.95) * sqrt(phat*(1-phat)/n)[1] 0.3381424 0.4518576\n# a shortcut is to take the midpoint between the confidence level and 1\n# another example, for a 99% interval, we want qnorm of 0.995\nphat + c(-1,1) * qnorm(0.995) * sqrt(phat*(1-phat)/n)[1] 0.3059614 0.4840386"},{"path":"proportions.html","id":"hypothesis-test","chapter":"12 Proportions","heading":"12.1.3 Hypothesis test","text":"one-proportion scenario, wish test following hypotheses:\\[\nH_0:p~=~p_0~~~~~~~\\\\\n~~~~~~~~H_a:p~<,\\,\\ne,\\,\\text{}>~p_0\n\\]\\(p_0\\) hypothesized true probability null.start, always, assuming null, .e. suppose \\(X\\sim\\bin(n,p_0)\\). Note means sample observation \\(x\\) drawn distribution.p-value, simply compute appropriate tail area \\(x\\) corresponding alternative. Remember rule one-sided take corresponding side tail, two-sided take two outer tails (take one outer tail double ).finally, compare \\(\\alpha\\) make conclusion.Continuing die example, saw sample \\(\\hat p=0.395\\) close expect fair die \\(p_0=0.4\\). Let’s formally test . Recall last chapter wish reject \\(H_0\\) sample statistic high low, choose two-sided alternative. applies , since \\(\\hat p\\) low high vs \\(0.4\\), reject. Thus, choose:\\[\nH_0:p=0.4\\\\\nH_a:p\\ne0.4\n\\]Next, null \\(X\\sim\\bin(200,0.4)\\). distribution shown , along red line observed sample \\(x=79\\).Recall two-sided alternative, take “outer-tail” corresponding observed statistic multiply 2 get final p-value. , means look \\(2\\cdot\\p(X\\le79)\\) \\(X\\sim\\bin(200,0.4)\\).Important note: sample \\(x\\) RIGHT half curve instead left, “outer-tail” RIGHT side tail, .e. \\(\\p(X\\ge x)\\).instead alternative \\(<\\) \\(>\\), instead p-value \\(\\p(X\\le79)\\) \\(\\p(X\\ge79)\\) respectively.cases, see p-value quite large compared \\(\\alpha\\). means experimental result \\(x=79\\) fact close expectations, ’s evidence refute null. Thus, reject null, conclude die appears fair.","code":"\nmu = n*.4 ; sd = sqrt(n*.4*(1-.4))\ntibble(k=floor(mu-3*sd):ceiling(mu+3*sd),p=dbinom(k,n,0.4)) %>% \n  ggplot(aes(x=k,y=p)) + geom_col() + geom_vline(xintercept=x, color=\"red\", linewidth=1.5) +\n  ggtitle(str_glue(\"Distribution of X under null hypothesis, i.e. Bin({n},0.4)\"))\n# our two-sided p-value here\n2 * pbinom(x, n, 0.4)[1] 0.9463115\n# p-value if Ha had been p<0.4\npbinom(x, n, 0.4)[1] 0.4731557\n# p-value if Ha had been p>0.4 (note the -1 to include the bar at x)\n1 - pbinom(x-1, n, 0.4)[1] 0.5838754"},{"path":"proportions.html","id":"approximate-z-test","chapter":"12 Proportions","heading":"12.1.4 Approximate Z-test","text":"Alternatively, normal approximation can also used—assuming approximation valid—resulting normal Z-test one proportion. Different sources list different conditions, commonly approximation considered valid least 5 successes 5 failures sample.null, sample proportion \\(\\hat p\\) approximate normal distribution:\\[\n\\hat p\\asim\\n\\left(p_0~,\\sqrt{p_0(1-p_0)/n}\\right)\n\\]Thus, can find corresponding p-value computing \\(z_\\obs\\) test statistic:\\[\nz_\\obs=\\frac{\\hat p-p_0}{\\sqrt{p_0(1-p_0)/n}}\n\\]finding appropriate tail area. additional accuracy, continuity correction \\(\\pm0.5/n\\) can added/subtracted onto \\(\\hat p\\) depending direction tail.die example, look like:result comparable earlier p-value.","code":"\n# compute z_obs test statistic with continuity correction\nz_obs <- (phat + 0.5/n - 0.4)/sqrt(0.4*(1-0.4)/n)\nz_obs[1] -0.07216878\n# compute 2-sided p-value for our earlier test\n2 * pnorm(z_obs)[1] 0.9424676"},{"path":"proportions.html","id":"r-method","chapter":"12 Proportions","heading":"12.1.5 R method","text":"course, can also use R compute interval testing. good way check manual calculations make sure right. function binom.test(). accepts several arguments:x observed sample count \\(x\\)x observed sample count \\(x\\)n sample size nn sample size np (defaults 0.5) hypothesized proportion null \\(p_0\\)p (defaults 0.5) hypothesized proportion null \\(p_0\\)alternative (defaults \"two.sided\") controls direction alternative can set instead either \"greater\" \"less\"alternative (defaults \"two.sided\") controls direction alternative can set instead either \"greater\" \"less\"conf.level (defaults 0.95) controls desired confidence levelconf.level (defaults 0.95) controls desired confidence levelImportant note: setting one-sided alternative generate one-sided confidence interval one end 0 1, generally desire, want standard confidence interval well one-sided test, run function twice.Continuing die example, recall still x, n definedLet’s compute 95% confidence interval problem:Note interval slightly wider computation, since uses modified method called Clopper-Pearson formula, technically slightly better method called Wald interval, beyond scope STAT240. purposes prefer simpler Wald method.can also compute 90% 99% interval:demand R method simpler Wald interval (e.g. check computation), BinomCI() function DescTools package works:hypothesis test, note previous outputs show two-sided p-value 0.9425. slightly different computation, since uses slightly exact method instead \\(2\\cdot\\p(X\\le79)\\), computes \\(\\p(X\\le79)+\\p(X\\ge81)\\)., can feel free continue doubling outer tail area one side approximation, usually fairly accurate. can also run one-sided tests wish:time, p-values exactly found previously (rounding).","code":"\nprint(c(x,n))[1]  79 200\nbinom.test(x, n, p=0.4)\n    Exact binomial test\n\ndata:  x and n\nnumber of successes = 79, number of trials = 200, p-value = 0.9425\nalternative hypothesis: true probability of success is not equal to 0.4\n95 percent confidence interval:\n 0.3267650 0.4663964\nsample estimates:\nprobability of success \n                 0.395 \nbinom.test(x, n, p=0.4, conf.level=0.90)\n    Exact binomial test\n\ndata:  x and n\nnumber of successes = 79, number of trials = 200, p-value = 0.9425\nalternative hypothesis: true probability of success is not equal to 0.4\n90 percent confidence interval:\n 0.3371018 0.4552518\nsample estimates:\nprobability of success \n                 0.395 \nbinom.test(x, n, p=0.4, conf.level=0.99)\n    Exact binomial test\n\ndata:  x and n\nnumber of successes = 79, number of trials = 200, p-value = 0.9425\nalternative hypothesis: true probability of success is not equal to 0.4\n99 percent confidence interval:\n 0.3069060 0.4882182\nsample estimates:\nprobability of success \n                 0.395 \n# compute wald interval to check our work\nDescTools::BinomCI(x, n, 0.95, method=\"wald\")       est  lwr.ci  upr.ci\n[1,] 0.395 0.32725 0.46275\npbinom(79,200,0.4) + 1-pbinom(80,200,.4)[1] 0.9424936\nbinom.test(x, n, p=0.4, alternative=\"less\")\n    Exact binomial test\n\ndata:  x and n\nnumber of successes = 79, number of trials = 200, p-value = 0.4732\nalternative hypothesis: true probability of success is less than 0.4\n95 percent confidence interval:\n 0.0000000 0.4552518\nsample estimates:\nprobability of success \n                 0.395 \nbinom.test(x, n, p=0.4, alternative=\"greater\")\n    Exact binomial test\n\ndata:  x and n\nnumber of successes = 79, number of trials = 200, p-value = 0.5839\nalternative hypothesis: true probability of success is greater than 0.4\n95 percent confidence interval:\n 0.3371018 1.0000000\nsample estimates:\nprobability of success \n                 0.395 "},{"path":"proportions.html","id":"two-proportions","chapter":"12 Proportions","heading":"12.2 Two proportions","text":"","code":""},{"path":"proportions.html","id":"model-notation-1","chapter":"12 Proportions","heading":"12.2.1 Model notation","text":"two-proportions scenario, suppose draw samples size \\(n_1\\), \\(n_2\\) two different populations, assume everything independent (within two samples). Suppose populations true probabilities \\(p_1\\), \\(p_2\\) succeeding trial, treated true unknown constants., models true distributions \\(X_1\\sim\\bin(n_1,p_1)\\) \\(X_2\\sim\\bin(n_2,p_2)\\).Let \\(x_1\\), \\(x_2\\) observed numbers successes samples \\(0\\le x_1\\le n_1\\) \\(0\\le x_2\\le n_2\\) define \\(\\hat p_1=x_1/n_1\\), \\(\\hat p_2=x_2/n_2\\) corresponding proportions successes samples., know LLN \\(\\hat p_1\\p_1\\) \\(\\hat p_2\\p_2\\) take \\(\\hat p_1\\), \\(\\hat p_2\\) point estimates \\(p_1\\), \\(p_2\\).far ’s pretty intuitive., let’s see context example. Let’s use dataset thoracic surgery outcomes lung cancer patients Wroclaw Thoracic Surgery Centre University Wroclaw. slightly cleaned version can found : thoracic.csv.lot columns can use (see linked page explanations variables), just keep example simple, suppose want see patients smokers worse 1-year survival rates non-smokers (spoiler: ).Let’s consider population 1 non-smokers population 2 smokers. population, can use dplyr calculate number 1-year survivors sample (\\(x_1\\), \\(x_2\\)) divide total number smokers non-smokers (\\(n_1\\), \\(n_2\\)) get 1-year survival rates group (\\(\\hat p_1\\), \\(\\hat p_2\\)).can see now \\(x_1=77\\), \\(x_2=323\\), \\(n_1=84\\), \\(n_2=386\\), \\(\\hat p_1=0.917\\), \\(\\hat p_2=0.837\\).can now take dataset proceed find confidence interval conduct hypothesis test.","code":"\n# remember to load packages and set any desired options\nthoracic <- read_csv(\"https://bwu62.github.io/stat240-revamp/data/thoracic.csv\")\nthoracic# A tibble: 470 × 17\n  dgn     fvc  fev1 perf  pain  haem  dysp  cough weak  size  type2dm mi6  \n  <chr> <dbl> <dbl> <chr> <lgl> <lgl> <lgl> <lgl> <lgl> <chr> <lgl>   <lgl>\n1 DGN2   2.88  2.16 PRZ1  FALSE FALSE FALSE TRUE  TRUE  OC14  FALSE   FALSE\n2 DGN3   3.4   1.88 PRZ0  FALSE FALSE FALSE FALSE FALSE OC12  FALSE   FALSE\n3 DGN3   2.76  2.08 PRZ1  FALSE FALSE FALSE TRUE  FALSE OC11  FALSE   FALSE\n4 DGN3   3.68  3.04 PRZ0  FALSE FALSE FALSE FALSE FALSE OC11  FALSE   FALSE\n5 DGN3   2.44  0.96 PRZ2  FALSE TRUE  FALSE TRUE  TRUE  OC11  FALSE   FALSE\n# ℹ 465 more rows\n# ℹ 5 more variables: pad <lgl>, smoker <lgl>, asthma <lgl>, age <dbl>,\n#   survive1 <lgl>\nthoracic_smoker_summary <- thoracic %>% \n  count(smoker, survive1, name=\"x\") %>% \n  group_by(smoker) %>% \n  mutate(n=sum(x)) %>% \n  summarise(x=last(x), n=last(n), phat=x/n)\nthoracic_smoker_summary# A tibble: 2 × 4\n  smoker     x     n  phat\n  <lgl>  <int> <int> <dbl>\n1 FALSE     77    84 0.917\n2 TRUE     323   386 0.837"},{"path":"proportions.html","id":"confidence-interval-1","chapter":"12 Proportions","heading":"12.2.2 Confidence interval","text":"two-proportions scenario, generally goal run inference difference underlying population probabilities \\(p_1-p_2\\). think parameter interest.parameter, point estimate naturally difference sample proportions \\(\\hat p_1-\\hat p_2\\). , interval takes form:\\[\n\\text{$C\\%$ $(1\\!-\\!\\alpha)$ interval}~=~(\\hat p_1-\\hat p_2)~\\pm~z_{\\alpha/2}\\cdot\\sqrt{\\frac{\\hat p_1(1-\\hat p_1)}{n_1}+\\frac{\\hat p_2(1-\\hat p_2)}{n_2}},~\\text{ }\n\\]\\(\\hat p_1-\\hat p_2\\) point estimate true difference \\(p_1-p_2\\),\\(\\hat p_1-\\hat p_2\\) point estimate true difference \\(p_1-p_2\\),\\(z_{\\alpha/2}\\) \\(\\alpha\\)-level normal critical value,\\(z_{\\alpha/2}\\) \\(\\alpha\\)-level normal critical value,\\(\\sqrt{\\frac{\\hat p_1(1-\\hat p_1)}{n_1}+\\frac{\\hat p_2(1-\\hat p_2)}{n_2}}\\) standard error difference sample proportions, .e. \\(\\se(\\hat p_1-\\hat p_2)\\). formula comes fact independent \\(X\\), \\(Y\\), \\(\\var(X\\pm Y)=\\var(X)+\\var(Y)\\)\\(\\sqrt{\\frac{\\hat p_1(1-\\hat p_1)}{n_1}+\\frac{\\hat p_2(1-\\hat p_2)}{n_2}}\\) standard error difference sample proportions, .e. \\(\\se(\\hat p_1-\\hat p_2)\\). formula comes fact independent \\(X\\), \\(Y\\), \\(\\var(X\\pm Y)=\\var(X)+\\var(Y)\\)Continuing thoracic surgery example, let’s compute 95% confidence interval true difference survival probability \\(p_1-p_2\\) smokers non-smokersThus can see 95% confidence interval true difference probabilities (0.010,0.15). words, based data, ’re 95% confident lung cancer patients don’t smoke 1.0% 15% likely survive least 1 year receiving thoracic surgery smoke.","code":"\n# starting with summary, mutate to add se contribution from each group\n# then summarize to get the point estimate, combined se, and interval bounds\n# note -diff() is necessary to get row1-row2\nthoracic_smoker_summary %>% mutate(se = sqrt(phat*(1-phat)/n)) %>%\n  summarize(p1mp2 = -diff(phat), se = sqrt(sum(se^2)),\n            lower95 = p1mp2-1.96*se, upper95 = p1mp2+1.96*se)# A tibble: 1 × 4\n   p1mp2     se lower95 upper95\n   <dbl>  <dbl>   <dbl>   <dbl>\n1 0.0799 0.0355  0.0102   0.150"},{"path":"proportions.html","id":"hypothesis-test-1","chapter":"12 Proportions","heading":"12.2.3 Hypothesis test","text":", two-proportions scenario, ’re usually interested inference difference underlying population probabilities \\(p_1-p_2\\). two proportions must resort approximate normal Z-test, typically considered valid approximation long least 5 successes 5 failures sample. hypotheses :\\[\nH_0:p_1-p_2~=~d~~~~~~~\\\\\n~~~~~~~~H_a:p_1-p_2~<,\\,\\ne,\\,\\text{}>~d\n\\]\\(d\\) hypothesized true difference populations’ probabilities null. far common case \\(d=0\\) means end testing following:\\[\nH_0:p_1~=~p_2~~~~~~~~\\\\\n~~~~~~~H_a:p_1~<,\\,\\ne,\\,\\text{}>~p_2\n\\]STAT240 focus \\(d=0\\) case42, null two populations probability observed differences sample proportion due random chance, must compute “pooled” sample proportion estimate \\(\\bar p\\), since doesn’t make sense two different sample proportion estimates.\\[\n\\bar p=\\frac{x_1+x_2}{n_1+n_2}\n\\], compute \\(z_\\obs\\) test statistic :\\[\nz_\\obs=\\frac{(\\hat p_1-\\hat p_2)-0}{\\sqrt{\\bar p(1-\\bar p)\\left(\\frac1{n_1}+\\frac1{n_2}\\right)}}\n\\]Finally, compute p-value taking appropriate tail area.Continuing thoracic surgery example, defined sample 1 non-smokers sample 2 smokers, makes sense ask non-smokers higher rates 1-year survival smokers. Thus, choose hypotheses:\\[\nH_0:p_1-p_2=0\\\\\nH_a:p_1-p_2>0\n\\]words,\\[\nH_0:p_1=p_2\\\\\nH_a:p_1>p_2\n\\], applying formulae, obtain:test statistic \\(z_\\obs=1.86\\) gives us upper-tail p-value \\(0.0312\\) less standard \\(\\alpha=0.05\\), can reject null .summary, \\(\\alpha=0.05\\) significance level, data suggests lung cancer patients don’t smoke significantly likely survive least 1 year receiving thoracic surgery smoke.","code":"\nthoracic_smoker_summary %>%\n  summarise(p1mp2 = -diff(phat), pbar = sum(x)/sum(n),\n            z = p1mp2/sqrt(pbar*(1-pbar)*sum(1/n)), pval = 1-pnorm(z))# A tibble: 1 × 4\n   p1mp2  pbar     z   pval\n   <dbl> <dbl> <dbl>  <dbl>\n1 0.0799 0.851  1.86 0.0312"},{"path":"proportions.html","id":"r-method-1","chapter":"12 Proportions","heading":"12.2.4 R method","text":"R method analyzing two proportions prop.test(), unlike binom.test() can handle one proportion. arguments similar (see help page details), except x n input vector success counts sample sizes corresponding two samples. Note alternative, one-sided, indicates first sample less/greater second sample.Note print-prop.test() actually references \\(\\chi^2\\)-test instead normal-based test, equivalent formulations methodology produce exact intervals p-values, ’ll see example .Continuing thoracic surgery example, recall sample statistics:interval testing approaches, must give x n columns prop.test(). arguments depend exactly wish .compute proper 95% confidence interval true probability difference, must always use default two-sided alternative:Note default also applies continuity correction, generally recommended, interval slightly widened adjusting upper lower bounds \\(\\pm0.5(1/n_1\\!+\\!1/n_2)\\). can verify quick modification earlier computation:forgo continuity correction, simply add argument correct = FALSE.run hypothesis instead, must remember set correct alternative hypothesis direction. want \"greater\" since group 1 non-smokers, believe greater survival probability non-smokers group 2., includes default continuity correction, applied \\(\\pm0.5(1/n_1\\!+\\!1/n_2)\\) onto \\((\\hat p_1\\!-\\!\\hat p_2)\\) absolute value lowered \\(z_\\obs\\) equation, effectively slightly “shrinking” observed difference sample proportions. can also verify quick modification earlier computation:, desired, turn correct = FALSEIn either case, can see everything matches perfectly content.","code":"\nthoracic_smoker_summary# A tibble: 2 × 4\n  smoker     x     n  phat\n  <lgl>  <int> <int> <dbl>\n1 FALSE     77    84 0.917\n2 TRUE     323   386 0.837\nprop.test(c(77,323), c(84,386), conf.level = 0.95)\n    2-sample test for equality of proportions with continuity correction\n\ndata:  th$x out of th$n\nX-squared = 2.8711, df = 1, p-value = 0.09018\nalternative hypothesis: two.sided\n95 percent confidence interval:\n 0.002970985 0.156787219\nsample estimates:\n   prop 1    prop 2 \n0.9166667 0.8367876 \nthoracic_smoker_summary %>% mutate(se = sqrt(phat*(1-phat)/n)) %>%\n summarize(p1mp2 = -diff(phat), se = sqrt(sum(se^2)),\n           lower95 = p1mp2-1.96*se, upper95 = p1mp2+1.96*se,\n           lower95_cor = lower95-0.5*sum(1/n), upper95_cor = upper95+0.5*sum(1/n))# A tibble: 1 × 6\n   p1mp2     se lower95 upper95 lower95_cor upper95_cor\n   <dbl>  <dbl>   <dbl>   <dbl>       <dbl>       <dbl>\n1 0.0799 0.0355  0.0102   0.150     0.00297       0.157\nprop.test(c(77,323), c(84,386), conf.level = 0.95, correct = FALSE)\n    2-sample test for equality of proportions without continuity correction\n\ndata:  th$x out of th$n\nX-squared = 3.4727, df = 1, p-value = 0.06239\nalternative hypothesis: two.sided\n95 percent confidence interval:\n 0.0102187 0.1495395\nsample estimates:\n   prop 1    prop 2 \n0.9166667 0.8367876 \nprop.test(c(77,323), c(84,386), alternative = \"greater\")\n    2-sample test for equality of proportions with continuity correction\n\ndata:  th$x out of th$n\nX-squared = 2.8711, df = 1, p-value = 0.04509\nalternative hypothesis: greater\n95 percent confidence interval:\n 0.01417053 1.00000000\nsample estimates:\n   prop 1    prop 2 \n0.9166667 0.8367876 \nthoracic_smoker_summary %>%\n  summarise(p1mp2 = -diff(phat), pbar = sum(x)/sum(n),\n            z = p1mp2/sqrt(pbar*(1-pbar)*sum(1/n)), pval = 1-pnorm(z),\n            z_cor = (p1mp2-sign(p1mp2)*0.5*sum(1/n))/sqrt(pbar*(1-pbar)*sum(1/n)),\n            pval_cor = 1-pnorm(z_cor))# A tibble: 1 × 6\n   p1mp2  pbar     z   pval z_cor pval_cor\n   <dbl> <dbl> <dbl>  <dbl> <dbl>    <dbl>\n1 0.0799 0.851  1.86 0.0312  1.69   0.0451\nprop.test(c(77,323), c(84,386), alternative = \"greater\", correct = FALSE)\n    2-sample test for equality of proportions without continuity correction\n\ndata:  th$x out of th$n\nX-squared = 3.4727, df = 1, p-value = 0.03119\nalternative hypothesis: greater\n95 percent confidence interval:\n 0.02141825 1.00000000\nsample estimates:\n   prop 1    prop 2 \n0.9166667 0.8367876 "},{"path":"means.html","id":"means","chapter":"13 Means","heading":"13 Means","text":"Now let’s look means-type interference setup, apply normal model sample means infer underlying true mean population. divide two broad scenarios :One-mean scenario, single mean interest single population, andOne-mean scenario, single mean interest single population, andTwo-means scenario, two means interest seek compare. time, subdivide additional sub-cases:\nPaired case, two samples natural one--one correspondence akin paired repeat observations population, vs\nUnpaired case, two samples fact two independent samples drawn two independent populations, mean.\nTwo-means scenario, two means interest seek compare. time, subdivide additional sub-cases:Paired case, two samples natural one--one correspondence akin paired repeat observations population, vsPaired case, two samples natural one--one correspondence akin paired repeat observations population, vsUnpaired case, two samples fact two independent samples drawn two independent populations, mean.Unpaired case, two samples fact two independent samples drawn two independent populations, mean.means-type inference approach using normal model appropriate sample consists either 1 2 samples , sample, data numeric values least approximately normal-looking.","code":""},{"path":"means.html","id":"t-distribution","chapter":"13 Means","heading":"13.1 T-distribution","text":"moment delve notation examples means inference, first let’s discuss t-distribution.proportions inference section, recall generally know \\(n\\) need infer \\(p\\), \\(p\\) estimated hypothesized, expectation variance resulting binomial distribution determined, since ’re direct functions \\(p\\).normal distributions, generally start knowing either \\(\\mu\\) \\(\\sigma\\), furthermore can shown corresponding sample statistics \\(\\bar x\\) \\(s\\) fact totally statistically independent 43.implication even though usually ’re primarily interested inference just mean(s), also need separately estimate standard deviation order “pin ” specific version normal. words, generally use LLN estimate \\(\\bar x\\\\mu\\) \\(s\\\\sigma\\).However, leads another problem, sometimes \\(s\\) underestimate \\(\\sigma\\) (sometimes may overestimate), distorts result reference distribution (use intervals testing) slightly away standard normal, especially small samples. Instead, use different reference distribution: t-distribution.student’s t-distribution, ’s formally called, complex PDF won’t show simple motivation: models distribution normalized sample mean true \\(\\sigma\\) unknown (similar proportions section, normal models distribution sample proportion).t-distribution single parameter called degrees freedom, sometimes \\(\\nu\\), must positive, usually integer, although technically real number allowed. sounds fancy, ’s just parameter like , .e. ’s number formula determines exact shape distribution, just like \\(\\mu\\) \\(p\\).’s plot t-distribution various degrees freedom:key facts hopefully intuitive examining plot:Conceptually, t-distributions basically standard normal distribution (Z) except modified “heavier tails”Conceptually, t-distributions basically standard normal distribution (Z) except modified “heavier tails”lower degrees freedom, bigger modification, .e. “heavier” tails\n\\(\\nu\\\\infty\\), t-distribution approaches normal\ncase \\(\\nu=1\\) special name: Cauchy distribution, interesting properties (tails “heavy” undefined—infinite—mean, variance, skew, higher moments), makes useful many counter examples, e.g. breaking LLN CLT.\nlower degrees freedom, bigger modification, .e. “heavier” tailsAs \\(\\nu\\\\infty\\), t-distribution approaches normalAs \\(\\nu\\\\infty\\), t-distribution approaches normalThe case \\(\\nu=1\\) special name: Cauchy distribution, interesting properties (tails “heavy” undefined—infinite—mean, variance, skew, higher moments), makes useful many counter examples, e.g. breaking LLN CLT.case \\(\\nu=1\\) special name: Cauchy distribution, interesting properties (tails “heavy” undefined—infinite—mean, variance, skew, higher moments), makes useful many counter examples, e.g. breaking LLN CLT.Similar normal, ’s bell-ish shaped symmetric center 0Similar normal, ’s bell-ish shaped symmetric center 0For \\(\\nu>2\\) variance \\(\\sigma^2=\\nu/(\\nu-2)\\)\\(\\nu>2\\) variance \\(\\sigma^2=\\nu/(\\nu-2)\\)R functions working t-distribution dt(), pt(), qt(), rt(), just ’d expect.\nfunctions also demands df argument degrees freedom, don’t forget specify using.\nR functions working t-distribution dt(), pt(), qt(), rt(), just ’d expect.functions also demands df argument degrees freedom, don’t forget specify using.","code":"\nlibrary(tidyverse) ; library(plotly)\nn <- 101 ; xl <- 4\ntdist <- tibble(\n  df = rep(c(1:10,12,14,16,18,20,25,30,35,40,50,60,80,100,Inf),each=n),\n  x = rep(seq(-xl,xl,length.out=n),length(unique(df))),\n  T = dt(x,df),\n  N = dnorm(x)) %>% pivot_longer(3:4, names_to = \"dist\", values_to = \"y\")\nplot_ly(tdist, type = \"scatter\", x = ~x, y = ~y, frame = ~df, color = ~dist, mode = \"lines\") %>%\n  config(displayModeBar = FALSE) %>% \n  layout(title = list(text = \"T-distribution with various degrees of freedom vs Normal(0,1)\", x = .05),\n         margin = list(l = 10, r = 10, b = 50, t = 50), dragmode=FALSE) %>% animation_opts(frame = 100) %>%\n  animation_slider(currentvalue = list(font = list(color = \"#444444\")))"},{"path":"means.html","id":"one-mean","chapter":"13 Means","heading":"13.2 One mean","text":"","code":""},{"path":"means.html","id":"model-notation-2","chapter":"13 Means","heading":"13.2.1 Model notation","text":"Now understand t-distribution, let’s discuss one-mean scenario. Suppose draw sample size \\(n\\) observations, done independently, underlying normal population underlying true mean \\(\\mu\\) SD \\(\\sigma\\).Thus, model population \\(X\\sim\\n(\\mu,\\sigma)\\) unknown, constant \\(\\mu\\), \\(\\sigma\\).Let \\(\\bar x\\) sample mean, \\(s\\) sample standard deviation (defined sections 5.1.1 5.2.1). LLN, know \\(\\bar x\\\\mu\\) \\(s\\\\sigma\\), naturally point estimates.notes important:Technically population required normal, however practice t-based mean inference methods ’re learning considered quite robust non-normality, .e. even moderate departures normality often considered problematic.Technically population required normal, however practice t-based mean inference methods ’re learning considered quite robust non-normality, .e. even moderate departures normality often considered problematic.Even though generally interested inference \\(\\mu\\), still need estimate \\(\\sigma\\) \\(s\\) order model variance structure, generally necessary part inference.Even though generally interested inference \\(\\mu\\), still need estimate \\(\\sigma\\) \\(s\\) order model variance structure, generally necessary part inference.far notation quite intuitive, let’s move directly confidence interval formula.","code":""},{"path":"means.html","id":"confidence-interval-2","chapter":"13 Means","heading":"13.2.2 Confidence interval","text":"one-mean scenario, confidence interval following form:\\[\n\\text{$C\\%$ $(1\\!-\\!\\alpha)$ interval}~=~\\bar x~\\pm~t_{\\alpha/2,n-1}\\cdot\\frac s{\\sqrt n},~\\text{ }\n\\]\\(\\bar x=\\sum x_i/n\\) sample mean,\\(\\bar x=\\sum x_i/n\\) sample mean,\\(s=\\sqrt{\\sum(x_i-\\bar x)^2/(n\\!-\\!1)}\\) sample standard deviation,\\(s=\\sqrt{\\sum(x_i-\\bar x)^2/(n\\!-\\!1)}\\) sample standard deviation,\\(t_{\\alpha/2,n-1}\\) \\(\\alpha\\)-level critical value t-distribution \\(n-1\\) degrees freedom, \\(\\p(|T|>|t_{\\alpha/2,n-1}|)=\\p(T>t_{\\alpha/2,n-1})+\\p(T<-t_{\\alpha/2,n-1})=\\alpha\\), words observation t-distribution normal two “outer-tails” defined mirror image sum \\(\\alpha\\) together.\n\n\nlibrary(latex2exp)\ndf <- 3 ; xl <- 6  # pick example df xlimits\nggplot() + geom_function(fun=\\(x)dt(x,df), xlim=c(-xl,xl)) +\n  stat_function(fun=\\(x)dt(x,df), geom=\"area\", xlim=c(-xl,qt(.025,df)), fill=\"red\") +\n  stat_function(fun=\\(x)dt(x,df), geom=\"area\", xlim=c(qt(.975,df),xl), fill=\"red\") +\n  scale_x_continuous(breaks=qt(c(.025,.5,.975),df), minor_breaks=NULL, expand=0,\n                     labels=TeX(c(\"$-t_{\\\\alpha/2,n-1}$\",\"0\",\"$t_{\\\\alpha/2,n-1}$\"))) +\n  scale_y_continuous(breaks=NULL, limits=c(0,dt(0,df)*1.01), expand=0) + theme(axis.text.x=element_text(size=13)) +\n  labs(x=NULL, y=NULL, title=TeX(\"$t_{\\\\alpha/2,n-1}$ critical value T (red areas sum $\\\\alpha$)\"))\n\n\nsimilar \\(z_{\\alpha/2}\\) proportion, except due heavier tails t-distribution, value always higher, .e. \\(t_{\\alpha/2,n-1}>z_{\\alpha/2}\\) \\(n\\). However, \\(n\\\\infty\\), approaches normal critical value, similar \\(n\\\\infty\\) t-distribution approaches normal.\nAlso note degrees freedom always \\(n-1\\), one less sample size. interesting interpretation order anything meaningful inference-wise, without running Cauchy-related issues, generally need \\(n\\ge3\\).\ncompute critical value, use qt(), taking care remember always specify df argument:\n\n# suppose sample size n=6, use common alpha\nn <- 6 ; alpha <- 0.05\n# corresponding n-1 df alpha/2 t-critical value\nqt(1-alpha/2, n-1)\n[1] 2.570582\nfinally, \\(\\se(\\bar x)=s/\\sqrt n\\) estimated standard error \\(\\bar x\\). perfect world know \\(\\sigma\\) use instead \\(s\\), reverts reference distribution back normal instead t–, usually case.\n\\(t_{\\alpha/2,n-1}\\) \\(\\alpha\\)-level critical value t-distribution \\(n-1\\) degrees freedom, \\(\\p(|T|>|t_{\\alpha/2,n-1}|)=\\p(T>t_{\\alpha/2,n-1})+\\p(T<-t_{\\alpha/2,n-1})=\\alpha\\), words observation t-distribution normal two “outer-tails” defined mirror image sum \\(\\alpha\\) together.similar \\(z_{\\alpha/2}\\) proportion, except due heavier tails t-distribution, value always higher, .e. \\(t_{\\alpha/2,n-1}>z_{\\alpha/2}\\) \\(n\\). However, \\(n\\\\infty\\), approaches normal critical value, similar \\(n\\\\infty\\) t-distribution approaches normal.Also note degrees freedom always \\(n-1\\), one less sample size. interesting interpretation order anything meaningful inference-wise, without running Cauchy-related issues, generally need \\(n\\ge3\\).compute critical value, use qt(), taking care remember always specify df argument:finally, \\(\\se(\\bar x)=s/\\sqrt n\\) estimated standard error \\(\\bar x\\). perfect world know \\(\\sigma\\) use instead \\(s\\), reverts reference distribution back normal instead t–, usually case.Suppose following dataset \\(n=15\\) masses grams ball bearings manufactured factory machine calibrated mean mass \\(\\mu=12\\) grams.Since \\(n=15\\) observations, require t-distribution \\(\\nu=14\\) degrees freedom; can also compute sample mean SD:Putting together, ’s 95% confidence interval true mean \\(\\mu\\) based sample:Thus, 95% interval true mean (11.86,12.01). words, 95% confident true mean mass ball bearings produced machine 11.86 12.01 grams.","code":"\nlibrary(latex2exp)\ndf <- 3 ; xl <- 6  # pick an example df and xlimits\nggplot() + geom_function(fun=\\(x)dt(x,df), xlim=c(-xl,xl)) +\n  stat_function(fun=\\(x)dt(x,df), geom=\"area\", xlim=c(-xl,qt(.025,df)), fill=\"red\") +\n  stat_function(fun=\\(x)dt(x,df), geom=\"area\", xlim=c(qt(.975,df),xl), fill=\"red\") +\n  scale_x_continuous(breaks=qt(c(.025,.5,.975),df), minor_breaks=NULL, expand=0,\n                     labels=TeX(c(\"$-t_{\\\\alpha/2,n-1}$\",\"0\",\"$t_{\\\\alpha/2,n-1}$\"))) +\n  scale_y_continuous(breaks=NULL, limits=c(0,dt(0,df)*1.01), expand=0) + theme(axis.text.x=element_text(size=13)) +\n  labs(x=NULL, y=NULL, title=TeX(\"$t_{\\\\alpha/2,n-1}$ critical value for T (red areas sum to $\\\\alpha$)\"))\n# suppose we have a sample of size n=6, and we use the common alpha\nn <- 6 ; alpha <- 0.05\n# the corresponding n-1 df alpha/2 t-critical value\nqt(1-alpha/2, n-1)[1] 2.570582\nbearings <- c(11.77, 11.83, 11.88, 12.00, 11.95, 11.96, 11.77, 11.91,\n              11.80, 12.09, 12.20, 11.85, 11.86, 12.18, 11.99)\nbearings [1] 11.77 11.83 11.88 12.00 11.95 11.96 11.77 11.91 11.80 12.09 12.20 11.85 11.86\n[14] 12.18 11.99\nn <- length(bearings)  # define our sample size\nmean(bearings)[1] 11.936\nsd(bearings)[1] 0.1369463\nqt(0.975, df=14)[1] 2.144787\nmean(bearings) + c(-1,1) * qt(0.975, df=n-1) * sd(bearings)/sqrt(n)[1] 11.86016 12.01184"},{"path":"means.html","id":"t-test","chapter":"13 Means","heading":"13.2.3 T-test","text":"one-mean hypothesis test, commonly simply called t-test, starts following familiar form hypotheses:\\[\nH_0:\\mu~=~\\mu_0\\,~~~~~~~\\\\\n~~~~~~~\\,H_a:\\mu~<,\\,\\ne,\\,\\text{}>~\\mu_0\n\\]\\(\\mu_0\\) hypothesized true mean null. null, sample mean \\(\\bar x\\) distributed normally:\\[\n\\bar x\\sim N(\\mu_0,\\sigma/\\!\\sqrt n)\n\\]discussed previously, estimate \\(s\\\\sigma\\) necessity, means following test statistic \\(t_\\obs\\) adopts t-distribution \\(\\nu=n-1\\) degrees freedom:\\[\nt_\\obs=\\frac{\\bar x-\\mu_0}{s/\\!\\sqrt n}~\\sim~T_{n-1}\n\\], , ’s simply matter taking appropriate tail area.Continuing ball bearing example , suppose wish test machine properly calibrated \\(\\mu=12\\) grams. Let’s use two-sided hypothesis , since ’s miscalibrated either side (high low), probably want recalibrate .\\[\nH_0:\\mu=12\\\\\nH_a:\\mu\\ne12\n\\]can compute test statistic:, compute p-value, remembering take twice outer tail:can see p-value less standard \\(\\alpha=0.05\\), reject null. words, ’s insufficient evidence suggest machine miscalibrated, ’s need recalibrate .","code":"\nt_obs <- (mean(bearings)-12) / (sd(bearings)/sqrt(n))\nt_obs[1] -1.809987\n2 * pt(t_obs, df=n-1)[1] 0.09181049"},{"path":"means.html","id":"r-method-2","chapter":"13 Means","heading":"13.2.4 R method","text":"R, can compute one-mean interval t-test using t.test() function, following important arguments:x vector observations,x vector observations,alternative alternative direction (t-test), can either \"two.sided\" (default) \"greater\" \"less\"alternative alternative direction (t-test), can either \"two.sided\" (default) \"greater\" \"less\"mu hypothesized \\(\\mu_0\\) nullmu hypothesized \\(\\mu_0\\) nullconf.level desired confidence (interval) also defaults 95%.conf.level desired confidence (interval) also defaults 95%.Continuing ball bearings example, 95% confidence interval true mean, well t-test testing \\(\\mu=12\\) vs \\(\\mu\\ne12\\).can see interval p-value match earlier computations.","code":"\nt.test(bearings, mu=12, conf.level=0.95, alternative=\"two.sided\")\n    One Sample t-test\n\ndata:  bearings\nt = -1.81, df = 14, p-value = 0.09181\nalternative hypothesis: true mean is not equal to 12\n95 percent confidence interval:\n 11.86016 12.01184\nsample estimates:\nmean of x \n   11.936 "},{"path":"means.html","id":"two-samples-paired","chapter":"13 Means","heading":"13.3 Two samples – Paired","text":"two-means scenario, start considering paired case.Suppose data consists \\(n\\) independent pairs comparable observations (.e. can differenced), primarily interested inference differences pairs. case, don’t separately model observations pair coming different independent distribution; instead model differences sample drawn population possible differences run inference .case quite common real world, e.g. vs measurements treatment, measurements twins, left vs right side measurements single individual, etc. cases, differencing pairs observations can help control certain additional unobserved pair--pair confounders.key thing remember paired data : ’s completely equivalent previously discussed one-sample T-based methods vector differences!small study done test effectiveness new diet lowering cholesterol levels. data \\(n=9\\) subjects.Find 95% confidence interval change cholesterol diet. Also run hypothesis test standard significance level test significant decrease cholesterol levels.First, let’s load dataset compute changes. Note changes differenced –, way around!Now, can simply apply one-mean methods previous section! ’s 95% confidence interval, manually using R:hypothesis test, choose \\(~H_0\\!:\\!\\mu\\!=\\!0~\\) vs \\(~H_a\\!:\\!\\mu\\!<\\!0~\\) \\(\\mu\\) represents true average change cholesterol levels resulting diet. , can compute t-statistic p-value, also check using R:Alternatively, can also give vectors t.test() separately x y arguments, set argument paired = TRUE, result exact results. Note default R always take difference arguments order first–second.Either way, can see everything agrees.","code":"\nbefore <- c(209, 210, 205, 198, 216, 217, 238, 240, 222)\nafter <- c(199, 207, 189, 209, 217, 202, 211, 223, 201)\ndiff <- after - before\ndiff[1] -10  -3 -16  11   1 -15 -27 -17 -21\nn <- length(diff)\nmean(diff) + c(-1,1) * qt(0.975, df=n-1) * sd(diff)/sqrt(n)[1] -19.895310  -1.660246\nt.test(diff, conf.level=0.95)\n    One Sample t-test\n\ndata:  diff\nt = -2.7259, df = 8, p-value = 0.02601\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -19.895310  -1.660246\nsample estimates:\nmean of x \n-10.77778 \nt_obs <- (mean(diff)-0) / (sd(diff)/sqrt(n))\nt_obs[1] -2.725913\npt(t_obs, df=n-1)[1] 0.01300485\nt.test(diff, mu=0, alternative=\"less\")\n    One Sample t-test\n\ndata:  diff\nt = -2.7259, df = 8, p-value = 0.013\nalternative hypothesis: true mean is less than 0\n95 percent confidence interval:\n      -Inf -3.425454\nsample estimates:\nmean of x \n-10.77778 \n# 95% confidence interval\nt.test(after, before, paired=TRUE, conf.level=0.95)\n    Paired t-test\n\ndata:  after and before\nt = -2.7259, df = 8, p-value = 0.02601\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -19.895310  -1.660246\nsample estimates:\nmean difference \n      -10.77778 \n# hypothesis test\nt.test(after, before, paired=TRUE, mu=0, alternative=\"less\")\n    Paired t-test\n\ndata:  after and before\nt = -2.7259, df = 8, p-value = 0.013\nalternative hypothesis: true mean difference is less than 0\n95 percent confidence interval:\n      -Inf -3.425454\nsample estimates:\nmean difference \n      -10.77778 "},{"path":"means.html","id":"two-samples-unpaired","chapter":"13 Means","heading":"13.4 Two samples – Unpaired","text":"Next, consider unpaired case, also called two independent samples case.","code":""},{"path":"means.html","id":"model-notation-3","chapter":"13 Means","heading":"13.4.1 Model notation","text":"case, suppose draw samples size \\(n_1\\), \\(n_2\\) two different normal populations means \\(\\mu_1\\), \\(\\mu_2\\) SDs \\(\\sigma_1\\), \\(\\sigma_2\\), assume everything independent (within two samples).Thus models two populations \\(X_1\\sim\\n(\\mu_1,\\sigma_1)\\), \\(X_2\\sim\\n(\\mu_2,\\sigma_2)\\) mean & SD parameters unknown constants.Let \\(\\bar x_1\\), \\(\\bar x_2\\) sample means \\(s_1\\), \\(s_2\\) sample SDs. LLN, know \\(\\bar x_i\\\\mu_i\\) \\(s_i\\\\sigma_i\\).two-means scenario, generally goal run inference difference underlying population means \\(\\mu_1-\\mu_2\\). think parameter interest.parameter, point estimate naturally difference sample means \\(\\bar x_1-\\bar x_2\\).Similar one-mean scenario, scenario sensitive non-normality data, t-distribution necessary since estimate \\(s_i\\\\sigma_i\\)example dataset , let’s use dataset Kevin Young’s 2004 Science article flat-tailed horned lizard. study, scientists collected two samples: lizards species alive, killed loggerhead shrikes, common predator. sample, lizard horn lengths measured see evidence longer horns provided better defense thus chances survival shrikes. data lizards.tsv.Let sample 1 shrike-killed lizards, sample 2 living lizards. \\(n_1=30\\), \\(n_2=154\\), \\(\\bar x_1=22.0\\), \\(\\bar x_2=24.3\\), \\(s_1=2.71\\), \\(s_2=2.63\\).","code":"\n# remember to load packages and set any desired options\n# read in data and summarize to groups (lengths are in mm)\nlizards <- read_tsv(\"https://bwu62.github.io/stat240-revamp/data/lizards.tsv\")\nlizards# A tibble: 184 × 2\n  hornLength survival\n       <dbl> <chr>   \n1       25.2 Living  \n2       26.9 Living  \n3       26.6 Living  \n4       25.6 Living  \n5       25.7 Living  \n# ℹ 179 more rows\nlizards_summary <- lizards %>% \n  group_by(survival) %>%\n  summarize(n = n(), mean = mean(hornLength), sd = sd(hornLength))\nlizards_summary# A tibble: 2 × 4\n  survival     n  mean    sd\n  <chr>    <int> <dbl> <dbl>\n1 Killed      30  22.0  2.71\n2 Living     154  24.3  2.63"},{"path":"means.html","id":"welch-method","chapter":"13 Means","heading":"13.4.2 Welch method","text":"days, generally recommended method inference Welch method interval testing, also sometimes called unequal variance method, although slight misnomer. Welch method doesn’t assume \\(\\sigma_1\\), \\(\\sigma_2\\) unequal, just doesn’t assume equal. opposed now outdated recommendation look \\(s_1\\), \\(s_2\\) choose equal-variance method, also called two-sample t-test, assume equality variances (later)., ’s now generally recommended default Welch method, presented .Welch method models uses t-based method model sample difference means. main equation interest degree freedom equation, known Satterthwaite approximation:\\[\n\\nu=\\frac{\\left(s_1^2/n_1+s_2^2/n_2\\right)^2}{\\frac{(s_1^2/n_1)^2}{n_1-1}+\\frac{(s_2^2/n_2)^2}{n_2-1}}\n\\]gives approximate degrees freedom relevant Welch-based interval t-test. Note result generally integer, perfectly fine use R, however ever need round integer (e.g. use t-table), must always rounded .Additionally, standard error sample mean difference, time used -interval testing equations :\\[\n\\se(\\bar x_1-\\bar x_2)=\\sqrt{\\frac{s_1^2}{n_1}+\\frac{s_2^2}{n_2}}\n\\]hand, confidence interval equation :\\[\n\\text{$C\\%$ $(1\\!-\\!\\alpha)$ interval}~=~(\\bar x_1-\\bar x_2)~\\pm~t_{\\alpha/2,\\,\\nu}\\cdot\\sqrt{\\frac{s_1^2}{n_1}+\\frac{s_2^2}{n_2}},~\\text{ }\n\\]\\(\\bar x_1-\\bar x_2\\) point estimate true difference \\(\\mu_1-\\mu_2\\),\\(\\bar x_1-\\bar x_2\\) point estimate true difference \\(\\mu_1-\\mu_2\\),\\(t_{\\alpha/2,\\,\\nu}\\) \\(\\alpha\\)-level t-distribution critical value \\(\\nu\\) degrees freedom,\\(t_{\\alpha/2,\\,\\nu}\\) \\(\\alpha\\)-level t-distribution critical value \\(\\nu\\) degrees freedom,\\(\\se(\\bar x_1-\\bar x_2)=\\sqrt{s_1^2/n_1+s_2^2/n_2}\\) standard error difference sample means.\\(\\se(\\bar x_1-\\bar x_2)=\\sqrt{s_1^2/n_1+s_2^2/n_2}\\) standard error difference sample means.similar fashion, Welch t-test, test following hypotheses:\\[\nH_0:\\mu_1-\\mu_2~=~d\\,~~~~~~~\\\\\n~~~~~~~\\,H_a:\\mu_1-\\mu_2~<,\\,\\ne,\\,\\text{}>~d\n\\]test statistic computed :\\[\nt_\\obs=\\frac{(\\bar x_1-\\bar x_2)-d}{\\sqrt{\\frac{s_1^2}{n_1}+\\frac{s_2^2}{n_2}}}~\\sim~T_\\nu\n\\]follows t-distribution \\(\\nu\\) degrees freedom.Continuing lizards example earlier, let’s compute 95% confidence interval true difference \\(\\mu_1-\\mu_2\\), also run \\(\\alpha=0.05\\) level test see indeed living lizards tended longer horn lengths. Recall earlier data summary:Let’s start computing \\(\\nu\\):Now can compute 95% confidence interval:hypothesis test, makes sense choose \\(~H_0\\!:\\!\\mu_1\\!=\\!\\mu_2~\\) vs \\(~H_a\\!:\\!\\mu_1\\!<\\!\\mu_2~\\). , can compute test statistic p-value:seems like killed lizards significantly shorter horns alive ones. can also check using t.test(), simply give vectors function appropriate additional arguments necessary.","code":"\nlizards_summary# A tibble: 2 × 4\n  survival     n  mean    sd\n  <chr>    <int> <dbl> <dbl>\n1 Killed      30  22.0  2.71\n2 Living     154  24.3  2.63\nn1 <- lizards_summary$n[1]  ; n2 <- lizards_summary$n[2]\ns1 <- lizards_summary$sd[1] ; s2 <- lizards_summary$sd[2]\ndf <- (s1^2/n1+s2^2/n2)^2 / ((s1^2/n1)^2/(n1-1)+(s2^2/n2)^2/(n2-1))\ndf[1] 40.37206\n-diff(lizards_summary$mean) + c(-1,1) * qt(0.975, df) * sqrt(s1^2/n1+s2^2/n2)[1] -3.381912 -1.207092\nt_obs <- (-diff(lizards_summary$mean)-0)/sqrt(s1^2/n1+s2^2/n2)\nt_obs[1] -4.26337\npt(t_obs, df)[1] 5.888765e-05\n# extract two samples as vectors\nkilled <- lizards %>% filter(survival == \"Killed\") %>% pull(hornLength)\nliving <- lizards %>% filter(survival == \"Living\") %>% pull(hornLength)\n\n# 95% confidence interval\nt.test(killed, living, conf.level=0.95)\n    Welch Two Sample t-test\n\ndata:  killed and living\nt = -4.2634, df = 40.372, p-value = 0.0001178\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -3.381912 -1.207092\nsample estimates:\nmean of x mean of y \n 21.98667  24.28117 \n# hypothesis test statistic\nt.test(killed, living, mu=0, alternative=\"less\")\n    Welch Two Sample t-test\n\ndata:  killed and living\nt = -4.2634, df = 40.372, p-value = 5.889e-05\nalternative hypothesis: true difference in means is less than 0\n95 percent confidence interval:\n      -Inf -1.388469\nsample estimates:\nmean of x mean of y \n 21.98667  24.28117 "},{"path":"means.html","id":"two-sample-t-test","chapter":"13 Means","heading":"13.4.3 Two-sample T-test","text":"now slightly old-fashioned alternative Welch method two-sample T-test, also sometimes referred equal variance T-test, since requires additional assumption \\(\\sigma_1=\\sigma_2\\). course doesn’t mean \\(s_1\\), \\(s_2\\) must exactly equal sample, just close (usually ’re within factor 2 , .e. \\(0.5<s_1/s_2<2\\) considered plausible).previously mentioned, modern statistical zeitgeist, ’s generally preferred default Welch test unless ’s clear foregone conclusion \\(\\sigma_1=\\sigma_2\\). However, two-sample T-test still appear often enough texts, sources, interview problems, collective consciousness data scientists think ’s worth brief discussion.time degree freedom equation much simpler:\\[\n\\nu=n_1+n_2-2\n\\]However, gain simplicity therein lost complex formula standard error sample mean difference:\\[\n\\se(\\bar x_1-\\bar x_2)=s_p\\sqrt{\\frac1{n_1}+\\frac1{n_2}}~,~~\\text{}\\\\\n~~\\text{“pooled” SD}~s_p=\\sqrt{\\frac{s^2_1(n_1-1)+s^2_2(n_2-1)}{n_1+n_2-2}}\n\\]formula \\(s_p\\) essentially adds corresponding sum-squared-differences observation sample mean \\(s^2_i(n_i-1)\\), re-dividing combined degrees freedom across samples \\(n_1+n_2-2\\), taking root get combined “pooled” SD estimate across samples, makes sense since assumed \\(\\sigma_1=\\sigma_2\\), doesn’t make sense two separate SD estimates samples.hand, confidence interval equation :\\[\n\\text{$C\\%$ $(1\\!-\\!\\alpha)$ interval}~=~(\\bar x_1-\\bar x_2)~\\pm~t_{\\alpha/2,\\,\\nu}\\cdot s_p\\sqrt{\\frac1{n_1}+\\frac1{n_2}}\n\\]test statistic \\(~H_0\\!:\\!\\mu_1\\!-\\!\\mu_2\\!=\\!d~\\) vs \\(~H_a\\!:\\!\\mu_1\\!-\\!\\mu_2\\!<,\\ne,>\\!d~\\) :\\[\nt_\\obs=\\frac{(\\bar x_1-\\bar x_2)-d}{s_p\\sqrt{\\frac1{n_1}+\\frac1{n_2}}}~\\sim~T_\\nu\n\\]follows t-distribution \\(\\nu\\) degrees freedom.Continuing lizards example, recall data summary:\\(s_1\\), \\(s_2\\) well within factor two , can apply two-sample t-test without concern. , let’s compute 95% confidence interval \\(\\alpha=0.05\\) level hypothesis test see living lizards longer horns.can check work t.test() , time setting var.equal = TRUE force equal variance version test:Everything agrees content.","code":"\nlizards_summary# A tibble: 2 × 4\n  survival     n  mean    sd\n  <chr>    <int> <dbl> <dbl>\n1 Killed      30  22.0  2.71\n2 Living     154  24.3  2.63\n# compute the degrees of freedom and pooled SD\ndf <- n1+n2-2\ndf[1] 182\nsp <- sqrt((s1^2*(n1-1)+s2^2*(n2-1))/df)\nsp[1] 2.643476\n# 95% confidence interval\n-diff(lizards_summary$mean) + c(-1,1) * qt(0.975, df) * sp * sqrt(1/n1+1/n2)[1] -3.335402 -1.253602\n# test statistic & p-value\nt_obs <- (-diff(lizards_summary$mean)-0) / (sp*sqrt(1/n1+1/n2))\nt_obs[1] -4.349358\npt(t_obs, df)[1] 1.135079e-05\n# 95% confidence interval\nt.test(killed, living, conf.level=0.95, var.equal=TRUE)\n    Two Sample t-test\n\ndata:  killed and living\nt = -4.3494, df = 182, p-value = 2.27e-05\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -3.335402 -1.253602\nsample estimates:\nmean of x mean of y \n 21.98667  24.28117 \n# hypothesis test\nt.test(killed, living, mu=0, alternative=\"less\", var.equal=TRUE)\n    Two Sample t-test\n\ndata:  killed and living\nt = -4.3494, df = 182, p-value = 1.135e-05\nalternative hypothesis: true difference in means is less than 0\n95 percent confidence interval:\n      -Inf -1.422321\nsample estimates:\nmean of x mean of y \n 21.98667  24.28117 "},{"path":"regression.html","id":"regression","chapter":"14 Regression","heading":"14 Regression","text":"Regression broad class methods predictor variables used predict numeric response. class, primarily focus simple linear regression, single numeric predictor used model linear.","code":""},{"path":"regression.html","id":"correlation-coefficient","chapter":"14 Regression","heading":"14.1 Correlation coefficient","text":"First, brief aside correlation coefficient. Suppose sample pairs observations \\(x_i\\), \\(y_i\\) drawn \\(X\\), \\(Y\\) pair independent pairs. Note however assumption independence made variables, different pairs.Similar statistics like mean variance, theoretical—.e. population—sample versions correlation coefficient definition. One underlying true value observe real dataset. population correlation coefficient \\(\\rho\\) defined :\\[\n\\rho=\\frac{\\e\\big[(X-\\mu_X)(Y-\\mu_Y)\\big]}{\\sigma_X\\sigma_Y}\n\\]numerator, also called covariance \\(X\\) \\(Y\\), represents much average \\(X\\) \\(Y\\) change together compared means, denominator normalizes product standard deviations.Compare sample correlation coefficient \\(r\\) defined :\\[\nr=\\frac{\\frac1{n-1}\\sum(x_i-\\bar x)(y_i-\\bar y)}{s_xs_y}\n\\]Note similar definitions \\(s_x\\), \\(s_y\\), numerator also use \\(n-1\\) instead \\(n\\) averaging. Also note since involve kind averaging, LLN assures us \\(r\\\\rho\\) sample size increases.’s easy show \\(\\rho\\), \\(r\\) always belong range \\([-1,1]\\), .e. correlation coefficient must always absolute value \\(\\le1\\).shows series simulated jointly-normal \\(x_i\\), \\(y_i\\) observations drawn \\(X\\), \\(Y\\) mean 0 SD 1 various population correlation coefficient values:, several things clear:correlation coefficient’s sign indicates direction trend\nPositive \\(+\\) sign indicates positive relationship (slope \\(>0\\))\nNegative \\(-\\) sign indicates negative relationship (slope \\(<0\\))\ncorrelation coefficient’s sign indicates direction trendPositive \\(+\\) sign indicates positive relationship (slope \\(>0\\))Positive \\(+\\) sign indicates positive relationship (slope \\(>0\\))Negative \\(-\\) sign indicates negative relationship (slope \\(<0\\))Negative \\(-\\) sign indicates negative relationship (slope \\(<0\\))correlation coefficient’s absolute value indicates “tight” points \nAbsolute value close 1 indicates points tight around line\ncorrelation coefficient’s absolute value indicates “tight” points areAbsolute value close 1 indicates points tight around lineCorrelation equal close 0 indicates little relationship (flat cloud points)Correlation equal close 0 indicates little relationship (flat cloud points)","code":"\nlibrary(mvtnorm) ; library(plotly) ; n <- 200\ncor_dfs <- do.call(rbind, lapply(\n  c(-20:20/20), \\(r) rmvnorm(n,c(0,0), matrix(c(1,r,r,1), ncol=2)) %>% round(3) %>% \n    as.data.frame %>% setNames(c(\"x\",\"y\")) %>% cbind(r=r))) %>% arrange(r, x)\nplot_ly(cor_dfs, type=\"scatter\", x=~x, y=~y, frame=~r, mode=\"markers\", height=600) %>%\n  config(displayModeBar=FALSE) %>% animation_opts(frame=100, transition=0) %>% \n  layout(title = list(text=\"Simulated X,Y with various correlation coefficients r\", x=.15),\n         xaxis = list(range=list(-3,3)), yaxis = list(range=list(-3,3)),\n         margin = list(l=0, r=10, b=135, t=50, automargin=FALSE), dragmode=FALSE) %>%\n  animation_slider(currentvalue = list(font = list(color=\"#444444\")))"},{"path":"regression.html","id":"caveats","chapter":"14 Regression","heading":"14.1.1 Caveats","text":"correlation coefficient major caveat: measures linear component relationship. relationship non-linear, may useful !even clear, correlation coefficient can used check linearity! ’s important plot data first make sure ’s linear directly jumping regression.famous example know Anscombe’s quartet, built-R. set 4 \\(x_i\\), \\(y_i\\) datasets, can checked mean SD \\(x\\) \\(y\\), well correlation coefficient \\(0.816\\).yet, plotted ’s clear 4 extremely different datasets, one actually appropriate linear regression model:even extreme example, ’s 4 datasets cooked , 0 correlation. , one actually appropriate linear regression model:demonstrate correlation used determine linear regression appropriate. can used assess strength relationship ’ve already visually checked dataset fact linear plotting!","code":"\nanscombe   x1 x2 x3 x4    y1   y2    y3    y4\n1  10 10 10  8  8.04 9.14  7.46  6.58\n2   8  8  8  8  6.95 8.14  6.77  5.76\n3  13 13 13  8  7.58 8.74 12.74  7.71\n4   9  9  9  8  8.81 8.77  7.11  8.84\n5  11 11 11  8  8.33 9.26  7.81  8.47\n6  14 14 14  8  9.96 8.10  8.84  7.04\n7   6  6  6  8  7.24 6.13  6.08  5.25\n8   4  4  4 19  4.26 3.10  5.39 12.50\n9  12 12 12  8 10.84 9.13  8.15  5.56\n10  7  7  7  8  4.82 7.26  6.42  7.91\n11  5  5  5  8  5.68 4.74  5.73  6.89\nanscombe %>%\n  pivot_longer(everything(), names_to=c(\".value\",\"set\"), names_pattern=\"(.)(.)\") %>% \n  group_by(set) %>% summarize(xbar = mean(x), ybar = mean(y), sx = sd(x),\n                              sy = sd(y), r = cor(x,y))# A tibble: 4 × 6\n  set    xbar  ybar    sx    sy     r\n  <chr> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 1         9  7.50  3.32  2.03 0.816\n2 2         9  7.50  3.32  2.03 0.816\n3 3         9  7.5   3.32  2.03 0.816\n4 4         9  7.50  3.32  2.03 0.817\nanscombe %>%\n  pivot_longer(everything(), names_to=c(\".value\",\"set\"), names_pattern=\"(.)(.)\") %>% \n  mutate(set = paste(\"set\",set)) %>% \n  ggplot(aes(x=x,y=y)) + geom_point() + geom_smooth(method=lm, se=F) +\n  facet_wrap(~set, scales=\"free\") + ggtitle(\"Anscombe's quartet datasets, each r ≈ 0.816\")\n# pick seed where rmvnorm generates dataset with especially low r\nset.seed(841)\nn <- 12\nrmvnorm(n,c(0,0),matrix(c(1,0,0,1),ncol=2)) %>% as.data.frame %>% setNames(c(\"x1\",\"y1\")) %>% \n  add_column(\n    x2 = cos(seq(0, 2*pi*(1-1/n), length.out=n)),\n    y2 = sin(seq(0, 2*pi*(1-1/n), length.out=n)),\n    x3 = seq(-1, 1, length.out=n),\n    y3 = x3^2-1,\n    x4 = x3,\n    y4 = abs(sin(x4*pi))) %>%\n  pivot_longer(everything(), names_to=c(\".value\",\"set\"), names_pattern=\"(.)(.)\") %>% \n  mutate(set = case_match(set, \"1\"~\"set 1: no correlation\", \"2\"~\"set 2: circle\",\n                          \"3\"~\"set 3: parabola\", \"4\"~\"set 4: McDonald's\")) %>% \n  group_by(set) %>% mutate(x = (x-mean(x))/sd(x), y = (y-mean(y))/sd(y)) %>% \n  ggplot(aes(x=x,y=y)) + geom_point() + geom_smooth(method=lm, se=F) +\n  facet_wrap(~set, scales=\"free\") + ggtitle(\"Additional quartet of datasets, each r = 0\")"},{"path":"regression.html","id":"correlation-in-r","chapter":"14 Regression","heading":"14.1.2 Correlation in R","text":"R, sample correlation coefficient \\(r\\) computed using cor() function, takes two vectors \\(x_i\\), \\(y_i\\) observations. order input matter.additional optional arguments like use can specify observations use missing values, method can set compute related statistics Kendall’s Spearman’s correlation coefficients beyond scope (method , default, called Pearson’s correlation coefficient).","code":"\n# continuing with set 1 from Anscombe's quartet\ncor(anscombe$x1, anscombe$y1)[1] 0.8164205\n# of course can also use inside tidyverse\nanscombe %>% summarize(r = cor(x1, y1))          r\n1 0.8164205\n# manual computation if desired\nanscombe %>%\n  summarize(r = sum((x1-mean(x1))*(y1-mean(y1)))/(length(x1)-1)/sd(x1)/sd(y1))          r\n1 0.8164205"},{"path":"regression.html","id":"model-notation-4","chapter":"14 Regression","heading":"14.2 Model notation","text":"Now, let’s move actual modeling notation used. , let’s suppose draw sample \\(n\\) pairs \\((x_i,y_i)\\) population variables \\(X,Y\\), suppose ’ve visually identified linear regression model appropriate, based graphically assessing degree linearity, well possibly checking correlation coefficient.theoretical linear model equation following:\\[\ny_i=\\beta_0+\\beta_1x_i+\\ep_i\n\\]\\(x_i\\), \\(y_i\\) actual data points sample,\\(x_i\\), \\(y_i\\) actual data points sample,\\(\\beta_0\\) true intercept line,\\(\\beta_0\\) true intercept line,\\(\\beta_1\\) true slope line,\\(\\beta_1\\) true slope line,\\(\\ep_i\\) errors point, model random fluctuations inherent every measurement. assumed distributed \\(\\ep_i\\sim\\n(0,\\sigma)\\), .e. modeled :\nNormally distributed, \nMean zero,\nConstant variance, \nIndependent one another.\n\\(\\ep_i\\) errors point, model random fluctuations inherent every measurement. assumed distributed \\(\\ep_i\\sim\\n(0,\\sigma)\\), .e. modeled :Normally distributed, withMean zero,Constant variance, andIndependent one another.point, ’s important take note following:statement model equation implicitly involves assumption underlying process \\(X,Y\\) truly linear. , linearity assumption can checked correlation coefficient; must visually assessed plotting data.statement model equation implicitly involves assumption underlying process \\(X,Y\\) truly linear. , linearity assumption can checked correlation coefficient; must visually assessed plotting data., \\(\\beta_0\\), \\(\\beta_1\\) represent true underlying intercept & slope values generate data (assuming data fact linear), best fit line based sample., \\(\\beta_0\\), \\(\\beta_1\\) represent true underlying intercept & slope values generate data (assuming data fact linear), best fit line based sample.\\(x_i\\), \\(y_i\\), \\(\\ep_i\\) \\(\\) indices since vary point, \\(\\beta_0\\), \\(\\beta_1\\) indices, since points supposed fall ONE line true slope intercept.\\(x_i\\), \\(y_i\\), \\(\\ep_i\\) \\(\\) indices since vary point, \\(\\beta_0\\), \\(\\beta_1\\) indices, since points supposed fall ONE line true slope intercept.\\(x_i\\) actually treated constants, just like \\(\\beta_0\\), \\(\\beta_1\\), means variable right hand side \\(\\ep_i\\).\\(x_i\\) actually treated constants, just like \\(\\beta_0\\), \\(\\beta_1\\), means variable right hand side \\(\\ep_i\\).assumptions \\(\\ep_i\\) mean \\(y_i\\) points distributed normally regression line. words, within every vertical slice along line, points follow normal distribution along y-axis.assumptions \\(\\ep_i\\) mean \\(y_i\\) points distributed normally regression line. words, within every vertical slice along line, points follow normal distribution along y-axis.","code":""},{"path":"regression.html","id":"simulated-demo","chapter":"14 Regression","heading":"14.2.1 Simulated demo","text":"quick demo setup notation, suppose pick true coefficients \\(\\beta_0=-3\\), \\(\\beta_1=0.5\\). Using , let’s generate \\(n=10\\) observations \\(x_i\\) uniformly sampled \\([10,20]\\), \\(\\sigma=2\\).Using values, can simulate one possible sample \\(x_i\\), \\(y_i\\) values drawn \\(X,Y\\) :","code":"\n# define values for simulation demo\nb0 <- -3 ; b1 <- 0.5 ; n <- 10 ; m <- 10 ; M <- 20 ; sigma <- 2\n# generate dataset for demo\nx <- runif(n, m, M)\ne <- rnorm(n, 0, sigma)\ny <- b0 + b1*x + e\n# pick a convenient point in quadrant IV as example\ni <- which.min((scale(x)[,1]-.5)^2+(scale(y)[,1])^2)\n\nggplot(tibble(x,y), aes(x=x,y=y)) + geom_point() +\n  geom_segment(aes(color=\"yi\",x=x[i],y=0,yend=y[i]),linewidth=.7) + \n  geom_segment(aes(color=\"xi\",y=y[i],x=10,xend=x[i]),linewidth=.7) + \n  geom_segment(aes(color=\"εi\",x=x[i],y=y[i],yend=(b0+b1*x[i])),linewidth=.7) + \n  geom_point(aes(color=\"i-th point\",x=x[i],y=y[i]),size=3,shape=\"square\") + \n  stat_smooth(aes(color=\"best fit line\"),method=lm,se=F,linewidth=1,linetype=\"dashed\",fullrange=T) +\n  geom_abline(aes(color=\"true line\", slope=b1, intercept=b0),linewidth=1,key_glyph=\"path\") + \n  scale_x_continuous(breaks=seq(10,20,2), labels=seq(10,20,2),\n                     limits=c(10,20), expand=0, minor_breaks=10:20) + \n  scale_y_continuous(breaks=seq(0,10,1), labels=seq(0,10,1),\n                     limits=c(0,10), expand=0, minor_breaks=NULL) + \n  scale_color_manual(breaks=c(\"best fit line\",\"true line\",\"i-th point\",\"εi\",\"yi\",\"xi\"),\n                     values=c(\"#7570B3FF\",\"#1B9E77FF\",\"#666666FF\",\"#E6AB02FF\",\"#A6761DFF\",\"#E7298AFF\")) + \n  theme(legend.key.width=unit(2.1,\"line\")) +\n  labs(color = \"Legend\", title = \"Simulated linear model demo\")"},{"path":"regression.html","id":"fitting-process","chapter":"14 Regression","heading":"14.2.2 Fitting process","text":"Without going much detail, typical fitting method, called Ordinary Least Squares (OLS) involves finding line minimizes sum squared errors vs data points.Suppose “test ” hypothetical line coefficients \\(\\beta_0',\\beta_1'\\). sum squared errors vs data points, also called loss function \\(\\ell(\\beta_0',\\beta_1')\\), defined :\\[\n\\ell(\\beta_0',\\beta_1')=\\sum_{=1}^n(y_i-(\\beta_0'+\\beta_1'x_i))^2\n\\]Note function \\(\\beta_0',\\beta_1'\\), .e. can “test ” different possible values coefficients, evaluated “loss” guess—.e. “bad” guess —plugging function get sum-squared-errors.Visually, looks like taking vertical error bars data point “test” line building square one side, adding areas square get total sum-squared-errors. , best fit line line minimizes sum-squared-errors:find best fit line, solve local minimum loss function, know partial derivatives respect \\(\\beta_0',\\beta_1'\\) must vanish:\\[\n\\text{solve}\\quad\n\\frac{\\partial\\ell(\\beta_0',\\beta_1')}{\\partial\\beta_0'}=0\n\\quad\\text{}\\quad\n\\frac{\\partial\\ell(\\beta_0',\\beta_1')}{\\partial\\beta_1'}=0\n\\]extremely difficult solve omit details.","code":"\nbh1 <- cor(x,y)*sd(y)/sd(x)\nbh0 <- mean(y)-bh1*mean(x)\nggplot(tibble(x,y), aes(x=x,y=y)) +\n  geom_rect(aes(xmin=x,xmax=x+(y-(x*bh1+bh0))*if_else(x>16.5|y>1&y<3,-1,1),\n                ymin=y,ymax=x*bh1+bh0,color=\"squared errors\"),alpha=.1) + \n  geom_segment(aes(yend=x*bh1+bh0, color=\"errors\"),linewidth=1) +\n  stat_smooth(aes(color=\"best fit line\"),method=lm,se=F,fullrange=T,linewidth=1) + \n  geom_point(size=2) +\n  scale_x_continuous(breaks=seq(10,20,2), labels=seq(10,20,2),\n                     limits=c(10,20), expand=0, minor_breaks=10:20) + \n  scale_y_continuous(breaks=seq(0,10,1), labels=seq(0,10,1),\n                     limits=c(0,10), expand=0, minor_breaks=NULL) + \n  scale_color_manual(breaks=c(\"best fit line\",\"errors\",\"squared errors\"),\n                     values=c(\"#7570B3FF\",\"#E6AB02FF\",\"#666666FF\")) +\n  labs(color = \"Legend\", title = \"Least squares visualized for best fit line\")"},{"path":"regression.html","id":"best-fit-coefficients","chapter":"14 Regression","heading":"14.3 Best fit coefficients","text":"One can show best fit coefficients \\(\\hat\\beta_0\\), \\(\\hat\\beta_1\\) minimize loss function :\\[\n\\hat\\beta_1~=~r\\cdot\\frac{s_y}{s_x}~~~~\\\\\n\\hat\\beta_0~=~\\bar y-\\hat\\beta_1\\bar x\n\\]Note purely functions sample statistics \\(\\bar x\\), \\(\\bar y\\), \\(s_x\\), \\(s_y\\), \\(r\\), straightforward compute.Also note second equation implies following key fact: best fit line always passes point \\((\\bar x,\\bar y)\\), since point clearly satisfied \\(\\bar y=\\hat\\beta_0+\\hat\\beta_1\\bar x\\).Another way thinking , best fit intercept \\(\\hat\\beta_0\\) determined automatically seeing line slope \\(\\hat\\beta_1\\) goes \\((\\bar x,\\bar y)\\) intersects y-axis.can shown theoretically best fit coefficients \\(\\hat\\beta_0\\), \\(\\hat\\beta_1\\) defined unbiased, .e. systematic deviation true value, also least variance (among linear estimators), .e. sense “precise” estimation true values \\(\\beta_0\\), \\(\\beta_1\\), sample size grows, ’ll \\(\\hat\\beta_0\\\\beta_0\\) \\(\\hat\\beta_1\\\\beta_1\\).44","code":""},{"path":"regression.html","id":"data-example-1","chapter":"14 Regression","heading":"14.4 Data example","text":"first data example, let’s example historic ice cream consumption dataset provided Kadiyala.45 data, found icecream.csv reports ice cream consumption per capita pints vs mean temperature Farenheit series 4-week periods 1951-53 region Michigan.can see dataset looks quite linear. can also compute correlation:signs suggest good candidate linear regression.","code":"\n# set comment=\"#\" to ignore description lines at top of file,\n# then select main columns of interest\nicecream <- read_csv(\"https://bwu62.github.io/stat240-revamp/data/icecream.csv\",\n                     comment=\"#\") %>% select(consumption, temperature)\nicecream# A tibble: 29 × 2\n  consumption temperature\n        <dbl>       <dbl>\n1       0.386          41\n2       0.374          56\n3       0.393          63\n4       0.425          68\n5       0.406          69\n# ℹ 24 more rows\nggplot(icecream,aes(x=temperature, y=consumption)) + geom_point() +\n  geom_smooth(method=lm, se=F) + labs(y=\"Consumption (pints per capita)\",\n  x=\"Mean temperature (F)\", title=\"Ice cream consumption vs temperature\")\nicecream %>% summarize(r = cor(consumption, temperature))# A tibble: 1 × 1\n      r\n  <dbl>\n1 0.786"},{"path":"regression.html","id":"fit-manually","chapter":"14 Regression","heading":"14.4.1 Fit manually","text":"can use expressions shown earlier manually compute best fit slope intercept \\(\\hat\\beta_1\\), \\(\\hat\\beta_0\\).","code":"\n# <<- is a dirty R trick to globally assign a variable from anywhere\nicecream %>% summarize(\n  xbar = (xbar <<- mean(temperature)),\n  ybar = (ybar <<- mean(consumption)),\n  sx   = (sx   <<- sd(temperature)),\n  sy   = (sy   <<- sd(consumption)),\n  r    = (r    <<- cor(consumption, temperature)),\n  b1   = (b1   <<- r * sy / sx),\n  b0   = (b0   <<- ybar - b1 * xbar))# A tibble: 1 × 7\n   xbar  ybar    sx     sy     r      b1    b0\n  <dbl> <dbl> <dbl>  <dbl> <dbl>   <dbl> <dbl>\n1  48.3 0.353  16.2 0.0563 0.786 0.00273 0.221\n# check fit by manually plotting best fit line (cross added at xbar, ybar)\nlibrary(latex2exp)\nggplot(icecream, aes(x=temperature, y=consumption)) + geom_point() +\n  geom_point(aes(x=xbar, y=ybar), shape=3, size=6) +\n  geom_abline(slope=b1, intercept=b0, color=\"red\") +\n  labs(x=\"Mean temperature (F)\", y=\"Consumption (pints per capita)\", title=TeX(\n    \"Ice cream consumption vs temperature (cross shows $\\\\bar{x},\\\\bar{y}$)\"))"},{"path":"regression.html","id":"fit-with-lm","chapter":"14 Regression","heading":"14.5 Fit with lm()","text":"course can also fit using R. function use lm(), short Linear Model. many advanced arguments, minimally always specified:formula, 1st argument, expects R “formula”, special object created ~ character specifies dependency structure variables (see page details)\nsimple linear regression, formula response ~ predictor response predictor column names \\(Y\\) \\(X\\) variables data frame\nformula, 1st argument, expects R “formula”, special object created ~ character specifies dependency structure variables (see page details)simple linear regression, formula response ~ predictor response predictor column names \\(Y\\) \\(X\\) variables data framedata, 2nd argument, expects data frame variablesdata, 2nd argument, expects data frame variablesYou can also use two vectors directly formula specification instead using columns data frame, generally recommended since precludes certain additional useful features road. best results, always use lm() columns data frame!formula specification always order response ~ predictor, .e. Y ~ X, way around; order variables interchangeable!’s lot unpack, ’ll get later, now can see results match manual computation, \\(\\hat\\beta_0=0.2207\\), \\(\\hat\\beta_1=0.002734\\).","code":"\n# fit the model, saving the output object which will be useful later\nlm_icecream <- lm(consumption ~ temperature, data = icecream)\nlm_icecream\nCall:\nlm(formula = consumption ~ temperature, data = icecream)\n\nCoefficients:\n(Intercept)  temperature  \n   0.220727     0.002735  \n# print a more complete summary output of the model\nsummary(lm_icecream)\nCall:\nlm(formula = consumption ~ temperature, data = icecream)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.061253 -0.023111 -0.000007  0.025382  0.053154 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.2207275  0.0210905  10.466 5.30e-11 ***\ntemperature 0.0027346  0.0004144   6.598 4.44e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.03547 on 27 degrees of freedom\nMultiple R-squared:  0.6172,    Adjusted R-squared:  0.6031 \nF-statistic: 43.54 on 1 and 27 DF,  p-value: 4.444e-07"},{"path":"regression.html","id":"model-validation","chapter":"14 Regression","heading":"14.6 Model validation","text":"important step stage model validation, means check fit makes sense nothing unusual unexpected happening fit. usually done analyzing residuals fit.","code":""},{"path":"regression.html","id":"residuals","chapter":"14 Regression","heading":"14.6.1 Residuals","text":"Recall best fit line determined minimizing sum-squared-errors. best fit line found, taking difference \\(y_i\\) best-fit predictions \\(\\hat y_i\\) gives sample estimate errors, call \\(\\hat\\ep_i\\) residuals fit defined:\\[\n\\hat\\ep_i~=~y_i-\\hat y_i~=~y_i-\\left(\\hat\\beta_0+\\hat\\beta_1x_i\\right)\n\\]\\(\\hat\\ep_i\\) treated sample \\(\\ep_i\\) remember assumed distribution \\(\\ep_i\\sim\\n(0,\\sigma)\\).reason assumption made simply pure random noise generally normal, mean 0, constant variance, model correct correctly captured underlying linear relationship, whatever leftover fit look like pure noise!assumptions usually checked examining residual vs fitted QQ plots fit.","code":""},{"path":"regression.html","id":"plotting-residuals","chapter":"14 Regression","heading":"14.6.2 Plotting residuals","text":"Let’s first compute model’s residuals data point:can directly plotted predicted \\(\\hat y_i\\) QQ plot using methods ’ve learned (exercise left reader). Alternatively, ’s also rare slick base R shortcut :plots can see residuals appear normal, 0-mean, constant-variance, everything looks good. examples bad residual plots, see page page.R, “generic” function like plot() summary() used objects special classes like output lm(), actually redirect call functions plot.lm() summary.lm(), specific implementations generic function class.means need search specific functions’ help pages want additional details work.","code":"\n# recall we still have best fit coefficients b1, b0 defined from earlier\nicecream <- icecream %>% mutate(residuals = consumption - (b0 + b1*temperature))\nicecream# A tibble: 29 × 3\n  consumption temperature   residuals\n        <dbl>       <dbl>       <dbl>\n1       0.386          41  0.0532    \n2       0.374          56  0.000135  \n3       0.393          63 -0.00000701\n4       0.425          68  0.0183    \n5       0.406          69 -0.00341   \n# ℹ 24 more rows\n# check our residuals using R's built-in resid() function\nicecream$residuals [1]  5.315410e-02  1.351619e-04 -7.009962e-06  1.832001e-02 -3.414586e-03\n [6] -5.447620e-02 -6.053782e-02 -6.125347e-02 -3.923453e-02 -3.035777e-02\n[11] -1.129615e-02  6.173041e-03  2.076547e-02 -1.211130e-02  9.869758e-03\n[16] -1.200701e-02  5.238163e-02  2.538163e-02 -1.794539e-02 -4.280322e-02\n[21] -2.204969e-02 -2.311130e-02 -2.423453e-02  3.143845e-02  1.170385e-02\n[26]  4.803087e-02  4.315410e-02  5.307355e-02  4.125839e-02\nresid(lm_icecream) %>% unname  # remove names from vector (added by default) [1]  5.315410e-02  1.351619e-04 -7.009962e-06  1.832001e-02 -3.414586e-03\n [6] -5.447620e-02 -6.053782e-02 -6.125347e-02 -3.923453e-02 -3.035777e-02\n[11] -1.129615e-02  6.173041e-03  2.076547e-02 -1.211130e-02  9.869758e-03\n[16] -1.200701e-02  5.238163e-02  2.538163e-02 -1.794539e-02 -4.280322e-02\n[21] -2.204969e-02 -2.311130e-02 -2.423453e-02  3.143845e-02  1.170385e-02\n[26]  4.803087e-02  4.315410e-02  5.307355e-02  4.125839e-02\n# plot residuals (since residuals=response-predicted, predicted=response-residuals)\nggplot(icecream, aes(x=temperature, y=consumption)) +\n  geom_segment(aes(yend=consumption-residuals), color=\"#E6AB02FF\") + geom_point() +\n  geom_abline(slope=b1, intercept=b0, color=\"red\") +\n  labs(title=\"Ice cream consumption vs temperature (with residuals)\",\n  x=\"Mean temperature (F)\", y=\"Consumption (pints per capita)\")\nplot(lm_icecream, 1)\nplot(lm_icecream, 2)"},{"path":"regression.html","id":"inference","chapter":"14 Regression","heading":"14.7 Inference","text":"course, inference also possible regression. can easily construct confidence intervals \\(\\hat\\beta_0\\), \\(\\hat\\beta_1\\) perform hypothesis tests \\(\\beta_0\\), \\(\\beta_1\\).Generally, common inference questions ask data shows significant linear relationship, usually answered either computing interval \\(\\hat\\beta_1\\), trying reject \\(\\beta_1=0\\), since affirm significance \\(\\beta_1x_i\\) term linear model equation. Much less commonly, inference may also done intercept.","code":""},{"path":"regression.html","id":"t-distribution-1","chapter":"14 Regression","heading":"14.7.1 T-distribution","text":"cases, either slope intercept, process inference straightforward: use T-distribution \\(\\nu=n-2\\) degrees freedom, corresponding standard errors:\\[\n\\text{(intercept)}~~\\se(\\hat\\beta_0)=\\hat\\sigma\\cdot\\sqrt{\\frac1n+\\frac{\\bar x^2}{s_x^2(n-1)}}~,\\\\\n\\text{(slope)}~~\\se(\\hat\\beta_1)=\\hat\\sigma\\cdot\\sqrt{\\frac1{s_x^2(n-1)}}~,\\,~\\\\\n~~~~~~~\\text{}~~\\hat\\sigma=\\sqrt{\\frac1{n-2}\\sum\\hat\\ep_i^2}\n\\]example dataset, can computed :purposes 240, can always read summary() output, either provided easily computed R whenever regression inference involved., ’s output summary()Std. Error column, can see \\(\\se(\\hat\\beta_0)=0.02109\\), \\(\\se(\\hat\\beta_1)=0.0004144\\).Std. Error column, can see \\(\\se(\\hat\\beta_0)=0.02109\\), \\(\\se(\\hat\\beta_1)=0.0004144\\)., next Residual standard error, can see \\(\\hat\\sigma=0.03547\\), estimate \\(\\sigma\\), .e. standard deviation \\(\\ep_i\\) errors., next Residual standard error, can see \\(\\hat\\sigma=0.03547\\), estimate \\(\\sigma\\), .e. standard deviation \\(\\ep_i\\) errors.Additionally, see \\(\\nu=n-2=27\\) degrees freedom needed t-distribution.Additionally, see \\(\\nu=n-2=27\\) degrees freedom needed t-distribution.","code":"\n# recall we still have xbar, sx globally defined from earlier,\n# use <<- again to globally define sigma, s0, s1 by formulae above\nicecream %>% summarize(\n  n = (n <<- n()),\n  sigma = (sigma <<- sqrt(1/(n-2) * sum(residuals^2))),\n  s0 = (s0 <<- sigma * sqrt(1/n + xbar^2/(sx^2*(n-1)))),\n  s1 = (s1 <<- sigma * sqrt(1/(sx^2*(n-1))))\n) %>% data.frame  # convert to base R data frame, which prints more digits by default   n      sigma         s0           s1\n1 29 0.03546869 0.02109051 0.0004144333\nsummary(lm_icecream)\nCall:\nlm(formula = consumption ~ temperature, data = icecream)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.061253 -0.023111 -0.000007  0.025382  0.053154 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.2207275  0.0210905  10.466 5.30e-11 ***\ntemperature 0.0027346  0.0004144   6.598 4.44e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.03547 on 27 degrees of freedom\nMultiple R-squared:  0.6172,    Adjusted R-squared:  0.6031 \nF-statistic: 43.54 on 1 and 27 DF,  p-value: 4.444e-07"},{"path":"regression.html","id":"hypothesis-test-2","chapter":"14 Regression","heading":"14.7.2 Hypothesis test","text":"Now, can inference desire. Suppose want see significant linear relationship. words, test following hypotheses:\\[\nH_0:\\beta_1=0\\\\\nH_a:\\beta_1\\ne0\n\\]compute t-statistic p-value similar one-sample t-test:\\[\nt_\\obs~=~\\frac{\\hat\\beta_1-0}{\\se(\\hat\\beta_1)}~=~\\frac{0.002735}{0.0004144}~=~6.598~\\sim~T_{n-2}\n\\]match summary, can conclude ’s strong evidence suggest significant linear relationship exists.","code":"\n# recall we have b1, s1 defined as the slope and its standard error\n# since t_obs is positive, take twice the upper tail are\n2 * (1 - pt(b1/s1, df=n-2))[1] 4.443997e-07"},{"path":"regression.html","id":"confidence-interval-3","chapter":"14 Regression","heading":"14.7.3 Confidence interval","text":"can also compute 95% confidence interval \\(\\beta_1\\)\\[\n\\text{$C\\%$ $(1\\!-\\!\\alpha)$ interval}~=~\\hat\\beta_1~\\pm~t_{\\alpha/2,\\,n-2}\\cdot\\se(\\hat\\beta_1)\n\\]can check confint() function:, everything agrees! 95% confidence interval true slope \\(\\beta_1\\) \\((0.0019,0.0036)\\).","code":"\nb1 + c(-1,1) * qt(0.975, df=n-2) * s1[1] 0.001884249 0.003584943\nconfint(lm_icecream, level = 0.95)                  2.5 %      97.5 %\n(Intercept) 0.177453305 0.264001622\ntemperature 0.001884249 0.003584943"},{"path":"regression.html","id":"prediction","chapter":"14 Regression","heading":"14.8 Prediction","text":"model, can use make predictions new \\(X\\) observations.Two things keep mind :new \\(X\\) values within (perhaps slightly beyond) range observed \\(x_i\\) values considered viable candidates prediction model. Far outside \\(x_i\\), simply data linear model may valid therein.new \\(X\\) values within (perhaps slightly beyond) range observed \\(x_i\\) values considered viable candidates prediction model. Far outside \\(x_i\\), simply data linear model may valid therein.Linear models intended predict average \\(Y\\) given \\(X\\), individual \\(Y\\) observation. Individual observations course subject additional \\(\\ep_i\\) errors.Linear models intended predict average \\(Y\\) given \\(X\\), individual \\(Y\\) observation. Individual observations course subject additional \\(\\ep_i\\) errors.Suppose new period time mean temperature 30°F. model predict period’s average ice cream consumption pints per capita?can also check predict(), generic function calles predict.lm(). Using function, can easily make several new predictions . Suppose wish see average consumption predictions temperatures 30,40,…,70°F:","code":"\n# again, recall we have b0, b1 defined as the fitted intercept and slope\nb0 + b1*30[1] 0.3027653\n# predict() expects lm object + data frame with new X values in column of same name\npredict(lm_icecream, newdata = tibble(temperature=3:7*10))        1         2         3         4         5 \n0.3027653 0.3301113 0.3574573 0.3848032 0.4121492 \nggplot(icecream, aes(x=temperature, y=consumption)) + geom_point() +\n  geom_abline(slope=b1, intercept=b0, color=\"red\") + \n  geom_point(data=cbind(x=3:7*10,y=predict(lm_icecream,list(temperature=3:7*10))),\n             aes(x,y,color=\"new predictions\\nat 30,40,…,70\"),size=3,shape=\"square\") +\n  scale_color_manual(values=c(\"blue\")) +\n  labs(title=\"Ice cream consumption vs temperature (with new predictions)\",\n  x=\"Mean temperature (F)\", y=\"Consumption (pints per capita)\")"},{"path":"regression.html","id":"more-intervals","chapter":"14 Regression","heading":"14.8.1 More intervals","text":"can also augment new prediction one two new kinds confidence intervals:confidence interval mean response (confused coefficient confidence intervals discussed previously) interval predicted mean response given new predictor value.confidence interval mean response (confused coefficient confidence intervals discussed previously) interval predicted mean response given new predictor value.prediction interval single observation, interval response value one newly observed data point.prediction interval single observation, interval response value one newly observed data point.Another way thinking confidence interval interval best fit line computed data, whereas prediction interval interval cloud data points., shouldn’t surprising prediction interval typically much wider confidence interval, since needs account additional point--point variation dataset.Suppose new observation \\(X=x_\\new\\). use t-distribution \\(\\nu=n-2\\) degrees freedom, following standard error formulae confidence & prediction intervals:\\[\n\\se(\\hat y)=\\hat\\sigma\\sqrt{\\frac1n+\\frac{(x_\\new-\\bar x)^2}{s_x^2(n-1)}}~~~~~~~~~~~~~~~~\\\\\n\\se(y_\\new)=\\hat\\sigma\\sqrt{1+\\frac1n+\\frac{(x_\\new-\\bar x)^2}{s_x^2(n-1)}}~,~\\text{}\n\\]\\(\\se(\\hat y)\\) confidence interval standard error \\(\\hat y=\\hat\\beta_0+\\hat\\beta_1x_\\new\\), model prediction \\(x_\\new\\),\\(\\se(\\hat y)\\) confidence interval standard error \\(\\hat y=\\hat\\beta_0+\\hat\\beta_1x_\\new\\), model prediction \\(x_\\new\\),\\(\\se(y_\\new)\\) prediction interval standard error \\(y_\\new\\), hypothetical unobserved responsed correspond \\(x_\\new\\),\\(\\se(y_\\new)\\) prediction interval standard error \\(y_\\new\\), hypothetical unobserved responsed correspond \\(x_\\new\\),\\(\\hat\\sigma\\) residual standard error previously defined.\\(\\hat\\sigma\\) residual standard error previously defined., following formulae confidence & prediction intervals:\\[\n\\text{$C\\%$ $(1\\!-\\!\\alpha)$ confidence interval}\\,=~\\hat y\\:\\pm~t_{\\alpha/2,\\,n-2}\\cdot\\se(\\hat y)~~~~~\\\\\n\\text{$C\\%$ $(1\\!-\\!\\alpha)$ prediction interval}~=~\\hat y~\\pm~t_{\\alpha/2,\\,n-2}\\cdot\\se(y_\\new)\n\\]Suppose wish compute 95% confidence prediction intervals new observation 30°F. know earlier corresponds predicted \\(\\hat y=0.3027\\).can interpret two follows:95% confident true mean ice cream consumption mean temperature 30°F 0.28 0.32 pints per capita.95% confident true mean ice cream consumption mean temperature 30°F 0.28 0.32 pints per capita.95% confident ice cream consumption single new observation mean temperature 30°F 0.23 0.38 pints per capita.95% confident ice cream consumption single new observation mean temperature 30°F 0.23 0.38 pints per capita., can check R. easy setting interval argument predict() function (remember calls predict.lm()). Set argument \"confidence\" confidence interval \"prediction\" prediction interval. can course easily accommodate multiple predictions; let’s try temperatures 30,40,…,70°F:Finally, let’s visualize 95% confidence prediction intervals \\(x_\\new\\) along ","code":"\n# 95% confidence interval for new X at 30\n(b0+b1*30) + c(-1,1) * qt(0.975,n-2) * sigma*sqrt(1/n+(30-xbar)^2/(sx^2*(n-1)))[1] 0.2821262 0.3234045\n# 95% prediction interval for new X at 30\n(b0+b1*30) + c(-1,1) * qt(0.975,n-2) * sigma*sqrt(1+1/n+(30-xbar)^2/(sx^2*(n-1)))[1] 0.2271196 0.3784111\n# 95% confidence interval for new X at 30,40,...,70\npredict(lm_icecream, newdata=tibble(temperature=3:7*10), interval=\"confidence\")        fit       lwr       upr\n1 0.3027653 0.2821262 0.3234045\n2 0.3301113 0.3148475 0.3453751\n3 0.3574573 0.3438701 0.3710445\n4 0.3848032 0.3680444 0.4015620\n5 0.4121492 0.3893080 0.4349904\n# 95% prediction interval for new X at 30,40,...,70\npredict(lm_icecream, newdata=tibble(temperature=3:7*10), interval=\"prediction\")        fit       lwr       upr\n1 0.3027653 0.2271196 0.3784111\n2 0.3301113 0.2557521 0.4044705\n3 0.3574573 0.2834240 0.4314905\n4 0.3848032 0.3101228 0.4594836\n5 0.4121492 0.3358732 0.4884252\nggplot(icecream, aes(x=temperature, y=consumption)) + geom_point() +\n  geom_abline(aes(slope=b1,intercept=b0,color=\"\\nbest fit line\\n\"),key_glyph=\"path\",linewidth=1) +\n  stat_function(aes(color=\"confidence\\ninterval\"),linewidth=1,linetype=\"dashed\",\n                fun=\\(x)b0+x*b1+qt(0.975,n-2)*sigma*sqrt(1/n+(x-xbar)^2/(sx^2*(n-1)))) +\n  stat_function(linewidth=1,color=\"#7570b3\",linetype=\"dashed\",\n                fun=\\(x)b0+x*b1-qt(0.975,n-2)*sigma*sqrt(1/n+(x-xbar)^2/(sx^2*(n-1)))) +\n  stat_function(aes(color=\"\\nprediction\\ninterval\\n\"),linewidth=1.3,linetype=\"dotted\",\n                fun=\\(x)b0+x*b1+qt(0.975,n-2)*sigma*sqrt(1+1/n+(x-xbar)^2/(sx^2*(n-1)))) +\n  stat_function(linewidth=1.3,color=\"#1b9e77\",linetype=\"dotted\",\n                fun=\\(x)b0+x*b1-qt(0.975,n-2)*sigma*sqrt(1+1/n+(x-xbar)^2/(sx^2*(n-1)))) +\n  scale_color_manual(breaks=c(\"\\nbest fit line\\n\",\"confidence\\ninterval\",\"\\nprediction\\ninterval\\n\"),\n                     values=c(\"#d95f02\",\"#7570b3\",\"#1b9e77\")) + \n  scale_x_continuous(expand=0) + theme(legend.key.width=unit(2.2,\"line\")) +\n  labs(x=\"Mean temperature (F)\", y=\"Consumption (pints per capita)\",\n       title=\"Ice cream consumption vs temperature (with confidence/prediction intervals)\")"},{"path":"regression.html","id":"power-law","chapter":"14 Regression","heading":"14.9 Power law","text":"Linear regression can also sometimes used fit non-linear relationships, can linearized transformation. lots examples , e.g. exponential, polynomial, etc. ’re going examine just one example: power law relations.Power laws defined following equation \\(Y\\) \\(X\\)\\[\nY = aX^b\n\\]\\(\\), \\(b\\) constants, \\(>0\\) must positive \\(b\\) can positive negative. Note \\(b=1\\) ’d simply line generally considered power law, \\(b=0\\) ’d constant also considered power law.Power laws quite common across many disciplines worth examining closely. Note applying log transformation sides—valid since ’s equation—results following linear relationship \\(\\log(Y)\\) \\(\\log(X)\\)\\[\n\\log(Y)=\\log(aX^b)=\\log()+b\\cdot\\log(X)\n\\]\\(\\log()\\), \\(b\\) become intercept slope. Note course valid \\(X,Y>0\\) always true, since otherwise log return real value. Also note choice base doesn’t matter, free choose convenient base (usually base 10 preferred interpretability).power law relation often confirmed plotting data log-log plot, \\(Y\\) \\(X\\) log transformed, check linearity, way ’d check linearity graphically fitting ordinary linear model. , logged variables can simply regressed, can estimate coefficients inference just like linear model.difference note power law significance usually checked testing \\(b=1\\) vs \\(b\\ne1\\), since power one results simple linear equation, typically interpreted significant power law relationship.","code":""},{"path":"regression.html","id":"data-example-2","chapter":"14 Regression","heading":"14.9.1 Data example","text":"allometry, ’s found basal metabolic rate (BMR) body mass power law relationship; known Kleiber’s law. Empirically, law found exponent ¾, can explained modeling scaling rate capillary count vs total blood volume circulatory systems different sizes46. table BMR values 637 different mammalian species provided McNab (2008) saved bmr.csv.","code":"\n# set comment=\"#\" to ignore description lines at top of file\nbmr <- read_csv(\"https://bwu62.github.io/stat240-revamp/data/bmr.csv\", comment=\"#\")\nbmr# A tibble: 637 × 11\n  species    mass_g bmr_kjph food  climate habitat substrate torpor islands mountains\n  <chr>       <dbl>    <dbl> <chr> <chr>   <chr>   <chr>     <chr>  <chr>   <chr>    \n1 Tachyglos…   2140     5.63 A/T   T/TR    W       B         N      N       N        \n2 Zaglossus…  10300    24.4  W     TR      M       B         N      Y       Y        \n3 Ornithorh…   1300     9.4  V/IV  T       FW      AQ        N      N       N        \n4 Caluromys…    357     4.09 F     TR      M       TR        N      N       N        \n5 Marmosa m…     13     0.38 I/F   T       M       T/TR      Y      N       N        \n# ℹ 632 more rows\n# ℹ 1 more variable: reference <chr>"},{"path":"regression.html","id":"log-log-plot","chapter":"14 Regression","heading":"14.9.2 Log-log plot","text":"Let’s consider BMR response body mass predictor. can begin attempting make simple BMR vs mass plot ordinary linear scale:Looking plot, clearly just ordinary linear relationship. looks like points clustered together bottom corner. may tempting think just zoom issue, can fix zooming corner, wrong easily disproven:see pattern. surely zooming fix ?can clearly see now level zoom issue ; data simply evenly spread linear scale. Turns data follow power law always exhibit fractal-like self-similarity effect. correct approach use log-log plot instead:log-log scale, data following correlation:Remember log-log plot, equal steps along axes represent constant multiples values, constant additions, axes go \\(0\\) infinite left/side \\(\\infty\\) infinite right/top side!","code":"\nggplot(bmr,aes(x=mass_g,y=bmr_kjph)) + geom_point() + labs(\n  title = \"BMR vs body mass of various mammalian species\",\n  x = \"Body mass (g)\", y = \"BMR (kJ/h)\")\n# zoom into bottom left corner (note some points are cut off)\nggplot(bmr,aes(x=mass_g,y=bmr_kjph)) + geom_point() +\n  xlim(c(0,1e5)) + ylim(c(0,700)) + labs(\n    title = \"BMR vs body mass of various mammalian species (zoomed in)\",\n    x = \"Body mass (g)\", y = \"BMR (kJ/h)\")\n# zoom even MORE into bottom left corner (many points are now cut off)\nggplot(bmr,aes(x=mass_g,y=bmr_kjph)) + geom_point() +\n  xlim(c(0,1000)) + ylim(c(0,15)) + labs(\n    title = \"BMR vs body mass of various mammalian species (even more zoomed in)\",\n    x = \"Body mass (g)\", y = \"BMR (kJ/h)\")\n# log-log plot + best fit line; can be done by directly logging variables,\n# or modifying the scale, which I prefer for ease & interpretability\nggplot(bmr,aes(mass_g,bmr_kjph)) + geom_point() + geom_smooth(method=lm,se=F) +\n  scale_x_log10(labels=scales::label_log()) + scale_y_log10() + labs(\n    title = \"Log-log plot of BMR vs body mass of various mammalian species\",\n    x = \"Body mass (g)\", y = \"BMR (kJ/h)\")\nbmr %>% summarize(r = cor(log10(mass_g),log10(bmr_kjph)))# A tibble: 1 × 1\n      r\n  <dbl>\n1 0.979"},{"path":"regression.html","id":"fitting","chapter":"14 Regression","heading":"14.9.3 Fitting","text":"Fitting simple, can simply add necessary function transformation lm() formula expression. , free choose favorite base, commonly base 10 used interpretability., can see intercept, represents \\(\\log_{10}()\\), fitted \\(\\hat\\beta_0\\approx-1.1548\\) means \\(\\approx0.06843\\); slope, represents exponent \\(b\\), fitted \\(\\hat\\beta_1\\approx0.7191\\). go back remake last super zoomed linear-scale plot raw data fitted power law added , can see clearly also fits raw data quite well:","code":"\nlm_bmr <- lm(log10(bmr_kjph) ~ log10(mass_g), data=bmr) \nsummary(lm_bmr)\nCall:\nlm(formula = log10(bmr_kjph) ~ log10(mass_g), data = bmr)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.50474 -0.10540 -0.01011  0.10209  0.54159 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   -1.15488    0.01543  -74.84   <2e-16 ***\nlog10(mass_g)  0.71908    0.00599  120.04   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1662 on 635 degrees of freedom\nMultiple R-squared:  0.9578,    Adjusted R-squared:  0.9577 \nF-statistic: 1.441e+04 on 1 and 635 DF,  p-value: < 2.2e-16\n# use coef() to extract and define best fit coefficients\ncoef(lm_bmr)  (Intercept) log10(mass_g) \n   -1.1548790     0.7190833 \nb0 <- coef(lm_bmr)[1] ; b1 <- coef(lm_bmr)[2]\n# replot zoomed in linear-scale plot with fitted power law\nggplot(bmr,aes(x=mass_g,y=bmr_kjph)) + geom_point() +\n  geom_function(fun=\\(x) 10^b0 * x^b1, color=\"blue\", linewidth=1) +\n  xlim(c(0,1000)) + ylim(c(0,15)) + labs(\n    title = \"BMR vs body mass of various mammalian species (zoomed + power law)\",\n    x = \"Body mass (g)\", y = \"BMR (kJ/h)\")"},{"path":"regression.html","id":"model-validation-1","chapter":"14 Regression","heading":"14.9.4 Model validation","text":"fitting, ’s important remember check residuals assumption violations. make usual residual v. fitted QQ plots:appear exact perhaps slight non-linearity non-normality residuals, seems limited handful points near edges X-range. Generally linear regression shouldn’t sensitive slight deviations assumptions, probably significant concern. Overall, model appears fairly well fit proceed inference.","code":"\nplot(lm_bmr, 1)\nplot(lm_bmr, 2)"},{"path":"regression.html","id":"inference-1","chapter":"14 Regression","heading":"14.9.5 Inference","text":"Let’s look summary output:Recall t-statistics p-values presented therein always correspond testing vs 0, whereas power law typically wish test slope vs 1. Therefore, assess significance must compute test statistics. hypotheses :\\[\nH_0:b=1\\\\\nH_a:b\\ne1\n\\]\\(b=\\beta_1\\) true exponent power law true slope log-log line. , test statistic \\[\nt_\\obs~=~\\frac{\\hat\\beta_1-1}{\\se(\\hat\\beta_1)}~=~\\frac{0.7191-1}{0.005990}~=~-46.896~\\sim~T_{n-2}\n\\]Remember since large sample (n=637), statistic essentially normally distributed, z-score -46.896 infinitesimally small, can conclude power law model statistically much better fit vs linear alternative.Perhaps interesting question , especially since Kleiber’s law provides theoretical exponent ¾ model, can test data adherence model. Let’s retest following hypotheses:\\[\nH_0:b=\\tfrac34\\\\\nH_a:b\\ne\\tfrac34\n\\]gives us following statistic:\\[\nt_\\obs~=~\\frac{\\hat\\beta_1-\\frac34}{\\se(\\hat\\beta_1)}~=~\\frac{0.7191-\\frac34}{0.005990}~=~-5.1612~\\sim~T_{n-2}\n\\]results following p-value:Interestingly, large sample actually able detect small statistically significant deviation Kleiber’s law \\(Y=aX^{3/4}\\). can also formalize computing 95% confidence intervals coefficients:95% confident true exponent \\((0.707,0.731)\\).later expanded Nature paper Ballesteros et al (2018) alternative model proposed fitted combining power-law component well pure linear component, appeared result better fit.","code":"\nsummary(lm_bmr)\nCall:\nlm(formula = log10(bmr_kjph) ~ log10(mass_g), data = bmr)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.50474 -0.10540 -0.01011  0.10209  0.54159 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   -1.15488    0.01543  -74.84   <2e-16 ***\nlog10(mass_g)  0.71908    0.00599  120.04   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1662 on 635 degrees of freedom\nMultiple R-squared:  0.9578,    Adjusted R-squared:  0.9577 \nF-statistic: 1.441e+04 on 1 and 635 DF,  p-value: < 2.2e-16\n2 * pt(-5.1612, df=635)[1] 3.28589e-07\nconfint(lm_bmr, level = 0.95)                   2.5 %     97.5 %\n(Intercept)   -1.1851821 -1.1245759\nlog10(mass_g)  0.7073203  0.7308462"},{"path":"regression.html","id":"bonus-fitting-arbitrary-functions","chapter":"14 Regression","heading":"14.10 Bonus: fitting arbitrary functions","text":"slight bonus topic, let’s also explore Ballesteros’s proposed alternative model see can fit . paper, power-law + linear combination form:\\[\nY = aX+bX^{2/3}\n\\]fitted data compared pure power law fit:R, arbitrary relations can fit data using nls() function, stands Nonlinear Least Squares extends least squares method general function forms. , ’s closed form solutions, optimal fit found via numeric optimization (.e. basically fancy guessing--checking). optional start=list(...) argument can set pick initial starting guesses parameters.can check fit previous pure power law extracting coefficients new model plotting side side:time, must manually plot residuals view properly:can also compute likelihood-based confidence intervals confint(), though time ’re based t-distributions:’s immediately obvious model superior; seem fit data reasonably well.","code":"\nnls_bmr <- nls(bmr_kjph ~ a*mass_g + b*mass_g^(2/3), data=bmr, start=list(a=1,b=1))\nsummary(nls_bmr)\nFormula: bmr_kjph ~ a * mass_g + b * mass_g^(2/3)\n\nParameters:\n   Estimate Std. Error t value Pr(>|t|)    \na 1.746e-03  4.182e-05   41.76   <2e-16 ***\nb 9.988e-02  5.293e-03   18.87   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 48.55 on 635 degrees of freedom\n\nNumber of iterations to convergence: 1 \nAchieved convergence tolerance: 5.902e-06\ncoef(nls_bmr)          a           b \n0.001746319 0.099879075 \na <- coef(nls_bmr)[1] ; b <- coef(nls_bmr)[2]\nggplot(bmr,aes(mass_g,bmr_kjph)) + geom_point(size=.7,alpha=.5) +\n  geom_smooth(aes(color=\"power law\"), method=lm,se=F, linewidth=.7) +\n  geom_function(aes(color=\"power-linear\\ncombination\"),\n                fun=\\(x)a*x+b*x^(2/3), linewidth=.7) +\n  scale_x_log10(labels=scales::label_log()) + scale_y_log10() +\n  scale_color_manual(breaks=c(\"power law\",\"power-linear\\ncombination\"),\n                     values=c(\"blue\",\"red\")) +\n  labs(x = \"Body mass (g)\", y = \"BMR (kJ/h)\",\n       title = \"Log-log plot BMR vs body mass of various mammalian species\")\nbmr <- bmr %>% mutate(log_residuals = log10(bmr_kjph)-log10(a*mass_g+b*mass_g^(2/3)))\nggplot(bmr, aes(mass_g,log_residuals)) + geom_point() +\n  scale_x_log10()\nqqnorm(bmr$log_residuals)\nconfint(nls_bmr, level = 0.95)        2.5%       97.5%\na 0.00166419 0.001828441\nb 0.08948605 0.110272440"},{"path":"datasets.html","id":"datasets","chapter":"A Datasets","heading":"A Datasets","text":"page contains updating/processing scripts additional info datasets used course, well brief discussions chosen. Datasets ordered order appearance notes.Also, since mostly keep track datasets processing scripts, ’s meticulously formatted like rest notes, e.g. lines code kept ~80 characters, comments may brief, ’re . may also use advanced syntax additional packages. Read discretion.","code":"\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(xlsx)\nlibrary(magrittr)"},{"path":"datasets.html","id":"datasets-list","chapter":"A Datasets","heading":"A.1 List of datasets","text":"’s convenient list dataset files generated. Note files used notes!. primarily record keeping purposes. Also note files may automatically open download prompt others may . force download, right click file link choose “Save link ”.bmr.csvenrollment.csveruptions_recent.csveruptions_recent.delimeruptions_recent.tsveruptions_recent.xlsxfertility.csvfertility_meta.csvfertility_raw.csvicecream.csvlizards.tsvpenguins.csvthoracic.csvAlternatively, can also run following line, download files current working directory. ’s recommended first set working directory appropriate place running , e.g. data/ directory STAT240/ course folder.","code":"\nlapply(readLines(\"https://bwu62.github.io/stat240-revamp/data_list.txt\"),\n       \\(.)download.file(.,basename(.)))"},{"path":"datasets.html","id":"eruptions","chapter":"A Datasets","heading":"A.2 Eruptions","text":"introducing reading CSVs, wanted dataset 4 data types discussed (numeric, logical, character, date) interesting enough, without many columns rows, without problems add complexity since ’re just starting . volcanic eruptions dataset (specifically “Holocene Eruptions”) table seemed fit bill nicely.","code":""},{"path":"datasets.html","id":"load-raw-data","chapter":"A Datasets","heading":"A.2.1 Load raw data","text":"","code":"\n# load html source code\neruptions_raw <- read_html(\"https://volcano.si.edu/volcanolist_countries.cfm?country=United%20States\") %>% \n  # extract table code\n  html_nodes(xpath=\"//table[@title='Holocene Eruptions']\") %>% \n  # convert to data frame\n  html_table(header=T,na.strings=c(\"Uncertain\",\"Unknown\",\"[Unknown]\")) %>% \n  # remove list wrapper\n  .[[1]] %>% \n  # remove unnecessary evidence column\n  select(-Evidence) %>% \n  # make names nice\n  set_names(c(\"volcano\",\"start\",\"stop\",\"confirmed\",\"vei\"))"},{"path":"datasets.html","id":"process-data","chapter":"A Datasets","heading":"A.2.2 Process data","text":"","code":"\neruptions <- eruptions_raw %>% \n  mutate(\n    volcano = str_replace(volcano,\"°\",\"°\"),\n    # convert confirmed? column to logical\n    confirmed = if_else(replace_na(confirmed,\"NA\")==\"Confirmed\",T,F),\n    # OLD PLAN: replace continuing eruptions with today's date (continuation last validated 6/15/25)\n    # NEW PLAN: to avoid problems with unvalidated stop dates, as well as too many NAs at the top of the file\n    #           which interferes with introduction of data frames chapter, remove unstopped and into-2025 eruptions below\n    stop = if_else(str_detect(stop,\"continu\"),format(today(),\"%Y %b %e\"),stop,missing=stop)\n  ) %>%\n  # extract date error to new column\n  separate(start,c(\"start\",\"start_error\"),\"±\") %>% \n  separate(stop,c(\"stop\",\"stop_error\"),\"±\") %>% \n  mutate(\n    # fix a few names\n    volcano = volcano %>% str_replace(\"Asuncion\",\"Asunción\") %>% str_replace(\"Pajaros\",\"Pájaros\") %>% str_replace(\"Kilauea\",\"Kīlauea\"),\n    # parse error time string to number of days\n    start_error = as.duration(start_error)/ddays(1),\n    stop_error = as.duration(stop_error)/ddays(1),\n    # extract start year since some earlier eruptions are missing month/day\n    start_year = str_extract(start,\"(\\\\d{4,5})\"),\n    stop_year = str_extract(stop,\"(\\\\d{4,5})\"),\n    # parse start year, adding - if BCE\n    start_year = as.numeric(start_year) * if_else(str_detect(start,\"BCE\"),-1,1),\n    stop_year = as.numeric(stop_year) * if_else(str_detect(stop,\"BCE\"),-1,1),\n    # extract start month\n    start_month = str_replace(start,\".*\\\\d{4}\\\\s([:alpha:]{3}).*\",\"\\\\1\"),\n    stop_month = str_replace(stop,\".*\\\\d{4}\\\\s([:alpha:]{3}).*\",\"\\\\1\"),\n    start = start %>% str_replace_all(\"\\\\[|\\\\]|\\\\(.*?\\\\)\",\"\") %>% str_extract(\"^\\\\s?\\\\d+\\\\s\\\\w+\\\\s\\\\d+\") %>% ymd,\n    stop = stop %>% str_replace_all(\"\\\\[|\\\\]|\\\\(.*?\\\\)\",\"\") %>% str_extract(\"^\\\\s?\\\\d+\\\\s\\\\w+\\\\s\\\\d+\") %>% ymd,\n    # if missing date but has month, use middle day +- half-month error\n    # first, compute number of days in each month\n    start_mdays = days_in_month(ymd(str_c(start_year,start_month,\"1\"))),\n    stop_mdays = days_in_month(ymd(str_c(stop_year,stop_month,\"1\"))),\n    # next, if start/stop NA but month exists, set error as half of number of days in month rounded up, then set no error (NA) as 0\n    start_error = if_else(is.na(start) & !is.na(start_month) & is.na(start_error),ceiling(start_mdays/2),start_error) %>% replace_na(0),\n    stop_error = if_else(is.na(stop) & !is.na(stop_month) & is.na(stop_error),ceiling(stop_mdays/2),stop_error) %>% replace_na(0),\n    # finally, if start/stop NA but month exists, set start/stop as middle day of month rounded down\n    start = if_else(is.na(start) & !is.na(start_month),ymd(str_c(start_year,start_month,floor(start_mdays/2))),start),\n    stop = if_else(is.na(stop) & !is.na(stop_month),ymd(str_c(stop_year,stop_month,floor(stop_mdays/2))),stop),\n    duration = (stop-start)/ddays(1)\n  ) %>% \n  # remove intermediate rows\n  select(volcano,start,start_error,start_year,stop,stop_error,stop_year,duration,confirmed,vei)\n\n# get just subset for demo\neruptions_recent <- eruptions %>% \n  filter(start_error <= 30, start_year > 2000, stop_year < 2025, !is.na(stop)) %>% \n  select(-contains(\"_\"))"},{"path":"datasets.html","id":"write-out-data","chapter":"A Datasets","heading":"A.2.3 Write out data","text":"","code":"\n# # save complete file too\n# write_csv(eruptions,file=\"data/eruptions.csv\")\n\n# write out to different formats for reading\nwrite_csv(eruptions_recent,file=\"data/eruptions_recent.csv\")\nwrite_tsv(eruptions_recent,file=\"data/eruptions_recent.tsv\")\nwrite_delim(eruptions_recent,file=\"data/eruptions_recent.delim\",delim=\"|\",na=\"\")\neruptions_recent %>% as.data.frame %>% write.xlsx(file=\"data/eruptions_recent.xlsx\",row.names=F,showNA=F)\n\n# originally line below was b/c I wanted to prep example for read_table but turns out\n# its behavior changed recentlyish https://www.tidyverse.org/blog/2021/07/readr-2-0-0/\n# it no longer works well here, and read.table needs to be used instead\n# (alternatively read_fwf from readr also works but that seems beyond scope)\n# but I don't want to confuse students by introducing a mix of readr + base R\n# so quitting this example, need to reevaluate in the future importance\n# of reading whitespace aligned table formats\n\n# eruptions_recent %>% as.data.frame %>% print(print.gap=2,width=1000,row.names=F,right=F) %>% capture.output() %>% \n#   str_replace(\"<NA>\",\"NA  \") %>% str_replace(\"^ *\",'\"') %>% str_replace(\"( {2,})\",'\"\\\\1') %>% str_replace('\"name\"',\"name  \") %>% write_lines(file=\"data/eruptions_recent.txt\")"},{"path":"datasets.html","id":"inspect-data","chapter":"A Datasets","heading":"A.2.4 Inspect data","text":"","code":"\neruptions_recent# A tibble: 75 × 6\n   volcano               start      stop       duration confirmed   vei\n   <chr>                 <date>     <date>        <dbl> <lgl>     <dbl>\n 1 Kīlauea               2024-09-15 2024-09-20        5 TRUE         NA\n 2 Kīlauea               2024-06-03 2024-06-03        0 TRUE         NA\n 3 Atka Volcanic Complex 2024-03-27 2024-03-27        0 TRUE         NA\n 4 Ahyi                  2024-01-01 2024-03-27       86 TRUE         NA\n 5 Kanaga                2023-12-18 2023-12-18        0 TRUE          1\n 6 Ruby                  2023-09-14 2023-09-15        1 TRUE          1\n 7 Shishaldin            2023-07-11 2023-11-03      115 TRUE          3\n 8 Mauna Loa             2022-11-27 2022-12-10       13 TRUE          0\n 9 Ahyi                  2022-11-18 2023-06-11      205 TRUE          1\n10 Kīlauea               2021-09-29 2023-09-16      717 TRUE          0\n# ℹ 65 more rows"},{"path":"datasets.html","id":"palmer-penguins-1","chapter":"A Datasets","heading":"A.3 Palmer penguins","text":"data visualization, wanted feature rich dataset healthy combination numerics characters ready go, easy use, fun & interesting, make good looking plots demos. spent long brainstorming ideas, including scraping additional info volcanoes, wasn’t happy result. , looking inspiration, found Hadley Wickham’s excellent R4DS book Palmer penguins dataset, absolutely perfect.Now, want students continue practicing reading datasets, following code simple extracts dataset therein rewrites .","code":""},{"path":"datasets.html","id":"write-out-data-1","chapter":"A Datasets","heading":"A.3.1 Write out data","text":"","code":"\nlibrary(palmerpenguins)\nwrite_csv(penguins %>% drop_na,\"data/penguins.csv\")"},{"path":"datasets.html","id":"inspect-data-1","chapter":"A Datasets","heading":"A.3.2 Inspect data","text":"","code":"\npenguins# A tibble: 333 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex  \n   <chr>   <chr>              <dbl>         <dbl>             <dbl>       <dbl> <chr>\n 1 Adelie  Torgersen           39.1          18.7               181        3750 male \n 2 Adelie  Torgersen           39.5          17.4               186        3800 fema…\n 3 Adelie  Torgersen           40.3          18                 195        3250 fema…\n 4 Adelie  Torgersen           36.7          19.3               193        3450 fema…\n 5 Adelie  Torgersen           39.3          20.6               190        3650 male \n 6 Adelie  Torgersen           38.9          17.8               181        3625 fema…\n 7 Adelie  Torgersen           39.2          19.6               195        4675 male \n 8 Adelie  Torgersen           41.1          17.6               182        3200 fema…\n 9 Adelie  Torgersen           38.6          21.2               191        3800 male \n10 Adelie  Torgersen           34.6          21.1               198        4400 male \n# ℹ 323 more rows\n# ℹ 1 more variable: year <dbl>"},{"path":"datasets.html","id":"college-enrollment","chapter":"A Datasets","heading":"A.4 College enrollment","text":"also briefly needed nice time series dataset 1 groups demonstrate line plots. Eventually settled table 303.10 National Center Education Statistics (NCES) contains historic college enrollment data, stratified sex.","code":"\nlast.yy <- tryCatch({\n  rvest::read_html(\"https://nces.ed.gov/programs/digest/current_tables.asp\") %>%\n  rvest::html_nodes(xpath=\"//select[@name='quickjump']/option[2]/text()\") %>% \n  as.character %>% as.numeric\n},error = \\(e){\n  (lubridate::year(Sys.Date()-3*30)-1)\n}) %% 100"},{"path":"datasets.html","id":"process-data-1","chapter":"A Datasets","heading":"A.4.1 Process data","text":"","code":"\nenrollment <- \"https://nces.ed.gov/programs/digest/d{last.yy}/tables/dt{last.yy}_303.10.asp\" %>% \n  str_glue %>% \n  read_html %>% \n  html_nodes(xpath=\"//div[@class='nces']/table[1]\") %>% \n  html_table %>% \n  {.[[1]][-3,]} %>% \n  t %>% as.data.frame %>% \n  rownames_to_column %>% \n  unite(\"name\",1:3) %>% \n  column_to_rownames(\"name\") %>% \n  t %>% as.data.frame %>% \n  select(matches(\"Year|(Sex.*(Male|Female))|Nonprofit\",ignore.case=F)) %>% \n  set_names(c(\"year\",\"male\",\"female\",\"nonprofit\")) %>% \n  mutate(year = str_sub(year,1,4)) %>% \n  mutate_all(parse_number) %>% \n  filter(!(is.na(year)&is.na(male)&is.na(female))&year>20,\n         year<year(today())-10|!is.na(nonprofit)) %>% \n  select(-nonprofit) %>% \n  mutate(male = male/1e6, female = female/1e6) %>% \n  pivot_longer(male:female,names_to=\"sex\",values_to=\"enrolled_millions\")"},{"path":"datasets.html","id":"write-out-data-2","chapter":"A Datasets","heading":"A.4.2 Write out data","text":"","code":"\nwrite_csv(enrollment, file=\"data/enrollment.csv\")"},{"path":"datasets.html","id":"inspect-data-2","chapter":"A Datasets","heading":"A.4.3 Inspect data","text":"","code":"\nenrollment# A tibble: 148 × 3\n    year sex    enrolled_millions\n   <dbl> <chr>              <dbl>\n 1  1947 male               1.66 \n 2  1947 female             0.679\n 3  1948 male               1.71 \n 4  1948 female             0.694\n 5  1949 male               1.72 \n 6  1949 female             0.723\n 7  1950 male               1.56 \n 8  1950 female             0.721\n 9  1951 male               1.39 \n10  1951 female             0.711\n# ℹ 138 more rows"},{"path":"datasets.html","id":"fertility-rate","chapter":"A Datasets","heading":"A.5 Fertility rate","text":"advanced data operations section needed something suitable demonstrating grouping, joining, pivoting, hopefully interesting. found World Bank fertility rate dataset quite suitable purpose. dataset presented 2 ways, first fully cleaned version grouping, partially cleaned version joining pivoting.","code":""},{"path":"datasets.html","id":"process-data-2","chapter":"A Datasets","heading":"A.5.1 Process data","text":"","code":"\nif(!dir.exists(\"temp\")) dir.create(\"temp\")\nf <- \"temp/fertility.zip\"\ndownload.file(\"https://api.worldbank.org/v2/en/indicator/SP.DYN.TFRT.IN?downloadformat=csv\",f,mode=\"wb\")\nfiles = unzip(f, list=T)$Name %>% str_subset(\"Indicator\",negate=T)\nunzip(f,files,exdir=\"temp/\")\nfertility_meta <- str_subset(list.files(\"temp/\",full=T),\"^temp/Meta.*API_SP.DYN.TFRT\") %>% read_csv %>% distinct\nskip = str_subset(list.files(\"temp/\",full=T),\"^temp/API_SP.DYN.TFRT\") %>% read_lines %>% \n  str_detect(\"Country Name\") %>% which %>% min %>% subtract(1)\nfertility_raw <- str_subset(list.files(\"temp/\",full=T),\"^temp/API_SP.DYN.TFRT\") %>% read_csv(skip=skip) %>% distinct\n# simplify/shorten some names for convenience\nfertility_meta <- fertility_meta %>% mutate(\n  TableName = TableName %>% \n    str_replace_all(c(\n      \" and the \" = \" & \",\n      \" and \" = \" & \",\n      \", The\" = \"\",\n      \"SAR, China\" = \"\",\n      \"Korea, Rep.\" = \"South Korea\",\n      \"British Virgin Islands\" = \"Virgin Islands\",\n      \"Russian Federation\" = \"Russia\",\n      \" \\\\(.*\" = \"\",\n      \"Slovak Republic\" = \"Slovakia\",\n      \"Iran, Islamic Rep.\" = \"Iran\",\n      \"Brunei Darussalam\" = \"Brunei\",\n      \"Korea, Dem. People's Rep.\" = \"North Korea\",\n      \"Cabo Verde\" = \"Cape Verde\",\n      \"Türkiye\" = \"Turkey\",\n      \"Viet Nam\" = \"Vietnam\",\n      \"Lao PDR\" = \"Laos\",\n      \"Micronesia, Fed. Sts.\" = \"Micronesia\",\n      \"Syrian Arab Republic\" = \"Syria\",\n      \"Kyrgyz Republic\" = \"Kyrgyzstan\",\n      \"Egypt, Arab Rep.\" = \"Egypt\",\n      \"Timor-Leste\" = \"East Timor\",\n      \"Yemen, Rep.\" = \"Yemen\",\n      \"Côte d'Ivoire\" = \"Ivory Coast\"\n    ))\n)\n\n# remove some extra columns with only NAs?\n\nfertility_meta <- fertility_meta %>% select(where(\\(x)mean(is.na(x))<1))\nfertility_raw <- fertility_raw %>% select(where(\\(x)mean(is.na(x))<1))"},{"path":"datasets.html","id":"write-out-raw-data","chapter":"A Datasets","heading":"A.5.2 Write out raw data","text":"","code":"\nwrite_csv(fertility_meta, file=\"data/fertility_meta.csv\")\nwrite_csv(fertility_raw, file=\"data/fertility_raw.csv\")"},{"path":"datasets.html","id":"process-more-tidy-version","chapter":"A Datasets","heading":"A.5.3 Process more (tidy version)","text":"","code":"\nfertility_meta <- fertility_meta %>% filter(!is.na(IncomeGroup)) %>% \n  rename(code = \"Country Code\", country = \"TableName\", region = \"Region\", income_group = \"IncomeGroup\") %>% \n  mutate(income_group = str_replace(income_group,\" income\",\"\")) %>% \n  select(code, country, region, income_group)\n\nfertility <- fertility_raw %>% select(-matches(\"Indicator|Name|^\\\\.\\\\.\")) %>% \n  rename(code = \"Country Code\") %>% inner_join(fertility_meta) %>%\n  pivot_longer(matches(\"^\\\\d+\"),names_to=\"year\",values_to=\"rate\") %>% \n  mutate(income_group = str_replace(income_group,\" income\",\"\")) %>%\n  # mutate(income_group = factor(income_group,ordered = T, levels=c(\n  #   \"Low\", \"Lower middle\", \"Upper middle\", \"High\"))) %>%\n  arrange(country,year)\n# # show pct NA for each country with NAs\n# fertility %>% group_by(country) %>% summarize(pctna = round(100*mean(is.na(rate)))) %>% filter(pctna>0) %>% arrange(-pctna)\n# # show all years for these countries to see pattern of NAs\n# fertility %>% group_by(country) %>% mutate(pctna = 100*mean(is.na(rate))) %>% ungroup %>% \n#   filter(pctna>0) %>% arrange(-pctna) %>% pivot_wider(names_from = year,values_from = rate) %>% View\n\n# based on exploration, let's just drop the small number of country/year combos with NAs for simplicity\nfertility <- fertility %>% drop_na()"},{"path":"datasets.html","id":"write-out-tidy-data","chapter":"A Datasets","heading":"A.5.4 Write out tidy data","text":"","code":"\nwrite_csv(fertility, \"data/fertility.csv\")"},{"path":"datasets.html","id":"inspect-data-3","chapter":"A Datasets","heading":"A.5.5 Inspect data","text":"","code":"\nfertility# A tibble: 13,792 × 6\n   code  country     region     income_group  year  rate\n   <chr> <chr>       <chr>      <chr>        <dbl> <dbl>\n 1 AFG   Afghanistan South Asia Low           1960  7.28\n 2 AFG   Afghanistan South Asia Low           1961  7.28\n 3 AFG   Afghanistan South Asia Low           1962  7.29\n 4 AFG   Afghanistan South Asia Low           1963  7.30\n 5 AFG   Afghanistan South Asia Low           1964  7.30\n 6 AFG   Afghanistan South Asia Low           1965  7.30\n 7 AFG   Afghanistan South Asia Low           1966  7.32\n 8 AFG   Afghanistan South Asia Low           1967  7.34\n 9 AFG   Afghanistan South Asia Low           1968  7.36\n10 AFG   Afghanistan South Asia Low           1969  7.39\n# ℹ 13,782 more rows"},{"path":"datasets.html","id":"thoracic-surgery","chapter":"A Datasets","heading":"A.6 Thoracic Surgery","text":"two proportions section found dataset thoracic surgery outcomes Wroclaw Thoracic Surgery Centre University Wroclaw. Worked perfectly example mind.","code":""},{"path":"datasets.html","id":"process-data-3","chapter":"A Datasets","heading":"A.6.1 Process data","text":"","code":"\ntemp_path <- tempfile()\ndownload.file(\"https://archive.ics.uci.edu/static/public/277/thoracic+surgery+data.zip\", destfile=temp_path)\nunzip(temp_path)\nfilename <- list.files(\".\",\"Tho.*arff\")\nthoracic <- read_csv(filename,skip=21,col_names=c(\n  \"dgn\",\"fvc\",\"fev1\",\"perf\",\"pain\",\"haem\",\"dysp\",\"cough\",\"weak\",\n  \"size\",\"type2dm\",\"mi6\",\"pad\",\"smoker\",\"asthma\",\"age\",\"survive1\")) %>%\n  mutate(survive1 = !survive1) # properly recode survival column\n\n# remove temp file\nunlink(filename)"},{"path":"datasets.html","id":"write-out-data-3","chapter":"A Datasets","heading":"A.6.2 Write out data","text":"","code":"\nwrite_csv(thoracic, \"data/thoracic.csv\")"},{"path":"datasets.html","id":"inspect-data-4","chapter":"A Datasets","heading":"A.6.3 Inspect data","text":"","code":"\nthoracic# A tibble: 470 × 17\n   dgn     fvc  fev1 perf  pain  haem  dysp  cough weak  size  type2dm mi6   pad  \n   <chr> <dbl> <dbl> <chr> <lgl> <lgl> <lgl> <lgl> <lgl> <chr> <lgl>   <lgl> <lgl>\n 1 DGN2   2.88  2.16 PRZ1  FALSE FALSE FALSE TRUE  TRUE  OC14  FALSE   FALSE FALSE\n 2 DGN3   3.4   1.88 PRZ0  FALSE FALSE FALSE FALSE FALSE OC12  FALSE   FALSE FALSE\n 3 DGN3   2.76  2.08 PRZ1  FALSE FALSE FALSE TRUE  FALSE OC11  FALSE   FALSE FALSE\n 4 DGN3   3.68  3.04 PRZ0  FALSE FALSE FALSE FALSE FALSE OC11  FALSE   FALSE FALSE\n 5 DGN3   2.44  0.96 PRZ2  FALSE TRUE  FALSE TRUE  TRUE  OC11  FALSE   FALSE FALSE\n 6 DGN3   2.48  1.88 PRZ1  FALSE FALSE FALSE TRUE  FALSE OC11  FALSE   FALSE FALSE\n 7 DGN3   4.36  3.28 PRZ1  FALSE FALSE FALSE TRUE  FALSE OC12  TRUE    FALSE FALSE\n 8 DGN2   3.19  2.5  PRZ1  FALSE FALSE FALSE TRUE  FALSE OC11  FALSE   FALSE TRUE \n 9 DGN3   3.16  2.64 PRZ2  FALSE FALSE FALSE TRUE  TRUE  OC11  FALSE   FALSE FALSE\n10 DGN3   2.32  2.16 PRZ1  FALSE FALSE FALSE TRUE  FALSE OC11  FALSE   FALSE FALSE\n# ℹ 460 more rows\n# ℹ 4 more variables: smoker <lgl>, asthma <lgl>, age <dbl>, survive1 <lgl>"},{"path":"datasets.html","id":"ice-cream-consumption-data","chapter":"A Datasets","heading":"A.7 Ice cream consumption data","text":"small simple linear regression dataset introduce section, decided much deliberation ice cream consumption dataset referenced Kadiyala’s 1970 paper testing residual independence. original source Hildreth, Lu 1960 can’t find copy online.dataset several advantages: ’s real dataset literature ’s nice linear, small enough easy work effectively show importance t-distribution inference, high enough error variance make output “interesting” show discuss.data processing, just copied pdf csv file.","code":""},{"path":"datasets.html","id":"read-inspect-data","chapter":"A Datasets","heading":"A.7.1 Read & inspect data","text":"","code":"\nicecream <- read_csv(\"data/icecream.csv\", comment=\"#\")\nicecream# A tibble: 29 × 5\n       n consumption price income temperature\n   <dbl>       <dbl> <dbl>  <dbl>       <dbl>\n 1     1       0.386 0.27      78          41\n 2     2       0.374 0.282     79          56\n 3     3       0.393 0.277     81          63\n 4     4       0.425 0.28      80          68\n 5     5       0.406 0.272     76          69\n 6     6       0.344 0.262     78          65\n 7     7       0.327 0.275     82          61\n 8     8       0.288 0.267     79          47\n 9     9       0.269 0.265     76          32\n10    10       0.256 0.277     79          24\n# ℹ 19 more rows"},{"path":"datasets.html","id":"mammalian-bmr-data","chapter":"A Datasets","heading":"A.8 Mammalian BMR data","text":"additional regression topic, needed good power-law dataset, found excellent paper McNab (2008) collecting basal metabolic rates 637 species mammals, known follow power law.Due processing difficulty & time constraints, omitted order & family table insert rows now simply retained genus & species level info. can easily added back later, e.g. using ENA taxonomy database, allow order & family level subgroup comparisons.dataset easy copy minimally preprocess hand.","code":""},{"path":"datasets.html","id":"read-inspect-data-1","chapter":"A Datasets","heading":"A.8.1 Read & inspect data","text":"","code":"\nbmr <- read_csv(\"data/bmr.csv\", comment=\"#\")\nbmr# A tibble: 637 × 11\n   species   mass_g bmr_kjph food  climate habitat substrate torpor islands mountains\n   <chr>      <dbl>    <dbl> <chr> <chr>   <chr>   <chr>     <chr>  <chr>   <chr>    \n 1 Tachyglo…   2140     5.63 A/T   T/TR    W       B         N      N       N        \n 2 Zaglossu…  10300    24.4  W     TR      M       B         N      Y       Y        \n 3 Ornithor…   1300     9.4  V/IV  T       FW      AQ        N      N       N        \n 4 Caluromy…    357     4.09 F     TR      M       TR        N      N       N        \n 5 Marmosa …     13     0.38 I/F   T       M       T/TR      Y      N       N        \n 6 Thylamys…     40     0.86 I     T       M       T/TR      HIB    N       Y        \n 7 Monodelp…    104     1.21 I     TR      X       T         N      N       N        \n 8 Monodelp…    111     1.52 I     TR      M       T         Y      N       N        \n 9 Marmosa …    122     1.96 I/F   TR      M       T         Y      N       N        \n10 Metachir…    336     4.12 I/F   TR      M       T/TR      N      N       N        \n# ℹ 627 more rows\n# ℹ 1 more variable: reference <chr>"},{"path":"cheat-sheets.html","id":"cheat-sheets","chapter":"B Cheat Sheets","heading":"B Cheat Sheets","text":"consolidation “cheat sheets” mentioned notes, well extra mentioned. page intended use quick reference basically entire course without needing dig notes. ’re loosely organized order appearance.","code":""},{"path":"cheat-sheets.html","id":"rstudio-ide","chapter":"B Cheat Sheets","heading":"B.1 Rstudio IDE","text":"Rstudio cheat sheet detailed description every button Rstudio , well additional tips like common keyboard shortcuts workflow suggestions.","code":""},{"path":"cheat-sheets.html","id":"base-r","chapter":"B Cheat Sheets","heading":"B.2 Base R","text":"Matt Baggott’s R Reference Card v2.0 nice complete one-stop-shop R’s built-functions.IQSS’s Base R Cheat Sheet Alexey Shipunov’s One Page R Reference Card slightly shorter curated, offer nice, tighter set critical R commands, along useful examples syntax.slightly longer complete reference manual R, especially details R works different object types data structures, Emmanuel Paradis’s R Beginners may helpful.","code":""},{"path":"cheat-sheets.html","id":"r-markdown-1","chapter":"B Cheat Sheets","heading":"B.3 R Markdown","text":"R Markdown cheat sheet convenient summary everything need know effectively use R Markdown, including basic markdown syntax, common chunk/YAML options, additional styling settings, .’s also slightly longer R Markdown reference guide longer list chunk YAML options.’s also separate Markdown cheat sheet; note basic syntax completely supported, extended syntax supported. Feel free experiment .","code":""},{"path":"cheat-sheets.html","id":"latex","chapter":"B Cheat Sheets","heading":"B.4 \\(\\LaTeX\\)","text":"wish read \\(\\LaTeX\\), start Rong Zhuang’s MathJax cheat sheet David Richeson’s quick guide lots great beginner-friendly examples. slightly complete list symbols, Eric Torrence’s cheat sheet may also useful.","code":""},{"path":"cheat-sheets.html","id":"readr","chapter":"B Cheat Sheets","heading":"B.5 readr","text":"readr cheat sheet good overview read_* functions well example code common arguments.","code":""},{"path":"cheat-sheets.html","id":"lubridate","chapter":"B Cheat Sheets","heading":"B.6 lubridate","text":"first page lubridate cheat sheet useful quickly checking ’re using right date-related operation.","code":""},{"path":"cheat-sheets.html","id":"ggplot2-1","chapter":"B Cheat Sheets","heading":"B.7 ggplot2","text":"ggplot2 cheat sheet great summary every plot type first page, organized type number variables ’s designed visualize. second page, ’s nice list additional plot features, faceting (.e. subplot) options, setting scales, adjusting positions plot elements, changing themes, editing labels/legends, etc.","code":""},{"path":"cheat-sheets.html","id":"dplyr","chapter":"B Cheat Sheets","heading":"B.8 dplyr","text":"dplyr cheat sheet basic rundown common data frame transformation operations, grouped type, along brief helpful examples diagrams. includes everything subsetting rows columns, adding/editing columns, summarizing data frames, grouping operations, joining/binding multiple data frames, etc.","code":""},{"path":"cheat-sheets.html","id":"tidyr","chapter":"B Cheat Sheets","heading":"B.9 tidyr","text":"tidyr cheat shows summary different data tidying operations, cover class. Notably, use NA reshaping operations. Feel free read .","code":""},{"path":"additional-resources.html","id":"additional-resources","chapter":"C Additional Resources","heading":"C Additional Resources","text":"’s page additional loosely organized resources may find useful.Harvard’s online R programming course: https://cs50.harvard.edu/r/2024 info general R programming like conditionals, loops, functions etc.Harvard’s online R programming course: https://cs50.harvard.edu/r/2024 info general R programming like conditionals, loops, functions etc.R-charts: https://r-charts.com fairly comprehensive list plots base R ggplot2, great code examples.R-charts: https://r-charts.com fairly comprehensive list plots base R ggplot2, great code examples.R-statistics.co: https://r-statistics.co great tutorials everything ggplot2 regression time series models cluster computing.R-statistics.co: https://r-statistics.co great tutorials everything ggplot2 regression time series models cluster computing.Introduction Statistical Learning (ISLR): https://www.statlearning.com great high level survey key statistical learning concepts like regression, deep learning, even survival analysis. Code syntax sometimes slightly old-fashioned since uses base R, tidyverse , great intro nonetheless advanced topics.Introduction Statistical Learning (ISLR): https://www.statlearning.com great high level survey key statistical learning concepts like regression, deep learning, even survival analysis. Code syntax sometimes slightly old-fashioned since uses base R, tidyverse , great intro nonetheless advanced topics.added future…","code":""}]
