

# Big picture

I want to start the data modeling section with the following big picture view of what modeling & data science aim to do.

:::{.i96}
```{r,echo=F,fig.width=8,fig.height=4}
# import diagram library
library(diagram)
# set margin and create empty diagram
par(mar=c(0,0,1.5,0),cex.main=1.5)
openplotmat(main = "Data science life cycle")
# set number of elements in each row
nodes = coordinates(c(3,3,5))
# reverse second row boxes order
nodes = nodes[c(1:3,6:4,7:nrow(nodes)),]
# nudge experiment, exploration boxes to right
nodes[2,1] = nodes[2,1] + .02
nodes[5,1] = nodes[5,1] + .02
# shift rows around
nodes[1:3,2] = nodes[1:3,2] + .01
nodes[4:6,2] = nodes[4:6,2] + .04
nodes[7:11,2] = nodes[7:11,2] - .02
# tighen last row boxes
nodes[7:11,1] = mean(nodes[7:11,1]) + (nodes[7:11,1]-mean(nodes[7:11,1]))*.92 - .015
fromto <- matrix(ncol=2,byrow=T,data=c(1,2,2,3,3,4,4,5,5,6,6,7,6,8,6,9,6,10,6,11))
nr <- nrow(fromto)
arrpos <- matrix(ncol=2,nrow=nr)
for (i in 1:nr)
  arrpos[i, ] <- straightarrow(to=nodes[fromto[i,2],],from=nodes[fromto[i,1],],lwd=2,arr.pos=0.57,arr.length=0.5)
curvedarrow(nodes[6,],nodes[1,],curve=-.42,lty=2)
text(sum(nodes[1:2,1]*c(.45,.55)),nodes[1,2]+.04,"expand",cex=.9)
text(sum(nodes[2:3,1]*c(.45,.55)),nodes[2,2]+.04,"sample",cex=.9)
text(nodes[6,1]-.082,nodes[6,2]+.14,"iterate",cex=.9)
textrect(nodes[1,],.127,.08,lab="question of interest\n(testable hypothesis)",shadow.size = 0.005)
textrect(nodes[2,],.088,.08,lab="experiment/\nstudy design",shadow.size = 0.005)
textrect(nodes[3,],.065,.06,lab="dataset",shadow.size = 0.005)
textrect(nodes[4,],.1,.06,lab="cleaning/tidying",shadow.size = 0.005)
textrect(nodes[5,],.108,.08,lab="data exploration/\nvisualization",shadow.size = 0.005)
textrect(nodes[6,],.07,.065,lab="modeling",shadow.size = 0.005)
textrect(nodes[7,],.07,.07,lab="parameter\nestimates",shadow.size = 0.005)
textrect(nodes[8,],.07,.07,lab="hypothesis\ntesting",shadow.size = 0.005)
textrect(nodes[9,],.07,.07,lab="confidence\nintervals",shadow.size = 0.005)
textrect(nodes[10,],.07,.06,lab="predictions",shadow.size = 0.005)
textrect(nodes[11,],.075,.07,lab="etc...(further\ninference)",shadow.size = 0.005)
```
:::

Roughly speaking, data science can be divided into 3 phases:

 I.   In Phase I, you identify a research question, design an experiment, gather a sample, and collect some raw data. These steps correspond to the first row of the "life cycle".
 II.  In Phase II, you start with your raw data and clean it (this is probably where 60-90% of time is actually spent), explore it, and figure out the best way to model it. This is the second row.
 III. In Phase III, you fine tune your model, double check all your work, and interpret the results, which may involve reporting estimates, computing tests/intervals, making predictions, etc. This is the third row.

Experiment design & sampling is a more advanced topic, and thus not covered in detail in STAT 240, however we will summarize a few key ideas from Phase I in the next subsection since they are relevant to later topics.

We've spent a lot of time learning the basics of data cleaning, exploration, and visualization, so we're reasonably well covered for Phase II for now, though of course you're always encouraged to explore further on your own.

For most of the remainder of the class, we will focus on Phase III: identifying appropriate models, fitting them well, interpreting the results meaningfully, producing useful further inference such as hypothesis tests & confidence intervals, and communicating the results effectively to a broader audience.


