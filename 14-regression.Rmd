

# Regression

Regression is a broad class of methods where predictor variables are used to predict a numeric response. In this class, we will primarily focus on **simple linear regression**, where a single numeric predictor is used and the model is linear.


## Correlation coefficient

First, a brief aside on the correlation coefficient. Suppose we have a sample of pairs of observations $x_i$, $y_i$ drawn from $X$, $Y$ where each pair is independent from other pairs. Note however no assumption of independence is made between the variables, only between different pairs.

Similar to other statistics like mean or variance, there are both theoretical---i.e. population---and sample versions of the correlation coefficient definition. One is the underlying true value and the other is what you observe in your real dataset. The population correlation coefficient $\rho$ is defined as:

$$
\rho=\frac{\e\big[(X-\mu_X)(Y-\mu_Y)\big]}{\sigma_X\sigma_Y}
$$

where the numerator, also called the covariance of $X$ and $Y$, represents how much on average $X$ and $Y$ change together when compared to their means, and the denominator normalizes it by the product of their standard deviations.

Compare this with the sample correlation coefficient $r$ which is defined as:

$$
r=\frac{\frac1{n-1}\sum(x_i-\bar x)(y_i-\bar y)}{s_xs_y}
$$

Note similar to the definitions of $s_x$, $s_y$, the numerator also use $n-1$ instead of $n$ for its averaging. Also note that since these both involve a kind of averaging, the LLN assures us that $r\to\rho$ as our sample size increases.

It's easy to show that both $\rho$, $r$ always belong in the range $[-1,1]$, i.e. the **correlation coefficient must always have absolute value $\le1$**.

Below shows a series of simulated [jointly-normal](https://en.wikipedia.org/wiki/Multivariate_normal_distribution) $x_i$, $y_i$ observations drawn from $X$, $Y$ both with mean 0 and SD 1 but with various population correlation coefficient values:

:::{.fold .s}
```{r,include=F}
library(tidyverse)
options(pillar.print_min = 5)
source("ggplot_theme_options.R")
set.seed(3)
```
```{r,}
library(tidyverse) ; library(mvtnorm) ; library(plotly)
n <- 200
cor_dfs <- do.call(rbind, lapply(
  c(-20:20/20), \(r) rmvnorm(n,c(0,0),matrix(c(1,r,r,1),ncol=2)) %>% round(3) %>% 
    as.data.frame %>% setNames(c("x","y")) %>% cbind(r=r))) %>% arrange(r,x)
plot_ly(cor_dfs, type = "scatter", x = ~x, y = ~y, frame = ~r, mode = "markers", height=600) %>%
  config(displayModeBar = FALSE) %>% animation_opts(frame = 100, transition = 0) %>% 
  layout(title = list(text = "Simulated X,Y with various correlation coefficients r", x = .15),
         xaxis = list(range = list(-3,3)), yaxis = list(range = list(-3,3)),
         margin = list(l = 140, r = 140, b = 50, t = 50), dragmode=FALSE) %>%
  animation_slider(currentvalue = list(font = list(color = "#444444")))
```
:::

From this, several things should be clear:

 - The correlation coefficient's **sign indicates the direction of the trend**
 
   - Positive $+$ sign indicates a positive relationship (slope $>0$)
   
   - Negative $-$ sign indicates a negative relationship (slope $<0$)
   
 - The correlation coefficient's **absolute value indicates how "tight" the points are**
 
   - Absolute value close to 1 indicates the points are tight around the line

 - Correlation **equal or close to 0 indicates little or no relationship** (flat cloud of points)


### Caveats

The correlation coefficient has a major caveat: **it only measures the linear component of the relationship**. If the relationship is non-linear, it may not be useful at all!

To be even more clear, the **correlation coefficient can NOT be used to check linearity**! This is why it's important to **plot your data** first to make sure it's linear before directly jumping into regression.

Below is a famous example of this know as the Anscombe's quartet, which is built-in to R. This is a set of 4 $x_i$, $y_i$ datasets, each of which can be checked to have the same mean and SD in both $x$ and $y$, as well as the same correlation coefficient of about $0.816$.

```{r}
anscombe
anscombe %>%
  pivot_longer(everything(), names_to=c(".value","set"), names_pattern="(.)(.)") %>% 
  group_by(set) %>% summarize(xbar = mean(x), ybar = mean(y), sx = sd(x),
                              sy = sd(y), r = cor(x,y))
```

And yet, when plotted it's clear that these are 4 extremely different datasets, only one of which is actually appropriate for a linear regression model:

:::{.i96 .fold .s}
```{r}
anscombe %>%
  pivot_longer(everything(), names_to=c(".value","set"), names_pattern="(.)(.)") %>% 
  mutate(set = paste("set",set)) %>% 
  ggplot(aes(x=x,y=y)) + geom_point() + geom_smooth(method=lm, se=F) +
  facet_wrap(~set, scales="free") + ggtitle("Anscombe's quartet datasets, each r ≈ 0.816")
```
:::

As an even more extreme example, here's 4 datasets I cooked up, each of which has 0 correlation. Again, only one of which is actually appropriate for a linear regression model:

:::{.i96 .fold .s}
```{r}
# pick seed where rmvnorm generates dataset with especially low r
set.seed(841)
n <- 12
rmvnorm(n,c(0,0),matrix(c(1,0,0,1),ncol=2)) %>% as.data.frame %>% setNames(c("x1","y1")) %>% 
  add_column(
    x2 = cos(seq(0, 2*pi*(1-1/n), length.out=n)),
    y2 = sin(seq(0, 2*pi*(1-1/n), length.out=n)),
    x3 = seq(-1, 1, length.out=n),
    y3 = x3^2-1,
    x4 = x3,
    y4 = abs(sin(x4*pi))) %>%
  pivot_longer(everything(), names_to=c(".value","set"), names_pattern="(.)(.)") %>% 
  mutate(set = case_match(set, "1"~"set 1: no correlation", "2"~"set 2: circle",
                          "3"~"set 3: parabola", "4"~"set 4: McDonald's")) %>% 
  group_by(set) %>% mutate(x = (x-mean(x))/sd(x), y = (y-mean(y))/sd(y)) %>% 
  ggplot(aes(x=x,y=y)) + geom_point() + geom_smooth(method=lm, se=F) +
  facet_wrap(~set, scales="free") + ggtitle("Additional quartet of datasets, each r = 0")

```
:::

:::{.note}
All this is to demonstrate that the **correlation should NOT be used to determine if linear regression is appropriate**. It can ONLY be used to assess the strength of the relationship AFTER you've already visually checked the dataset is in fact linear by plotting!
:::


### Correlation in R

In R, the sample correlation coefficient $r$ is computed using the `cor()` function, which takes two vectors of the $x_i$, $y_i$ observations. The order of input does not matter.

```{r}
# continuing with set 1 from Anscombe's quartet
cor(anscombe$x1, anscombe$y1)
# of course can also use inside tidyverse
anscombe %>% summarize(r = cor(x1, y1))
# manual computation if desired
anscombe %>%
  summarize(r = sum((x1-mean(x1))*(y1-mean(y1)))/(length(x1)-1)/sd(x1)/sd(y1))
```

There are additional optional arguments like `use` which can specify which observations to use if there are missing values, or `method` which can be set to compute other related statistics such as [Kendall's](https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient) or [Spearman's](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient) correlation coefficients which are beyond our scope (our method above, which is the default, is called [Pearson's](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) correlation coefficient).


## Model notation

Now, let's move on to the actual modeling notation used. Again, let's suppose we draw a **sample of $n$ pairs $(x_i,y_i)$ from some population variables $X,Y$**, and suppose we've visually identified a linear regression model as appropriate, based on graphically assessing the degree of linearity, as well as possibly checking the correlation coefficient.

Our theoretical **linear model equation** is then the following:

$$
y_i=\beta_0+\beta_1x_i+\ep_i
$$

 - $x_i$, $y_i$ are the actual **data points** in our sample,
 
 - $\beta_0$ is the **true intercept** of the line,
 
 - $\beta_1$ is the **true slope** of the line,
 
 - $\ep_i$ are the **errors** of each point, which model the random fluctuations inherent to every measurement. These are assumed to be distributed $\ep_i\sim\n(0,\sigma)$, i.e. they are modeled as:
   
   i)   Normally distributed, with
   ii)  Mean zero,
   iii) Constant variance, and
   iV)  Independent from one another.

:::{.note}
At this point, it's important to take note of the following:

 - The statement of this model equation here implicitly involves an **assumption that the underlying process between $X,Y$ is truly linear**. Again, this linearity assumption can NOT be checked with the correlation coefficient; it must be visually assessed by plotting the data.

 - Here, $\beta_0$, $\beta_1$ represent the **true underlying intercept & slope** values that generate the data (assuming the data is in fact linear), NOT a best fit line based on a sample.

 - $x_i$, $y_i$, $\ep_i$ all have $i$ indices since they vary for each point, but $\beta_0$, $\beta_1$ do NOT have indices, since the points are supposed to all fall on ONE line with some true slope and intercept.

 - The $x_i$ are actually treated as constants, just like $\beta_0$, $\beta_1$, which means the only variable on the right hand side is $\ep_i$.

 - The assumptions on $\ep_i$ mean that the $y_i$ points are **distributed normally _about_ the regression line**. In other words, within every vertical slice along the line, the points follow a normal distribution along the y-axis.
:::

### Simulated demo

```{r,include=F}
set.seed(1)
b0 <- -3; b1 <- 0.5 ; n <- 10; m <- 10 ; M <- 20 ; sigma <- 2
```

As a quick demo of this setup and notation, suppose we pick true coefficients $\beta_0=`r b0`$, $\beta_1=`r b1`$. Using this, let's generate $n=`r n`$ observations where $x_i$ are uniformly sampled from $[`r m`,`r M`]$, and $\sigma=`r sigma`$.

Using these values, we can simulate one possible sample of $x_i$, $y_i$ values drawn from $X,Y$ with:

```{r,echo=F,results='asis'}
cat(glue::glue('``` r
# define values for simulation demo
b0 <- {b0} ; b1 <- {b1} ; n <- {n} ; m <- {m} ; M <- {M} ; sigma <- {sigma}
```'))
```
```{r}
# generate dataset for demo
x <- runif(n, m, M)
e <- rnorm(n, 0, sigma)
y <- b0 + b1*x + e
```

:::{.i9 .fold .s}
```{r,fig.width=6,fig.height=4.6}
# pick a convenient point in quadrant IV as example
i <- which.min((scale(x)[,1]-.5)^2+(scale(y)[,1])^2)

ggplot(tibble(x,y), aes(x=x,y=y)) + geom_point() +
  geom_segment(aes(color="yi",x=x[i],y=0,yend=y[i]),linewidth=.7) + 
  geom_segment(aes(color="xi",y=y[i],x=10,xend=x[i]),linewidth=.7) + 
  geom_segment(aes(color="εi",x=x[i],y=y[i],yend=(b0+b1*x[i])),linewidth=.7) + 
  geom_point(aes(color="i-th point",x=x[i],y=y[i]),size=3,shape="square") + 
  stat_smooth(aes(color="fitted line"),method=lm,se=F,linewidth=1,linetype="dashed",fullrange=T) +
  geom_abline(aes(color="true line", slope=b1, intercept=b0),linewidth=1,key_glyph="path") + 
  scale_x_continuous(breaks=seq(10,20,2), labels=seq(10,20,2),
                     limits=c(10,20), expand=0, minor_breaks=10:20) + 
  scale_y_continuous(breaks=seq(0,10,1), labels=seq(0,10,1),
                     limits=c(0,10), expand=0, minor_breaks=NULL) + 
  scale_color_manual(breaks=c("fitted line","true line","i-th point","εi","yi","xi"),
                     values=c("#7570B3FF","#1B9E77FF","#666666FF","#E6AB02FF","#A6761DFF","#E7298AFF")) + 
  theme(legend.key.width=unit(2.1,"line")) +
  labs(color = "Legend", title = "Simulated linear model demo")
```
:::

### Fitting process

Without going into too much detail, the typical fitting method, called **Ordinary Least Squares** (OLS) involves **finding the line that minimizes the _sum of squared errors_ vs the data points**.

Suppose we "test out" a hypothetical line with coefficients $\beta_0',\beta_1'$. The sum of the squared errors vs the data points, also called the **loss function $\ell(\beta_0',\beta_1')$**, is defined as:

$$
\ell(\beta_0',\beta_1')=\sum_{i=1}^n(y_i-(\beta_0'+\beta_1'x_i))^2
$$

Note this is a function of $\beta_0',\beta_1'$, i.e. we can "test out" different possible values of the coefficients, and evaluated the "loss" of each guess---i.e. how "bad" the guess is---by plugging it into the function to get the sum-squared-errors.

Visually, this looks like taking the vertical error bars from each data point to the "test" line and building a square with that as one side, then adding up the areas of each square to get the total sum-squared-errors. Then, the best fit line is the line that minimizes these sum-squared-errors:

:::{.i9 .fold .s}
```{r,fig.width=6,fig.height=4.6}
bh1 <- cor(x,y)*sd(y)/sd(x)
bh0 <- mean(y)-bh1*mean(x)
ggplot(tibble(x,y), aes(x=x,y=y)) +
  geom_rect(aes(xmin=x,xmax=x+(y-(x*bh1+bh0))*if_else(x>16.5|y>1&y<3,-1,1),
                ymin=y,ymax=x*bh1+bh0,color="squared errors"),alpha=.1) + 
  geom_segment(aes(yend=x*bh1+bh0, color="errors"),linewidth=1) +
  stat_smooth(aes(color="fitted line"),method=lm,se=F,fullrange=T,linewidth=1) + 
  geom_point(size=2) +
  scale_x_continuous(breaks=seq(10,20,2), labels=seq(10,20,2),
                     limits=c(10,20), expand=0, minor_breaks=10:20) + 
  scale_y_continuous(breaks=seq(0,10,1), labels=seq(0,10,1),
                     limits=c(0,10), expand=0, minor_breaks=NULL) + 
  scale_color_manual(breaks=c("fitted line","errors","squared errors"),
                     values=c("#7570B3FF","#E6AB02FF","#666666FF")) +
  labs(color = "Legend", title = "Least squares visualized for best fit line")
```
:::

To find this best fit line, we **solve for the local minimum of the loss function**, where we know the **partial derivatives with respect to $\beta_0',\beta_1'$ must both vanish**:

$$
\text{solve}\quad
\frac{\partial\ell(\beta_0',\beta_1')}{\partial\beta_0'}=0
\quad\text{and}\quad
\frac{\partial\ell(\beta_0',\beta_1')}{\partial\beta_1'}=0
$$

This is not extremely difficult to solve but we omit the [details](https://statproofbook.github.io/P/slr-ols.html).


## Fitted coefficients

One can show that the best fit coefficients $\hat\beta_0$, $\hat\beta_1$ that minimize the loss function are:

$$
\hat\beta_1~=~r\cdot\frac{s_y}{s_x}~~~~\\
\hat\beta_0~=~\bar y-\hat\beta_1\bar x
$$

Note these are purely functions of the sample statistics $\bar x$, $\bar y$, $s_x$, $s_y$, $r$, and straightforward to compute.

Also note the second equation implies the following key fact: **the best fit line _always_ passes through the point $(\bar x,\bar y)$, since this point clearly satisfied $\bar y=\hat\beta_0+\hat\beta_1\bar x$.

Another way of thinking about it, the best fit intercept $\hat\beta_0$ is determined automatically by seeing where the line with slope $\hat\beta_1$ that goes through $(\bar x,\bar y)$ intersects the y-axis.

It can be shown that theoretically the best fit coefficients $\hat\beta_0$, $\hat\beta_1$ as defined above are not only **unbiased**, i.e. no systematic deviation from the true value, but also **least variance** (among linear estimators), i.e. in some sense the most "precise" estimation of the true values $\beta_0$, $\beta_1$, and that as your sample size grows, we'll have $\hat\beta_0\to\beta_0$ and $\hat\beta_1\to\beta_1$.^[This result is known as the [Gauss–Markov theorem](https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem)]


## Data example

For a first data example, let's example a historic ice cream consumption dataset provided by Kadiyala.^[https://doi.org/10.2307/1909244] The data, found in [`icecream.csv`](data/icecream.csv) reports ice cream consumption per capita in pints vs mean temperature in Farenheit over a series of 4-week periods between 1951-53 in a region in Michigan.

```{r,eval=F}
# set comment="# to ignore description lines at top of file,
# then select main columns of interest
icecream <- read_csv("https://bwu62.github.io/stat240-revamp/data/icecream.csv",
                     comment="#") %>% select(consumption, temperature)
icecream
```
```{r,echo=F}
icecream <- read_csv("data/icecream.csv", comment="#") %>% select(consumption, temperature)
icecream
```
```{r}
ggplot(icecream,aes(x=temperature, y=consumption)) + geom_point() +
  geom_smooth(method=lm, se=F) + labs(y="Consumption (pints per capita)",
  x="Mean temperature (F)", title="Ice cream consumption vs temperature")
```

We can see this dataset looks quite linear. We can also compute the correlation:

```{r}
icecream %>% summarize(r = cor(consumption, temperature))
```

All signs suggest this is a good candidate for linear regression.


### Fit manually

We can use the expressions shown earlier to manually compute the best fit slope and intercept $\hat\beta_1$, $\hat\beta_0$.

```{r}
# <<- is a dirty R trick to globally assign a variable from anywhere
icecream %>% summarize(
  b1 = (b1 <<- cor(consumption,temperature) * sd(consumption) / sd(temperature)),
  b0 = (b0 <<- mean(consumption) - b1 * mean(temperature)))
# check our fit by manually replotting the best fit line
ggplot(icecream, aes(x=temperature, y=consumption)) + geom_point() +
  geom_abline(slope=b1, intercept=b0, color="red") + labs(x="Mean temperature (F)",
  y="Consumption (pints per capita)", title="Ice cream consumption vs temperature")
```


## Fit with `lm()`

Of course we can also fit using R. The function to use is `lm()`, which is short for **L**inear **M**odel. There are many advanced arguments, but minimally these should always be specified:

 - `formula`, the 1^st^ argument, expects an R "formula" object, which is a special object with its own syntax created with the `~` character and which specifies a dependency structure for variables (see [this page](https://www.econometrics.blog/post/the-r-formula-cheatsheet) for details)
 
   - For simple linear regression, this formula will be `response ~ predictor` where `response` and `predictor` are **column names of the $Y$ and $X$ variables in your data frame**

 - `data`, the 2^nd^ argument, expects the **data frame with your variables**

You can also use two vectors directly for the formula specification instead of using columns in a data frame, but this is generally not recommended since this precludes certain additional useful features down the road. **For best results, _always_ use `lm()` with columns from a data frame!**

:::{.note}
The formula specification is always in order of **`response ~ predictor`**, i.e. **`Y ~ X`**, and NOT the other way around; the order of variables here is not interchangeable!
:::

```{r}
# fit the model, saving the output object which will be useful later
lm_icecream <- lm(consumption ~ temperature, data = icecream)
lm_icecream
# print a more complete summary output of the model
summary(lm_icecream)
```

There's a lot of info here to unpack, some of which we'll get to later and some of which are beyond our scope, but for now we can see the results match our manual computation, and we have $\hat\beta_1=`r coef(lm_icecream)[2]`$, $\hat\beta_0=`r coef(lm_icecream)[1]`$.


## Model validation

A very important step at this stage is to do **model validation**, which means **check the fit makes sense** and nothing unusual or unexpected is happening with your fit. This is usually done by analyzing the residuals of the fit.


### Residuals

Recall the best fit line is determined by minimizing the sum-squared-errors. Once the best fit line is found, taking the difference between $y_i$ and the best-fit predictions $\hat y_i$ gives our sample estimate of the errors, which we call $\hat\ep_i$ or the **residuals** of the fit and are defined:

$$
\hat\ep_i~=~y_i-\hat y_i~=~y_i-\left(\hat\beta_0+\hat\beta_1x_i\right)
$$

These $\hat\ep_i$ are treated as a sample of $\ep_i$ which remember we assumed to have distribution $\ep_i\sim\n(0,\sigma)$.

The reason this assumption is made is simply that **pure random noise is generally normal, mean 0, and constant variance**, so if our model is correct and we have correctly captured the underlying linear relationship, whatever is leftover after our fit should look like pure noise!

These assumptions are usually checked by examining the **residual vs fitted** and **QQ** plots for the fit.


### Plotting residuals

Let's first compute the model's residuals for each data point:

```{r}
# recall we still have best fit coefficients b1, b0 defined from earlier
icecream <- icecream %>% mutate(residuals = consumption - (b0 + b1*temperature))
icecream
# check our residuals using R's built-in residuals() function
icecream$residuals
residuals(lm_icecream) %>% unname  # remove names from vector (added by default)
# plot residuals (since residuals=response-predicted, predicted=response-residuals)
ggplot(icecream, aes(x=temperature, y=consumption)) +
  geom_segment(aes(yend=consumption-residuals), color="#E6AB02FF") + geom_point() +
  geom_abline(slope=b1, intercept=b0, color="red") +
  labs(title="Ice cream consumption vs temperature (with residuals)",
  x="Mean temperature (F)", y="Consumption (pints per capita)")
```

These can then be directly plotted both against the predicted $\hat y_i$ and in a QQ plot using methods we've learned before (exercise left to reader). Alternatively, there's also the rare slick base R shortcut here:

```{r,eval=F}
plot(lm_icecream, 1)
```
```{r,echo=F}
par(mar=c(4,3,1.5,.5),mgp=c(2,.6,0))
plot(lm_icecream, 1)
```

```{r,eval=F}
plot(lm_icecream, 2)
```
```{r,echo=F}
par(mar=c(4,3,1.5,.5),mgp=c(2,.6,0))
plot(lm_icecream, 2)
```

From these plots we can see the residuals do appear to be **normal, 0-mean, and constant-variance**, so everything looks good. For examples of bad residual plots, see [this page](https://online.stat.psu.edu/stat462/node/124) or [this page](https://scales.arabpsychology.com/stats/what-is-considered-a-good-vs-bad-residual-plot).


<!--
## prediction

## inference on coefficients

## intervals for prediction

## fitting other relations

### exponential, power law, etc.


## bonus: list of all lm object functions
```{r}
# get all stats::: functions implementing .lm method
# sort(grep("\\.lm",unclass(lsf.str(envir = asNamespace("stats"), all = T)),value = T))
```
-->
