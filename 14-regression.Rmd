

# Regression

Regression is a broad class of methods where predictor variables are used to predict a numeric response. In this class, we will primarily focus on **simple linear regression**, where a single numeric predictor is used and the model is linear.


## Correlation coefficient

First, a brief aside on the correlation coefficient. Suppose we have a sample of pairs of observations $x_i$, $y_i$ drawn from $X$, $Y$ where each pair is independent from other pairs. Note however no assumption of independence is made between the variables, only between different pairs.

Similar to other statistics like mean or variance, there are both theoretical---i.e. population---and sample versions of the correlation coefficient definition. One is the underlying true value and the other is what you observe in your real dataset. The population correlation coefficient $\rho$ is defined as:

$$
\rho=\frac{\e\big[(X-\mu_X)(Y-\mu_Y)\big]}{\sigma_X\sigma_Y}
$$

where the numerator, also called the covariance of $X$ and $Y$, represents how much on average $X$ and $Y$ change together when compared to their means, and the denominator normalizes it by the product of their standard deviations.

Compare this with the sample correlation coefficient $r$ which is defined as:

$$
r=\frac{\frac1{n-1}\sum(x_i-\bar x)(y_i-\bar y)}{s_xs_y}
$$

Note similar to the definitions of $s_x$, $s_y$, the numerator also use $n-1$ instead of $n$ for its averaging. Also note that since these both involve a kind of averaging, the LLN assures us that $r\to\rho$ as our sample size increases.

It's easy to show that both $\rho$, $r$ always belong in the range $[-1,1]$, i.e. the **correlation coefficient must always have absolute value $\le1$**.

Below shows a series of simulated [jointly-normal](https://en.wikipedia.org/wiki/Multivariate_normal_distribution) $x_i$, $y_i$ observations drawn from $X$, $Y$ both with mean 0 and SD 1 but with various population correlation coefficient values:

:::{.fold .s}
```{r,include=F}
library(tidyverse)
options(pillar.print_min = 5)
source("ggplot_theme_options.R")
set.seed(3)
```
```{r,eval=F}
library(mvtnorm) ; library(plotly) ; n <- 200
cor_dfs <- do.call(rbind, lapply(
  c(-20:20/20), \(r) rmvnorm(n,c(0,0), matrix(c(1,r,r,1), ncol=2)) %>% round(3) %>% 
    as.data.frame %>% setNames(c("x","y")) %>% cbind(r=r))) %>% arrange(r, x)
plot_ly(cor_dfs, type="scatter", x=~x, y=~y, frame=~r, mode="markers", height=600) %>%
  config(displayModeBar=FALSE) %>% animation_opts(frame=100, transition=0) %>% 
  layout(title = list(text="Simulated X,Y with various correlation coefficients r", x=.15),
         xaxis = list(range=list(-3,3)), yaxis = list(range=list(-3,3)),
         margin = list(l=0, r=10, b=135, t=50, automargin=FALSE), dragmode=FALSE) %>%
  animation_slider(currentvalue = list(font = list(color="#444444")))
```
:::

<style>
@media (min-width: 992px){
  #cor-app{max-width:75%;}
}
</style>
<center><div id="cor-app">

```{r,echo=F}
library(mvtnorm) ; library(plotly) ; n <- 200
cor_dfs <- do.call(rbind, lapply(
  c(-20:20/20), \(r) rmvnorm(n,c(0,0), matrix(c(1,r,r,1), ncol=2)) %>% round(3) %>% 
    as.data.frame %>% setNames(c("x","y")) %>% cbind(r=r))) %>% arrange(r, x)
plot_ly(cor_dfs, type="scatter", x=~x, y=~y, frame=~r, mode="markers", height=600) %>%
  config(displayModeBar=FALSE) %>% animation_opts(frame=100, transition=0) %>% 
  layout(title = list(text="Simulated X,Y with various correlation coefficients r", x=.08),
         xaxis = list(range=list(-3,3)), yaxis = list(range=list(-3,3)),
         margin = list(l=0, r=10, b=135, t=50, automargin=FALSE), dragmode=FALSE) %>%
  animation_slider(currentvalue = list(font = list(color="#444444")))
```

</div></center>

From this, several things should be clear:

 - The correlation coefficient's **sign indicates the direction of the trend**
 
   - Positive $+$ sign indicates a positive relationship (slope $>0$)
   
   - Negative $-$ sign indicates a negative relationship (slope $<0$)
   
 - The correlation coefficient's **absolute value indicates how "tight" the points are**
 
   - Absolute value close to 1 indicates the points are tight around the line

 - Correlation **equal or close to 0 indicates little or no relationship** (flat cloud of points)


### Caveats

The correlation coefficient has a major caveat: **it only measures the linear component of the relationship**. If the relationship is non-linear, it may not be useful at all!

To be even more clear, the **correlation coefficient can NOT be used to check linearity**! This is why it's important to **plot your data** first to make sure it's linear before directly jumping into regression.

Below is a famous example of this know as the Anscombe's quartet, which is built-in to R. This is a set of 4 $x_i$, $y_i$ datasets, each of which can be checked to have the same mean and SD in both $x$ and $y$, as well as the same correlation coefficient of about $0.816$.

```{r}
anscombe
anscombe %>%
  pivot_longer(everything(), names_to=c(".value","set"), names_pattern="(.)(.)") %>% 
  group_by(set) %>% summarize(xbar = mean(x), ybar = mean(y), sx = sd(x),
                              sy = sd(y), r = cor(x,y))
```

And yet, when plotted it's clear that these are 4 extremely different datasets, only one of which is actually appropriate for a linear regression model:

:::{.i96 .fold .s}
```{r}
anscombe %>%
  pivot_longer(everything(), names_to=c(".value","set"), names_pattern="(.)(.)") %>% 
  mutate(set = paste("set",set)) %>% 
  ggplot(aes(x=x,y=y)) + geom_point() + geom_smooth(method=lm, se=F) +
  facet_wrap(~set, scales="free") + ggtitle("Anscombe's quartet datasets, each r ≈ 0.816")
```
:::

As an even more extreme example, here's 4 datasets I cooked up, each of which has 0 correlation. Again, only one of which is actually appropriate for a linear regression model:

:::{.i96 .fold .s}
```{r}
# pick seed where rmvnorm generates dataset with especially low r
set.seed(841)
n <- 12
rmvnorm(n,c(0,0),matrix(c(1,0,0,1),ncol=2)) %>% as.data.frame %>% setNames(c("x1","y1")) %>% 
  add_column(
    x2 = cos(seq(0, 2*pi*(1-1/n), length.out=n)),
    y2 = sin(seq(0, 2*pi*(1-1/n), length.out=n)),
    x3 = seq(-1, 1, length.out=n),
    y3 = x3^2-1,
    x4 = x3,
    y4 = abs(sin(x4*pi))) %>%
  pivot_longer(everything(), names_to=c(".value","set"), names_pattern="(.)(.)") %>% 
  mutate(set = case_match(set, "1"~"set 1: no correlation", "2"~"set 2: circle",
                          "3"~"set 3: parabola", "4"~"set 4: McDonald's")) %>% 
  group_by(set) %>% mutate(x = (x-mean(x))/sd(x), y = (y-mean(y))/sd(y)) %>% 
  ggplot(aes(x=x,y=y)) + geom_point() + geom_smooth(method=lm, se=F) +
  facet_wrap(~set, scales="free") + ggtitle("Additional quartet of datasets, each r = 0")

```
:::

:::{.note}
All this is to demonstrate that the **correlation should NOT be used to determine if linear regression is appropriate**. It can ONLY be used to assess the strength of the relationship AFTER you've already visually checked the dataset is in fact linear by plotting!
:::


### Correlation in R

In R, the sample correlation coefficient $r$ is computed using the `cor()` function, which takes two vectors of the $x_i$, $y_i$ observations. The order of input does not matter.

```{r}
# continuing with set 1 from Anscombe's quartet
cor(anscombe$x1, anscombe$y1)
# of course can also use inside tidyverse
anscombe %>% summarize(r = cor(x1, y1))
# manual computation if desired
anscombe %>%
  summarize(r = sum((x1-mean(x1))*(y1-mean(y1)))/(length(x1)-1)/sd(x1)/sd(y1))
```

There are additional optional arguments like `use` which can specify which observations to use if there are missing values, or `method` which can be set to compute other related statistics such as [Kendall's](https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient) or [Spearman's](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient) correlation coefficients which are beyond our scope (our method above, which is the default, is called [Pearson's](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) correlation coefficient).


## Model notation

Now, let's move on to the actual modeling notation used. Again, let's suppose we draw a **sample of $n$ pairs $(x_i,y_i)$ from some population variables $X,Y$**, and suppose we've visually identified a linear regression model as appropriate, based on graphically assessing the degree of linearity, as well as possibly checking the correlation coefficient.

Our theoretical **linear model equation** is then the following:

$$
y_i=\beta_0+\beta_1x_i+\ep_i
$$

 - $x_i$, $y_i$ are the actual **data points** in our sample,
 
 - $\beta_0$ is the **true intercept** of the line,
 
 - $\beta_1$ is the **true slope** of the line,
 
 - $\ep_i$ are the **errors** of each point, which model the random fluctuations inherent to every measurement. These are assumed to be distributed $\ep_i\sim\n(0,\sigma)$, i.e. they are modeled as:
   
   i)   Normally distributed, with
   ii)  Mean zero,
   iii) Constant variance, and
   iv)  Independent from one another.

:::{.note}
At this point, it's important to take note of the following:

 - The statement of this model equation here implicitly involves an **assumption that the underlying process between $X,Y$ is truly linear**. Again, this linearity assumption can NOT be checked with the correlation coefficient; it must be visually assessed by plotting the data.

 - Here, $\beta_0$, $\beta_1$ represent the **true underlying intercept & slope** values that generate the data (assuming the data is in fact linear), NOT a best fit line based on a sample.

 - $x_i$, $y_i$, $\ep_i$ all have $i$ indices since they vary for each point, but $\beta_0$, $\beta_1$ do NOT have indices, since the points are supposed to all fall on ONE line with some true slope and intercept.

 - The $x_i$ are actually treated as constants, just like $\beta_0$, $\beta_1$, which means the only variable on the right hand side is $\ep_i$.

 - The assumptions on $\ep_i$ mean that the $y_i$ points are **distributed normally _about_ the regression line**. In other words, within every vertical slice along the line, the points follow a normal distribution along the y-axis.
:::

### Simulated demo

```{r,include=F}
set.seed(1)
b0 <- -3; b1 <- 0.5 ; n <- 10; m <- 10 ; M <- 20 ; sigma <- 2
```

As a quick demo of this setup and notation, suppose we pick true coefficients $\beta_0=`r b0`$, $\beta_1=`r b1`$. Using this, let's generate $n=`r n`$ observations where $x_i$ are uniformly sampled from $[`r m`,`r M`]$, and $\sigma=`r sigma`$.

Using these values, we can simulate one possible sample of $x_i$, $y_i$ values drawn from $X,Y$ with:

```{r,echo=F,results='asis'}
cat(glue::glue('``` r
# define values for simulation demo
b0 <- {b0} ; b1 <- {b1} ; n <- {n} ; m <- {m} ; M <- {M} ; sigma <- {sigma}
```'))
```
```{r}
# generate dataset for demo
x <- runif(n, m, M)
e <- rnorm(n, 0, sigma)
y <- b0 + b1*x + e
```

:::{.i9 .fold .s}
```{r,fig.width=6,fig.height=4.6}
# pick a convenient point in quadrant IV as example
i <- which.min((scale(x)[,1]-.5)^2+(scale(y)[,1])^2)

ggplot(tibble(x,y), aes(x=x,y=y)) + geom_point() +
  geom_segment(aes(color="yi",x=x[i],y=0,yend=y[i]),linewidth=.7) + 
  geom_segment(aes(color="xi",y=y[i],x=10,xend=x[i]),linewidth=.7) + 
  geom_segment(aes(color="εi",x=x[i],y=y[i],yend=(b0+b1*x[i])),linewidth=.7) + 
  geom_point(aes(color="i-th point",x=x[i],y=y[i]),size=3,shape="square") + 
  stat_smooth(aes(color="best fit line"),method=lm,se=F,linewidth=1,linetype="dashed",fullrange=T) +
  geom_abline(aes(color="true line", slope=b1, intercept=b0),linewidth=1,key_glyph="path") + 
  scale_x_continuous(breaks=seq(10,20,2), labels=seq(10,20,2),
                     limits=c(10,20), expand=0, minor_breaks=10:20) + 
  scale_y_continuous(breaks=seq(0,10,1), labels=seq(0,10,1),
                     limits=c(0,10), expand=0, minor_breaks=NULL) + 
  scale_color_manual(breaks=c("best fit line","true line","i-th point","εi","yi","xi"),
                     values=c("#7570B3FF","#1B9E77FF","#666666FF","#E6AB02FF","#A6761DFF","#E7298AFF")) + 
  theme(legend.key.width=unit(2.1,"line")) +
  labs(color = "Legend", title = "Simulated linear model demo")
```
:::

### Fitting process

Without going into too much detail, the typical fitting method, called **Ordinary Least Squares** (OLS) involves **finding the line that minimizes the _sum of squared errors_ vs the data points**.

Suppose we "test out" a hypothetical line with coefficients $\beta_0',\beta_1'$. The sum of the squared errors vs the data points, also called the **loss function $\ell(\beta_0',\beta_1')$**, is defined as:

$$
\ell(\beta_0',\beta_1')=\sum_{i=1}^n(y_i-(\beta_0'+\beta_1'x_i))^2
$$

Note this is a function of $\beta_0',\beta_1'$, i.e. we can "test out" different possible values of the coefficients, and evaluated the "loss" of each guess---i.e. how "bad" the guess is---by plugging it into the function to get the sum-squared-errors.

Visually, this looks like taking the vertical error bars from each data point to the "test" line and building a square with that as one side, then adding up the areas of each square to get the total sum-squared-errors. Then, the best fit line is the line that minimizes these sum-squared-errors:

:::{.i9 .fold .s}
```{r,fig.width=6,fig.height=4.6}
bh1 <- cor(x,y)*sd(y)/sd(x)
bh0 <- mean(y)-bh1*mean(x)
ggplot(tibble(x,y), aes(x=x,y=y)) +
  geom_rect(aes(xmin=x,xmax=x+(y-(x*bh1+bh0))*if_else(x>16.5|y>1&y<3,-1,1),
                ymin=y,ymax=x*bh1+bh0,color="squared errors"),alpha=.1) + 
  geom_segment(aes(yend=x*bh1+bh0, color="errors"),linewidth=1) +
  stat_smooth(aes(color="best fit line"),method=lm,se=F,fullrange=T,linewidth=1) + 
  geom_point(size=2) +
  scale_x_continuous(breaks=seq(10,20,2), labels=seq(10,20,2),
                     limits=c(10,20), expand=0, minor_breaks=10:20) + 
  scale_y_continuous(breaks=seq(0,10,1), labels=seq(0,10,1),
                     limits=c(0,10), expand=0, minor_breaks=NULL) + 
  scale_color_manual(breaks=c("best fit line","errors","squared errors"),
                     values=c("#7570B3FF","#E6AB02FF","#666666FF")) +
  labs(color = "Legend", title = "Least squares visualized for best fit line")
```
:::

To find this best fit line, we **solve for the local minimum of the loss function**, where we know the **partial derivatives with respect to $\beta_0',\beta_1'$ must both vanish**:

$$
\text{solve}\quad
\frac{\partial\ell(\beta_0',\beta_1')}{\partial\beta_0'}=0
\quad\text{and}\quad
\frac{\partial\ell(\beta_0',\beta_1')}{\partial\beta_1'}=0
$$

This is not extremely difficult to solve but we omit the [details](https://statproofbook.github.io/P/slr-ols.html).


## Best fit coefficients

One can show that the best fit coefficients $\hat\beta_0$, $\hat\beta_1$ that minimize the loss function are:

$$
\hat\beta_1~=~r\cdot\frac{s_y}{s_x}~~~~\\
\hat\beta_0~=~\bar y-\hat\beta_1\bar x
$$

Note these are purely functions of the sample statistics $\bar x$, $\bar y$, $s_x$, $s_y$, $r$, and straightforward to compute.

Also note the second equation implies the following key fact: **the best fit line _always_ passes through the point $(\bar x,\bar y)$**, since this point clearly satisfied $\bar y=\hat\beta_0+\hat\beta_1\bar x$.

Another way of thinking about it, the best fit intercept $\hat\beta_0$ is determined automatically by seeing where the line with slope $\hat\beta_1$ that goes through $(\bar x,\bar y)$ intersects the y-axis.

It can be shown that theoretically the best fit coefficients $\hat\beta_0$, $\hat\beta_1$ as defined above are not only **unbiased**, i.e. no systematic deviation from the true value, but also **least variance** (among linear estimators), i.e. in some sense the most "precise" estimation of the true values $\beta_0$, $\beta_1$, and that as your sample size grows, we'll have $\hat\beta_0\to\beta_0$ and $\hat\beta_1\to\beta_1$.^[This result is known as the [Gauss–Markov theorem](https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem)]


## Data example

For a first data example, let's example a historic ice cream consumption dataset provided by Kadiyala.^[https://doi.org/10.2307/1909244] The data, found in [`icecream.csv`](data/icecream.csv) reports ice cream consumption per capita in pints vs mean temperature in Farenheit over a series of 4-week periods between 1951-53 in a region in Michigan.

```{r,eval=F}
# set comment="#" to ignore description lines at top of file,
# then select main columns of interest
icecream <- read_csv("https://bwu62.github.io/stat240-revamp/data/icecream.csv",
                     comment="#") %>% select(consumption, temperature)
icecream
```
```{r,echo=F}
icecream <- read_csv("data/icecream.csv", comment="#") %>% select(consumption, temperature)
icecream
```
```{r}
ggplot(icecream,aes(x=temperature, y=consumption)) + geom_point() +
  geom_smooth(method=lm, se=F) + labs(y="Consumption (pints per capita)",
  x="Mean temperature (F)", title="Ice cream consumption vs temperature")
```

We can see this dataset looks quite linear. We can also compute the correlation:

```{r}
icecream %>% summarize(r = cor(consumption, temperature))
```

All signs suggest this is a good candidate for linear regression.


### Fit manually

We can use the expressions shown earlier to manually compute the best fit slope and intercept $\hat\beta_1$, $\hat\beta_0$.

```{r}
# <<- is a dirty R trick to globally assign a variable from anywhere
icecream %>% summarize(
  xbar = (xbar <<- mean(temperature)),
  ybar = (ybar <<- mean(consumption)),
  sx   = (sx   <<- sd(temperature)),
  sy   = (sy   <<- sd(consumption)),
  r    = (r    <<- cor(consumption, temperature)),
  b1   = (b1   <<- r * sy / sx),
  b0   = (b0   <<- ybar - b1 * xbar))
# check fit by manually plotting best fit line (cross added at xbar, ybar)
library(latex2exp)
ggplot(icecream, aes(x=temperature, y=consumption)) + geom_point() +
  geom_point(aes(x=xbar, y=ybar), shape=3, size=6) +
  geom_abline(slope=b1, intercept=b0, color="red") +
  labs(x="Mean temperature (F)", y="Consumption (pints per capita)", title=TeX(
    "Ice cream consumption vs temperature (cross shows $\\bar{x},\\bar{y}$)"))
```


## Fit with `lm()`

Of course we can also fit using R. The function to use is `lm()`, which is short for **L**inear **M**odel. There are many advanced arguments, but minimally these should always be specified:

 - `formula`, the 1^st^ argument, expects an R "formula", which is a special object created with the `~` character and which specifies a dependency structure for variables (see [this page](https://www.econometrics.blog/post/the-r-formula-cheatsheet) for details)
 
   - For simple linear regression, this formula will be `response ~ predictor` where `response` and `predictor` are **column names of the $Y$ and $X$ variables in your data frame**

 - `data`, the 2^nd^ argument, expects the **data frame with your variables**

You can also use two vectors directly for the formula specification instead of using columns in a data frame, but this is generally not recommended since this precludes certain additional useful features down the road. **For best results, _always_ use `lm()` with columns from a data frame!**

:::{.note}
The formula specification is always in order of **`response ~ predictor`**, i.e. **`Y ~ X`**, and NOT the other way around; the order of variables here is not interchangeable!
:::

```{r}
# fit the model, saving the output object which will be useful later
lm_icecream <- lm(consumption ~ temperature, data = icecream)
lm_icecream
# print a more complete summary output of the model
summary(lm_icecream)
```
There's a lot here to unpack, some of which we'll get to later, but for now we can see the results match our manual computation, and we have $\hat\beta_0=`r sf(b0,4)`$, $\hat\beta_1=`r tr(b1,6)`$.


## Model validation

A very important step at this stage is to do **model validation**, which means **check the fit makes sense** and nothing unusual or unexpected is happening with your fit. This is usually done by analyzing the residuals of the fit.


### Residuals

Recall the best fit line is determined by minimizing the sum-squared-errors. Once the best fit line is found, taking the difference between $y_i$ and the best-fit predictions $\hat y_i$ gives our sample estimate of the errors, which we call $\hat\ep_i$ or the **residuals** of the fit and are defined:

$$
\hat\ep_i~=~y_i-\hat y_i~=~y_i-\left(\hat\beta_0+\hat\beta_1x_i\right)
$$

These $\hat\ep_i$ are treated as a sample of $\ep_i$ which remember we assumed to have distribution $\ep_i\sim\n(0,\sigma)$.

The reason this assumption is made is simply that **pure random noise is generally normal, mean 0, and constant variance**, so if our model is correct and we have correctly captured the underlying linear relationship, whatever is leftover after our fit should look like pure noise!

These assumptions are usually checked by examining the **residual vs fitted** and **QQ** plots for the fit.


### Plotting residuals

Let's first compute the model's residuals for each data point:

```{r}
# recall we still have best fit coefficients b1, b0 defined from earlier
icecream <- icecream %>% mutate(residuals = consumption - (b0 + b1*temperature))
icecream
# check our residuals using R's built-in resid() function
icecream$residuals
resid(lm_icecream) %>% unname  # remove names from vector (added by default)
# plot residuals (since residuals=response-predicted, predicted=response-residuals)
ggplot(icecream, aes(x=temperature, y=consumption)) +
  geom_segment(aes(yend=consumption-residuals), color="#E6AB02FF") + geom_point() +
  geom_abline(slope=b1, intercept=b0, color="red") +
  labs(title="Ice cream consumption vs temperature (with residuals)",
  x="Mean temperature (F)", y="Consumption (pints per capita)")
```

These can then be directly plotted both against the predicted $\hat y_i$ and in a QQ plot using methods we've learned before (exercise left to reader). Alternatively, there's also the rare slick base R shortcut here:

```{r,eval=F}
plot(lm_icecream, 1)
```
```{r,echo=F}
par(mar=c(4,3,1.5,.5),mgp=c(2,.6,0))
plot(lm_icecream, 1)
```

```{r,eval=F}
plot(lm_icecream, 2)
```
```{r,echo=F}
par(mar=c(4,3,1.5,.5),mgp=c(2,.6,0))
plot(lm_icecream, 2)
```

From these plots we can see the residuals do appear to be **normal, 0-mean, and constant-variance**, so everything looks good. For examples of bad residual plots, see [this page](https://online.stat.psu.edu/stat462/node/124) or [this page](https://scales.arabpsychology.com/stats/what-is-considered-a-good-vs-bad-residual-plot).

:::{.tip}
In R, when a ["generic"](https://www.stat.umn.edu/geyer/3701/notes/generic.html) function like `plot()` or `summary()` is used with objects of special classes like the output of `lm()`, they actually redirect and call the functions <code><a href="https://rdrr.io/r/stats/plot.lm.html" title="https://rdrr.io/r/stats/plot.lm.html" target="_blank">plot.lm()</a></code> or `summary.lm()`, which are specific implementations of the generic function for that class.

This means you need to search for those specific functions' help pages if you want additional details on how they work.
:::


## Inference

Of course, inference is also possible for regression. We can easily construct **confidence intervals for $\hat\beta_0$, $\hat\beta_1$** or perform **hypothesis tests for $\beta_0$, $\beta_1$**.

Generally, the most common inference questions to ask is **if the data shows a significant _linear_ relationship**, and this is usually answered by either computing an interval for $\hat\beta_1$, or trying to reject $\beta_1=0$, since these will affirm the significance of the $\beta_1x_i$ term in the linear model equation. Much less commonly, inference may also be done on the intercept.

### T-distribution

In both cases, for either slope or intercept, the process of inference is straightforward: **use a T-distribution with $\nu=n-2$ degrees of freedom**, with the corresponding **standard errors**:

$$
\text{(intercept)}~~\se(\hat\beta_0)=\hat\sigma\cdot\sqrt{\frac1n+\frac{\bar x^2}{s_x^2(n-1)}}~,\\
\text{(slope)}~~\se(\hat\beta_1)=\hat\sigma\cdot\sqrt{\frac1{s_x^2(n-1)}}~,\,~\\
~~~~~~~\text{where}~~\hat\sigma=\sqrt{\frac1{n-2}\sum\hat\ep_i^2}
$$

For our example dataset, these can be computed as:

```{r}
# recall we still have xbar, sx globally defined from earlier,
# use <<- again to globally define sigma, s0, s1 by formulae above
icecream %>% summarize(
  n = (n <<- n()),
  sigma = (sigma <<- sqrt(1/(n-2) * sum(residuals^2))),
  s0 = (s0 <<- sigma * sqrt(1/n + xbar^2/(sx^2*(n-1)))),
  s1 = (s1 <<- sigma * sqrt(1/(sx^2*(n-1))))
) %>% data.frame  # convert to base R data frame, which prints more digits by default
```

For the purposes of 240, **you can always read these from the `summary()` output**, which will either be provided for you or easily computed in R whenever regression inference is involved.

Once again, here's the output of `summary()`

```{r}
summary(lm_icecream)
```

 - Under the `Std. Error` column, we can see $\se(\hat\beta_0)=`r sf(s0,4)`$, $\se(\hat\beta_1)=`r sf(s1,4)`$.
 
 - Below that, next to `Residual standard error`, we can see $\hat\sigma=`r sf(sigma,4)`$, which is our estimate of $\sigma$, i.e. the standard deviation of the $\ep_i$ errors.

 - Additionally, we see $\nu=n-2=`r n-2`$ which is the degrees of freedom needed for our t-distribution.

### Hypothesis test

Now, we can do all the inference we desire. Suppose we want to see **if there is a significant linear relationship**. In other words, we test the following hypotheses:

$$
H_0:\beta_1=0\\
H_a:\beta_1\ne0
$$

We compute our t-statistic and p-value similar to the one-sample t-test:

$$
t_\obs~=~\frac{\hat\beta_1-0}{\se(\hat\beta_1)}~=~\frac{`r sf(b1,4)`}{`r sf(s1,4)`}~=~`r sf(b1/s1,4)`~\sim~T_{n-2}
$$

```{r}
# recall we have b1, s1 defined as the slope and its standard error
# since t_obs is positive, take twice the upper tail are
2 * (1 - pt(b1/s1, df=n-2))
```

These all match the summary, and we can conclude there's very strong evidence to suggest a significant linear relationship exists.

### Confidence interval

We can also compute a 95% confidence interval for $\beta_1$

$$
\text{$C\%$ or $(1\!-\!\alpha)$ interval}~=~\hat\beta_1~\pm~t_{\alpha/2,\,n-2}\cdot\se(\hat\beta_1)
$$

```{r}
b1 + c(-1,1) * qt(0.975, df=n-2) * s1
```

We can check this with the `confint()` function:

```{r}
confint(lm_icecream, level = 0.95)
```

Again, everything agrees! Our 95% confidence interval for the true slope $\beta_1$ is $`r ci(confint(lm_icecream, level = 0.95)[2,])`$.


## Prediction

Once we have our model, we can use it to make predictions for new $X$ observations.

:::{.note}
Two things to keep in mind here:

1. Only new $X$ values within (or perhaps only slightly beyond) our range of observed $x_i$ values should be considered as viable candidates for prediction with our model. Far outside our $x_i$, we simply have no data and the linear model may NOT be valid therein.

2. Linear models are intended to **predict the average $Y$ for a given $X$**, not an individual $Y$ observation. Individual observations are of course subject to additional $\ep_i$ errors.
:::

Suppose we have a new period of time with mean temperature 30°F. What would our model predict for this period's average ice cream consumption in pints per capita?

```{r}
# again, recall we have b0, b1 defined as the fitted intercept and slope
b0 + b1*30
```

We can also check this with `predict()`, which is a generic function that calles `predict.lm()`. Using this function, we can easily make several new predictions at once. Suppose we wish to see average consumption predictions for temperatures 30,40,...,70°F:

```{r}
# predict() expects lm object + data frame with new X values in column of same name
predict(lm_icecream, newdata = tibble(temperature=3:7*10))
```

:::{.i96 .fold .s}
```{r,fig.width=8}
ggplot(icecream, aes(x=temperature, y=consumption)) + geom_point() +
  geom_abline(slope=b1, intercept=b0, color="red") + 
  geom_point(data=cbind(x=3:7*10,y=predict(lm_icecream,list(temperature=3:7*10))),
             aes(x,y,color="new predictions\nat 30,40,…,70"),size=3,shape="square") +
  scale_color_manual(values=c("blue")) +
  labs(title="Ice cream consumption vs temperature (with new predictions)",
  x="Mean temperature (F)", y="Consumption (pints per capita)")
```
:::


### More intervals

We can also augment our new prediction with one of two new kinds of confidence intervals:

 1. A **confidence interval** for the mean response (not to be confused with the coefficient confidence intervals discussed previously) is an interval for the **predicted mean response** for a given new predictor value.

 2. A **prediction interval** for a single observation, which is an interval for the response value of one newly observed data point.

Another way of thinking about it is that a **confidence interval is an interval for the best fit line** that we computed through our data, whereas a **prediction interval is an interval for our cloud of data points**.

From this, it shouldn't be surprising that a **prediction interval will typically be much wider than a confidence interval**, since it needs to account for the additional point-to-point variation in the dataset.

Suppose we have a new observation $X=x_\new$. Again we will use a t-distribution with $\nu=n-2$ degrees of freedom, with the following standard error formulae for the confidence & prediction intervals:

$$
\se(\hat y)=\hat\sigma\sqrt{\frac1n+\frac{(x_\new-\bar x)^2}{s_x^2(n-1)}}~~~~~~~~~~~~~~~~\\
\se(y_\new)=\hat\sigma\sqrt{1+\frac1n+\frac{(x_\new-\bar x)^2}{s_x^2(n-1)}}~,~\text{where}
$$

 - $\se(\hat y)$ is the confidence interval standard error for $\hat y=\hat\beta_0+\hat\beta_1x_\new$, the model prediction for $x_\new$,

 - $\se(y_\new)$ is the prediction interval standard error for $y_\new$, a hypothetical unobserved responsed that could correspond to $x_\new$,

 - and $\hat\sigma$ is again the residual standard error as previously defined.

Then, we have the following formulae for the confidence & prediction intervals:

$$
\text{$C\%$ or $(1\!-\!\alpha)$ confidence interval}\,=~\hat y\:\pm~t_{\alpha/2,\,n-2}\cdot\se(\hat y)~~~~~\\
\text{$C\%$ or $(1\!-\!\alpha)$ prediction interval}~=~\hat y~\pm~t_{\alpha/2,\,n-2}\cdot\se(y_\new)
$$

Suppose we wish to compute 95% confidence and prediction intervals for our new observation of 30°F. We know from earlier this corresponds to predicted $\hat y=`r tr(b0+b1*30,4)`$.

```{r}
# 95% confidence interval for new X at 30
(b0+b1*30) + c(-1,1) * qt(0.975,n-2) * sigma*sqrt(1/n+(30-xbar)^2/(sx^2*(n-1)))
# 95% prediction interval for new X at 30
(b0+b1*30) + c(-1,1) * qt(0.975,n-2) * sigma*sqrt(1+1/n+(30-xbar)^2/(sx^2*(n-1)))
```
```{r,include=F}
C <- (b0+b1*30) + c(-1,1) * qt(0.975,n-2) * sigma*sqrt(1/n+(30-xbar)^2/(sx^2*(n-1)))
P <- (b0+b1*30) + c(-1,1) * qt(0.975,n-2) * sigma*sqrt(1+1/n+(30-xbar)^2/(sx^2*(n-1)))
```

We can interpret these two as follows:

 1. We are 95% confident the **true mean ice cream consumption at mean temperature of 30°F** is between `r sf(C[1],2)` and `r sf(C[2],2)` pints per capita.

 2. We are 95% confident the **ice cream consumption for a single _new_ observation at mean temperature of 30°F** would be between `r sf(P[1],2)` and `r sf(P[2],2)` pints per capita.

Again, we can check these with R. This is as easy as setting the `interval` argument to the `predict()` function (which remember calls `predict.lm()`). Set the argument to `"confidence"` for a confidence interval or `"prediction"` for a prediction interval. This can of course easily accommodate multiple predictions; let's try it again for temperatures 30,40,...,70°F:

```{r}
# 95% confidence interval for new X at 30,40,...,70
predict(lm_icecream, newdata=tibble(temperature=3:7*10), interval="confidence")
# 95% prediction interval for new X at 30,40,...,70
predict(lm_icecream, newdata=tibble(temperature=3:7*10), interval="prediction")
```

Finally, let's visualize the 95% confidence and prediction intervals for each $x_\new$ along the

:::{.i96 .fold .s}
```{r,fig.width=8.2}
ggplot(icecream, aes(x=temperature, y=consumption)) + geom_point() +
  geom_abline(aes(slope=b1,intercept=b0,color="\nbest fit line\n"),key_glyph="path",linewidth=1) +
  stat_function(aes(color="confidence\ninterval"),linewidth=1,linetype="dashed",
                fun=\(x)b0+x*b1+qt(0.975,n-2)*sigma*sqrt(1/n+(x-xbar)^2/(sx^2*(n-1)))) +
  stat_function(linewidth=1,color="#7570b3",linetype="dashed",
                fun=\(x)b0+x*b1-qt(0.975,n-2)*sigma*sqrt(1/n+(x-xbar)^2/(sx^2*(n-1)))) +
  stat_function(aes(color="\nprediction\ninterval\n"),linewidth=1.3,linetype="dotted",
                fun=\(x)b0+x*b1+qt(0.975,n-2)*sigma*sqrt(1+1/n+(x-xbar)^2/(sx^2*(n-1)))) +
  stat_function(linewidth=1.3,color="#1b9e77",linetype="dotted",
                fun=\(x)b0+x*b1-qt(0.975,n-2)*sigma*sqrt(1+1/n+(x-xbar)^2/(sx^2*(n-1)))) +
  scale_color_manual(breaks=c("\nbest fit line\n","confidence\ninterval","\nprediction\ninterval\n"),
                     values=c("#d95f02","#7570b3","#1b9e77")) + 
  scale_x_continuous(expand=0) + theme(legend.key.width=unit(2.2,"line")) +
  labs(x="Mean temperature (F)", y="Consumption (pints per capita)",
       title="Ice cream consumption vs temperature (with confidence/prediction intervals)")
```
:::

<!--
## fitting other relations

### exponential, power law, etc.

## bonus: list of all lm object functions
```{r}
# get all stats::: functions implementing .lm method
# sort(grep("\\.lm",unclass(lsf.str(envir = asNamespace("stats"), all = T)),value = T))
```
-->
