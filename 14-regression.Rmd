

# Regression

Regression is a broad class of methods where predictor variables are used to predict a numeric response. In this class, we will primarily focus on **simple linear regression**, where a single numeric predictor is used and the model is linear.


## Correlation coefficient

First, a brief aside on the correlation coefficient. Suppose we have a sample of pairs of observations $x_i$, $y_i$ drawn from $X$, $Y$ where each pair is independent from other pairs. Note however no assumption of independence is made between the variables, only between different pairs.

Similar to other statistics like mean or variance, there are both theoretical---i.e. population---and sample versions of the correlation coefficient definition. One is the underlying true value and the other is what you observe in your real dataset. The population correlation coefficient is defined as:

$$
\rho=\frac{\e\big[(X-\mu_X)(Y-\mu_Y)\big]}{\sigma_X\sigma_Y}
$$

where the numerator, also called the covariance of $X$ and $Y$, represents how much on average $X$ and $Y$ change together when compared to their means, and the denominator normalizes it by the product of their standard deviations.

Compare this with the definition of the sample correlation coefficient:

$$
r=\frac{\frac1{n-1}\sum(x_i-\bar x)(y_i-\bar y)}{s_xs_y}
$$

Note similar to the definitions of $s_x$, $s_y$, the numerator also use $n-1$ instead of $n$ for its averaging.

It's easy to show that both $\rho$, $r$ always belong in the range $[-1,1]$, i.e. the **correlation coefficient must always have absolute value $\le1$**.

Below shows a series of simulated [jointly-normal](https://en.wikipedia.org/wiki/Multivariate_normal_distribution) $x_i$, $y_i$ observations drawn from $X$, $Y$ both with mean 0 and SD 1 but with various population correlation coefficient values:

:::{.fold .s}
```{r,include=F}
library(tidyverse)
options(pillar.print_min = 5)
source("ggplot_theme_options.R")
set.seed(3)
```
```{r,}
library(tidyverse) ; library(mvtnorm) ; library(plotly)
n <- 200
cor_dfs <- do.call(rbind, lapply(
  c(-20:20/20), \(r) rmvnorm(n,c(0,0),matrix(c(1,r,r,1),ncol=2)) %>% round(3) %>% 
    as.data.frame %>% setNames(c("x","y")) %>% cbind(r=r))) %>% arrange(r,x)
plot_ly(cor_dfs, type = "scatter", x = ~x, y = ~y, frame = ~r, mode = "markers", height=600) %>%
  config(displayModeBar = FALSE) %>% animation_opts(frame = 100, transition = 0) %>% 
  layout(title = list(text = "Simulated X,Y with various correlation coefficients r", x = .15),
         xaxis = list(range = list(-3,3)), yaxis = list(range = list(-3,3)),
         margin = list(l = 140, r = 140, b = 50, t = 50), dragmode=FALSE) %>%
  animation_slider(currentvalue = list(font = list(color = "#444444")))
```
:::

From this, several things should be clear:

 - The correlation coefficient's **sign indicates the direction of the trend**
 
   - Positive $+$ sign indicates a positive relationship (slope $>0$)
   
   - Negative $-$ sign indicates a negative relationship (slope $<0$)
   
 - The correlation coefficient's **absolute value indicates how "tight" the points are**
 
   - Absolute value close to 1 indicates the points are tight around the line

 - Correlation **equal or close to 0 indicates little or no relationship** (flat cloud of points)


### Caveats

The correlation coefficient has a major caveat: **it only measures the linear component of the relationship**. If the relationship is non-linear, it may not be useful at all!

To be even more clear, the **correlation coefficient can NOT be used to check linearity**! This is why it's important to **plot your data** first to make sure it's linear before directly jumping into regression.

Below is a famous example of this know as the Anscombe's quartet, which is built-in to R. This is a set of 4 $x_i$, $y_i$ datasets, each of which can be checked to have the same mean and SD in both $x$ and $y$, as well as the same correlation coefficient of about $0.816$.

```{r}
anscombe
anscombe %>%
  pivot_longer(everything(), names_to=c(".value","set"), names_pattern="(.)(.)") %>% 
  group_by(set) %>% summarize(xbar = mean(x), ybar = mean(y), sx = sd(x),
                              sy = sd(y), r = cor(x,y))
```

And yet, when plotted it's clear that these are 4 extremely different datasets, only one of which is actually appropriate for a linear regression model:

:::{.i96 .fold .s}
```{r}
anscombe %>%
  pivot_longer(everything(), names_to=c(".value","set"), names_pattern="(.)(.)") %>% 
  mutate(set = paste("set",set)) %>% 
  ggplot(aes(x=x,y=y)) + geom_point() + geom_smooth(method=lm, se=F) +
  facet_wrap(~set, scales="free") + ggtitle("Anscombe's quartet datasets, each r ≈ 0.816")
```
:::

As an even more extreme example, here's 4 datasets I cooked up, each of which has 0 correlation. Again, only one of which is actually appropriate for a linear regression model:

:::{.i96 .fold .s}
```{r}
# pick seed where rmvnorm generates dataset with especially low r
set.seed(841)
n <- 12
rmvnorm(n,c(0,0),matrix(c(1,0,0,1),ncol=2)) %>% as.data.frame %>% setNames(c("x1","y1")) %>% 
  add_column(
    x2 = cos(seq(0, 2*pi*(1-1/n), length.out=n)),
    y2 = sin(seq(0, 2*pi*(1-1/n), length.out=n)),
    x3 = seq(-1, 1, length.out=n),
    y3 = x3^2-1,
    x4 = x3,
    y4 = abs(sin(x4*pi))) %>%
  pivot_longer(everything(), names_to=c(".value","set"), names_pattern="(.)(.)") %>% 
  group_by(set) %>% mutate(x = (x-mean(x))/sd(x), y = (y-mean(y))/sd(y)) %>% 
  ggplot(aes(x=x,y=y)) + geom_point() + geom_smooth(method=lm, se=F) +
  facet_wrap(~set, scales="free") + ggtitle("Additional quartet of datasets, each r = 0")

```
:::

:::{.note}
All this is to demonstrate that the **correlation should NOT be used to determine if linear regression is appropriate**. It can ONLY be used to assess the strength of the relationship AFTER you've already visually checked the dataset is in fact linear by plotting!
:::

## Model notation

Now, let's move on to the actual modeling notation used. Again, let's suppose we draw a **sample of $n$ pairs $(x_i,y_i)$ from some population variables $X,Y$**, and suppose we've visually identified a linear regression model as appropriate, based on graphically assessing the degree of linearity, as well as possibly checking the correlation coefficient.

Our theoretical linear model is then the following:

$$
y_i=\beta_0+\beta_1x_i+\ep_i
$$

 - $x_i$, $y_i$ are the actual **data points** in our sample,
 
 - $\beta_0$ is the **true intercept** of the line,
 
 - $\beta_1$ is the **true slope** of the line,
 
 - $\ep_i$ are the **errors** of each point, which model the random fluctuations inherent to every measurement. These are assumed to be distributed $\ep_i\sim\n(0,\sigma)$, i.e. they are modeled as:
   
   i)   Normally distributed, with
   ii)  Mean zero, and
   iii) Constant variance.

:::{.eg}
At this point, it's important to take note of the following:

 - Here, $\beta_0$, $\beta_1$ represent the **true underlying intercept & slope** values that generate the data (assuming the data is in fact linear), NOT a best fit line based on a sample.

 - $x_i$, $y_i$, $\ep_i$ all have $i$ indices since they vary for each point, but $\beta_0$, $\beta_1$ do NOT have indices, since the points are supposed to all fall on ONE line with some true slope and intercept.
 
 - The $x_i$ are actually treated as constants, just like $\beta_0$, $\beta_1$, which means the only variable on the right hand side is $\ep_i$.
 
 - The assumptions on $\ep_i$ mean that the $y_i$ points are **distributed normally _about_ the regression line**. In other words, within every vertical slice along the line, the points follow a normal distribution along the y-axis.
:::

### Simulated demo

```{r,include=F}
set.seed(1)
b0 <- -3; b1 <- 0.5 ; n <- 10; m <- 10 ; M <- 20 ; sigma <- 2
```

As a quick demo of this setup and notation, suppose we pick true coefficients $\beta_0=`r b0`$, $\beta_1=`r b1`$. Using this, let's generate $n=`r n`$ observations where $x_i$ are uniformly sampled from $[`r m`,`r M`]$, and $\sigma=`r sigma`$.

Using these values, we can simulate one possible sample of $x_i$, $y_i$ values drawn from $X,Y$ with:

```{r,echo=F,results='asis'}
cat(glue::glue('``` r
# define values for simulation demo
b0 <- {b0} ; b1 <- {b1} ; n <- {n} ; m <- {m} ; M <- {M} ; sigma <- {sigma}
```'))
```
```{r}
# generate dataset for demo
x <- runif(n, m, M)
e <- rnorm(n, 0, sigma)
y <- b0 + b1*x + e
```

:::{.i9 .fold .s}
```{r,fig.width=6,fig.height=4.6}
# pick a convenient point in quadrant IV as example
i <- which.min((scale(x)[,1]-.5)^2+(scale(y)[,1])^2)

ggplot(tibble(x,y), aes(x=x,y=y)) + geom_point() +
  geom_segment(aes(color="xi",x=x[i],y=0,yend=y[i]),linewidth=.7) + 
  geom_segment(aes(color="yi",y=y[i],x=10,xend=x[i]),linewidth=.7) + 
  geom_segment(aes(color="εi",x=x[i],y=y[i],yend=(b0+b1*x[i])),linewidth=.7) + 
  geom_point(aes(color="i-th point",x=x[i],y=y[i]),size=3,shape="square") + 
  stat_smooth(aes(color="fitted line"),method=lm,se=F,linewidth=1,linetype="dashed",fullrange=T) +
  geom_abline(aes(color="true line", slope=b1, intercept=b0),linewidth=1,key_glyph="path") + 
  scale_x_continuous(breaks=c(seq(10,20,2),x[i]), labels=c(seq(10,20,2),"xi"),
                     limits=c(10,20), expand=0, minor_breaks=10:20) + 
  scale_y_continuous(breaks=c(seq(0,10,1)[-5],y[i]), labels=c(seq(0,10,1)[-5],"yi"),
                     limits=c(0,10), expand=0, minor_breaks=NULL) + 
  scale_color_manual(breaks=c("fitted line","true line","i-th point","εi","yi","xi"),
                     values=c("#7570B3FF","#1B9E77FF","#666666FF","#E6AB02FF","#E7298AFF","#A6761DFF")) + 
  theme(legend.key.width=unit(2.1,"line")) +
  labs(color = "Legend", title = "Simulated linear model demo")
```
:::

## Fitting process

Without going into too much detail, the typical fitting method, called **Ordinary Least Squares** (OLS) involves **finding the line that minimizes the _sum of squared errors_ vs the data points**.

Suppose we "test out" a hypothetical line with coefficients $\beta_0',\beta_1'$. The sum of the squared errors vs the data points, also called the **loss function $\ell(\beta_0',\beta_1')$**, is defined as:

$$
\ell(\beta_0',\beta_1')=\sum_{i=1}^n(y_i-(\beta_0'+\beta_1'x_i))^2
$$

Note this is a function of $\beta_0',\beta_1'$, i.e. we can "test out" different possible values of the coefficients, and evaluated the "loss" of each guess---i.e. how "bad" the guess is---by plugging it into the function to get the sum-squared-errors. To find the coefficients that give the best fit to the data, we solve for the $\beta_0',\beta_1'$ that would minimize the above function, which is done by **solving for the local minimum** where the **partial derivatives with respect to $\beta_0',\beta_1'$ both vanish**:

$$
\text{solve}\quad
\frac{\partial\ell(\beta_0',\beta_1')}{\partial\beta_0'}=0
\quad\text{and}\quad
\frac{\partial\ell(\beta_0',\beta_1')}{\partial\beta_1'}=0
$$

This is not extremely difficult to solve but we omit the [details](https://statproofbook.github.io/P/slr-ols.html). One can show that the best-fit coefficients $\hat\beta_0,\hat\beta_1$ that minimize the loss function are:

$$
\hat\beta_1~=~r\cdot\frac{s_y}{s_x}~~~~\\
\hat\beta_0~=~\bar y-\hat\beta_1\bar x
$$

Here's a visualization of the squared errors for the line of best fit with coefficients $\hat\beta_0,\hat\beta_1$ that minimize the loss function:

:::{.i9 .fold .s}
```{r,fig.width=6,fig.height=4.6}
bh1 <- cor(x,y)*sd(y)/sd(x)
bh0 <- mean(y)-bh1*mean(x)
ggplot(tibble(x,y), aes(x=x,y=y)) +
  geom_rect(aes(xmin=x,xmax=x+(y-(x*bh1+bh0))*if_else(x>16.5|y>1&y<3,-1,1),
                ymin=y,ymax=x*bh1+bh0,color="squared errors"),alpha=.1) + 
  geom_segment(aes(x=x, y=y, yend=x*bh1+bh0, color="errors"),linewidth=1) +
  stat_smooth(aes(color="fitted line"),method=lm,se=F,fullrange=T,linewidth=1) + 
  geom_point(size=2) +
  scale_x_continuous(breaks=seq(10,20,2), labels=seq(10,20,2),
                     limits=c(10,20), expand=0, minor_breaks=10:20) + 
  scale_y_continuous(breaks=seq(0,10,1), labels=seq(0,10,1),
                     limits=c(0,10), expand=0, minor_breaks=NULL) + 
  scale_color_manual(breaks=c("fitted line","errors","squared errors"),
                     values=c("#7570B3FF","#E6AB02FF","#666666FF")) +
  labs(color = "Legend", title = "Least squares visualized for best fit line")
```
:::


