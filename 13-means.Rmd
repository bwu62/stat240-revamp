

# Means

Now let's look at a means-type inteference setup, where we apply a **normal model** to sample means to infer about an underlying **true mean** in the population. We will divide this into two broad scenarios again:

- One-mean scenario, where there is a single mean of interest from a single population, and

- Two-means scenario, where there are two means of interest that we seek to compare. This time, we will subdivide this into additional sub-cases:

  i)  **Paired case**, where the two samples have a **natural one-to-one correspondence** and are more akin to **paired or repeat observations from the same population**, vs
  
  ii) **Unpaired case**, where the two samples are in fact two independent samples drawn from two independent populations, each with its own mean.


:::{.note}
A means-type inference approach using a normal model is **only appropriate** if your sample consists of either 1 or 2 samples where, for each sample, your data are **numeric values** and are at least **_approximately_ normal-looking**.
:::

## T-distribution

In a moment we will delve into notation and examples for means inference, but first let's discuss the t-distribution.

In the proportions inference section, recall we generally know $n$ and only need to infer $p$, and once $p$ has been estimated or hypothesized, both the expectation and variance of the resulting binomial distribution are determined, since they're both direct functions of $p$.

For normal distributions, you generally start off not knowing either $\mu$ or $\sigma$, and furthermore it can be shown that the corresponding sample statistics $\bar x$ and $s$ are in fact totally statistically independent of each other ^[this comes from [Basu's theorem](https://en.wikipedia.org/wiki/Basu%27s_theorem)].

The implication of this is even though usually we're primarily interested on doing **inference on just mean(s)**, we also need to **separately estimate the standard deviation** in order to "pin down" a specific vesion of the normal. In other words, we generally **use LLN to both estimate $\bar x\to\mu$ and $s\to\sigma$**.

However, this leads to another problem, which is sometimes $s$ will underestimate $\sigma$ (and sometimes it may overestimate), and this distorts the result reference distribution (use for both intervals and testing) slightly away from the standard normal, **especially for small samples**. Instead, we use a different **reference distribution: the t-distribution**.

The [**student's t-distribution**](https://en.wikipedia.org/wiki/Student%27s_t-distribution), as it's formally called, has a complex PDF which we won't show but has a simple motivation: it models the **distribution of a normalized sample mean when the true $\sigma$ is unknown** (similar to how in the proportions section, the normal models the distribution of a sample proportion).

The t-distribution has a single parameter called the **degrees of freedom**, or sometimes $\nu$, that **must be positive, and usually an integer**, although technically any real number is allowed. It sounds fancy, but it's just a parameter like any other, i.e. it's a number in a formula that determines the exact shape of the distribution, just like $\mu$ or $p$.

Here's a plot of the t-distribution for various degrees of freedom:

:::{.fold .s}
```{r,fig.height=5}
library(tidyverse) ; library(plotly)
N <- 101 ; xl <- 4
tdist <- tibble(
  df = rep(c(1:10,12,14,16,18,20,25,30,35,40,50,60,80,100,Inf),each=N),
  x = rep(seq(-xl,xl,length.out=N),length(unique(df))),
  T = dt(x,df),
  N = dnorm(x)) %>% pivot_longer(3:4, names_to = "dist", values_to = "y")
plot_ly(tdist, type = "scatter", x = ~x, y = ~y, frame = ~df, color = ~dist, mode = "lines") %>%
  config(displayModeBar = FALSE) %>% 
  layout(title = list(text = "T-distribution with various degrees of freedom vs Normal(0,1)", x = .05),
         margin = list(l = 10, r = 10, b = 50, t = 50), dragmode=FALSE) %>% animation_opts(frame = 100) %>%
  animation_slider(currentvalue = list(font = list(color = "#444444")))
```
:::

A few key facts that hopefully are intuitive after examining the plot:

 - Conceptually, t-distributions are basically a standard normal distribution (Z) except modified to have "heavier tails"
 
 - The lower the degrees of freedom, the bigger the modification, i.e. the "heavier" the tails
 
   - As $\nu\to\infty$, the t-distribution approaches the normal
   
   - The case where $\nu=1$ has a special name: the [Cauchy distribution](https://en.wikipedia.org/wiki/Cauchy_distribution), and it has interesting properties (its tails are so "heavy" that it has undefined---or infinite---mean, variance, skew, and all higher [moments](https://www.shiksha.com/online-courses/articles/moments-in-statistics)), which makes it very useful for many counter examples, e.g. breaking the [LLN](https://awstringer1.github.io/sta238-book/section-introduction-to-statistics-law-of-large-numbers-and-central-limit-theorem.html#section-cb124) or the [CLT](https://web.ma.utexas.edu/users/mks/M358KInstr/Cauchy.pdf).

 - Similar to the normal, it's bell-ish shaped and is symmetric about its center 0
 
 - For $\nu>2$ it has variance $\sigma^2=\nu/(\nu-2)$
 
 - The R functions for working with the t-distribution are `dt()`, `pt()`, `qt()`, `rt()`, just as you'd expect.
 
   - Each of these functions also demands a `df` argument for the degrees of freedom, so don't forget to specify it when using.


## One mean

### Model notation

Now that we understand the t-distribution, let's discuss the one-mean scenario. Suppose we draw a **sample of size $n$** observations, each of which is done independently, from an underlying **normal population** with underlying true **mean $\mu$ and SD $\sigma$**.

Thus, our model for the population is $X\sim\n(\mu,\sigma)$ for some unknown, constant $\mu$, $\sigma$.

Let **$\bar x$ be the sample mean**, and **$s$ be the sample standard deviation** (as defined in sections \@ref(sample-mean) and \@ref(sample-var-sd)). By the LLN, we know $\bar x\to\mu$ and $s\to\sigma$, so these are naturally our point estimates.

:::{.note}
A few notes here that are importants:

- Technically the population is required to be normal, however in practice the t-based mean inference methods we're learning are considered to be quite **robust against non-normality**, i.e. even moderate departures from normality are often not considered problematic.

- Even though we are generally interested only in inference on $\mu$, we still need to estimate $\sigma$ with $s$ in order to model the variance structure, which is generally a necessary part of inference.
:::

So far the notation is quite intuitive, so let's move on directly to the confidence interval formula.

### Confidence interval

For a one-mean scenario, the confidence interval has the following form:

$$
\text{$C\%$ or $(1\!-\!\alpha)$ interval}~=~\bar x~\pm~t_{\alpha/2,n-1}\cdot\frac s{\sqrt n},~\text{ where}
$$

 - $\bar x=\sum x_i/n$ is the **sample mean**,
 
 - $s=\sqrt{\sum(x_i-\bar x)^2/(n\!-\!1)}$ is the **sample standard deviation**,
 
 - $t_{\alpha/2,n-1}$ is the **$\alpha$-level critical value for a t-distribution with $n-1$ degrees of freedom**, such that $\p(|T|>|t_{\alpha/2,n-1}|)=\p(T>t_{\alpha/2,n-1})+\p(T<-t_{\alpha/2,n-1})=\alpha$, in other words the observation on the t-distribution normal such that the two "outer-tails" defined by it and its mirror image sum to $\alpha$ together.
   
   ::::{.i5 .fold .s}
   ```{r setup-all,echo=F}
   source("ggplot_theme_options.R")
   ```
   ```{r,fig.width=4,fig.height=2.8}
   library(latex2exp)
   df <- 3 ; xl <- 6  # pick an example df and xlimits
   ggplot() + geom_function(fun=\(x)dt(x,df), xlim=c(-xl,xl)) +
     stat_function(fun=\(x)dt(x,df), geom="area", xlim=c(-xl,qt(.025,df)), fill="red") +
     stat_function(fun=\(x)dt(x,df), geom="area", xlim=c(qt(.975,df),xl), fill="red") +
     scale_x_continuous(breaks=qt(c(.025,.5,.975),df), minor_breaks=NULL, expand=0,
                        labels=TeX(c("$-t_{\\alpha/2,n-1}$","0","$t_{\\alpha/2,n-1}$"))) +
     scale_y_continuous(breaks=NULL, limits=c(0,dt(0,df)*1.01), expand=0) + theme(axis.text.x=element_text(size=13)) +
     labs(x=NULL, y=NULL, title=TeX("$t_{\\alpha/2,n-1}$ critical value for T (red areas sum to $\\alpha$)"))
   ```
   ::::
   
   This is similar to $z_{\alpha/2}$ for the proportion, except due to the heavier tails of the t-distribution, this value is always higher, i.e. $t_{\alpha/2,n-1}>z_{\alpha/2}$ for all $n$. However, as $n\to\infty$, this approaches the normal critical value, similar to how as $n\to\infty$ the t-distribution approaches the normal.  
   
   Also note the **degrees of freedom is _always_ $n-1$**, or one less than the sample size. An interesting interpretation of this is that in order to do anything meaningful inference-wise, without running into Cauchy-related issues, we generally need $n\ge3$.  
   
   To compute the critical value, we use `qt()`, taking care to remember to always specify the `df` argument:
   
   ```{r}
   # suppose we have a sample of size n=6, and we use the common alpha
   n <- 6 ; alpha <- 0.05
   # the corresponding n-1 df alpha/2 t-critical value
   qt(1-alpha/2, n-1)
   ```
   
   - and finally, $\se(\bar x)=s/\sqrt n$ is the **estimated standard error of $\bar x$**. In a perfect world where we know $\sigma$ we could use that instead of $s$, which reverts the reference distribution back to a normal instead of t--, but this is not usually the case.

:::{.eg}
Suppose we have the following dataset of $n=`r (n=15)`$ masses in grams of ball bearings manufactured by a factory machine that should be calibrated to a mean mass of $\mu=12$ grams.

```{r,include=F}
set.seed(11)
bearings = round(sample(rnorm(15,12,.15)),2)
t.test(bearings,mu=12)
```
```{r,echo=F,results='asis'}
cat(glue::glue('``` r
bearings <- c({toString(sf(bearings[1:8],4))},
              {toString(sf(bearings[9:15],4))})
bearings
```'))
```
```{r,echo=F}
bearings
```
```{r}
n <- length(bearings)  # define our sample size
```

Since we have $n=`r n`$ observations, we require a t-distribution with $\nu=`r n-1`$ degrees of freedom; we can also compute our sample mean and SD:

```{r}
mean(bearings)
sd(bearings)
```
```{r,echo=F,results='asis'}
cat(glue::glue('``` r
qt(0.975, df={n-1})
```'))
```
```{r,echo=F}
qt(0.975, df=n-1)
```

Putting it all together, here's out 95% confidence interval for the true mean $\mu$ based on our sample:

```{r}
mean(bearings) + c(-1,1) * qt(0.975, df=n-1) * sd(bearings)/sqrt(n)
```
```{r,include=F}
C <- mean(bearings) + c(-1,1) * qt(0.975, df=n-1) * sd(bearings)/sqrt(n)
```

Thus, our 95% interval for the true mean is `r ci(C,4)`. In other words, we are 95% confident the true mean mass of ball bearings produced by this machine is between `r sf(C[1],4)` and `r sf(C[2],4)` grams.
:::

### T-test

The one-mean hypothesis test, more commonly simply called the **t-test**, starts with the following familiar form of hypotheses:

$$
H_0:\mu~=~\mu_0\,~~~~~~~\\
~~~~~~~\,H_a:\mu~<,\,\ne,\,\text{or}>~\mu_0
$$

where $\mu_0$ is the hypothesized true mean under the null. Under the null, the sample mean $\bar x$ is distributed normally:

$$
\bar x\sim N(\mu_0,\sigma/\!\sqrt n)
$$

As discussed previously, we estimate $s\to\sigma$ out of necessity, which means the following test statistic $t_\obs$ adopts a t-distribution with $\nu=n-1$ degrees of freedom:

$$
t_\obs=\frac{\bar x-\mu_0}{s/\!\sqrt n}~\sim~T_{n-1}
$$

Then, again, it's simply a matter of taking the appropriate tail area.

:::{.eg}
Continuing with the ball bearing example above, suppose we wish to test if the machine is properly calibrated for $\mu=12$ grams. Let's use the two-sided hypothesis here, since if it's miscalibrated on either side (too high or too low), we probably want to recalibrate it.

$$
H_0:\mu=\mu_0\\
H_a:\mu\ne\mu_0
$$

We can compute our test statistic:

```{r}
t_obs <- (mean(bearings)-12) / (sd(bearings)/sqrt(n))
t_obs
```

Then, we compute our p-value, remembering to take twice the outer tail:

```{r}
2 * pt(t_obs, df=n-1)
```

We can see our p-value here is not less than the standard $\alpha=0.05$, so here we do not reject the null. In other words, there's insufficient evidence to suggest the machine is miscalibrated, so there's no need to recalibrate it.
:::

### R method

In R, you can compute both the one-mean interval and t-test using the `t.test()` function, which has the following important arguments:

 - `x` is the vector of observations,
 
 - `alternative` is the alternative direction (if doing a t-test), and again can be either `"two.sided"` (the default) or `"greater"` or `"less"`
 
 - `mu` is the hypothesized $\mu_0$ under the null
 
 - `conf.level` is the desired confidence (if doing an interval) which also defaults to 95%.

:::{.eg}
Continuing again with the ball bearings example, below are teh 95% confidence interval for the true mean, as well as the t-test for testing $\mu=12$ vs $\mu\ne12$.
```{r}
t.test(bearings, mu=12, conf.level=0.95)
```

We can see both the interval and p-value match our earlier computations.
:::


## Two means -- Paired

For the two-means scenario, we start by considering the paired case.

Suppose our data consists of $n$ independent pairs of **comparable observations** (i.e. can be differenced), and we are primarily interested on **inference on the differences between the pairs**. In this case, we don't separately model the observations in each pair as coming from a different independent distribution; we instead model the **differences as a sample drawn from a population of possible differences** and run inference on it.

This case is quite common in the real world, e.g. before vs after measurements for some treatment, measurements on twins, left vs right side measurements on a single individual, etc. In all these cases, differencing the pairs of observations can help **control for certain additional unobserved pair-to-pair confounders**.

The key thing to remember for paired data is this: it's completely ***equivalent to the previously discussed one-sample T-based methods on the vector of differences***!

:::{.eg}
A small study was done to test the effectiveness of a new diet on lowering cholesterol levels. Below is the before and after data for $n=9$ subjects.

:::{#chol}

<style>#chol table{width:75%}#chol th,#chol td{border:1px solid black}</style>

<center>

| Subject | A   | B   | C   | D   | E   | F   | G   | H   | I   |
|:-------:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
| Before  | 209 | 210 | 205 | 198 | 216 | 217 | 238 | 240 | 222 |
| After   | 199 | 207 | 189 | 209 | 217 | 202 | 211 | 223 | 201 |

</center>

:::

Find a 95% confidence interval for the change in cholesterol after the diet. Also run a hypothesis test at the standard significance level to test if there was a significant decrease in cholesterol levels.

First, let's load in the dataset and compute the changes. Note the **changes should be differenced as _after–before_**, not the other way around!

```{r}
before <- c(209, 210, 205, 198, 216, 217, 238, 240, 222)
after <- c(199, 207, 189, 209, 217, 202, 211, 223, 201)
diff <- after - before
diff
n <- length(diff)
```

Now, we can simply apply our one-mean methods from the previous section! Here's the 95% confidence interval, manually and using R:

```{r}
mean(diff) + c(-1,1) * qt(0.975, df=n-1) * sd(diff)/sqrt(n)
t.test(diff, conf.level=0.95)
```

For a hypothesis test, we choose $H_0:\mu=0$ vs $H_a:\mu<0$ where $\mu$ represents the true average change in cholesterol levels resulting from the diet. Then, we can compute out t-statistic and p-value, and also check using R:

```{r}
t_obs <- (mean(diff)-0) / (sd(diff)/sqrt(n))
t_obs
pt(t_obs, df=n-1)
t.test(diff, mu=0, alternative="less")
```

Alternatively, you can also give both the `after` and `before` vectors to `t.test()` separately as the `x` and `y` arguments, and set the argument `paired = TRUE`, which will result in the exact same results. Note by default R will always take the difference of the arguments in the order **first–second**.

```{r}
# 95% confidence interval
t.test(after, before, paired=TRUE, conf.level=0.95)
# hypothesis test
t.test(after, before, paired=TRUE, mu=0, alternative="less")
```

Either way, we can see everything agrees.
:::


## Two means -- Unpaired

Next, we consider the unpaired case, also called the **two independent samples** case.

### Model notation

For this case, suppose we draw **samples of size $n_1$, $n_2$ from two different normal populations with means $\mu_1$, $\mu_2$ and SDs $\sigma_1\sigma_2$**, and again we assume everything is independent (both within and between the two samples).

Thus our models for the two populations are $X_1\sim\n(\mu_1,\sigma_1)$, $X_2\sim\n(\mu_2,\sigma_2)$ where all the mean & SD parameters are unknown constants.

Let $\bar x_1$, $\bar x_2$ be the sample means and $s_1$, $s_2$ be the sample SDs. Again by the LLN, we know $\bar x_i\to\mu_i$ and $s_i\to\sigma_i$ so these are naturally our point estimates.

:::{.note}
Similar to the one-mean scenario, this scenario is not sensitive to non-normality in the data, and the t-distribution will be necessary again since we estimate $s_i\to\sigma_i$
:::

<!--

### Confidence interval

For a two-means scenario, generally our goal is to run inference on the **difference of the underlying population means $\mu_1-\mu_2$**. We think of this as our parameter of interest.

For this parameter, our point estimate naturally is the **difference of our sample means $\bar x_1-\bar x_2$**. Then, our interval takes the form:

$$
\text{$C\%$ or $(1\!-\!\alpha)$ interval}~=~(\bar x_1-\bar x_2)~\pm~t_{\alpha/2,n-1}\cdot\sqrt{\frac{s_1^2}{n_1}+\frac{s_2^2}{n_2}},~\text{ where}
$$

- $\bar x_1-\bar x_2$ is our point estimate of the true difference $\mu_1-\mu_2$,

- $t_{\alpha/2,n-1}$

-->
